{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"DatawarehouseSpark Application\") \\\n",
    "    .getOrCreate()\n",
    " \n",
    "inputFileHire ='/home/mose/Downloads/CSVS/client_hiring_dt.csv' #\"/user/user17/mosedata_proj/input/client_hiring_dt.csv\"\n",
    "#inputFileBio ='/home/mose/Downloads/CSVS/client_bio_dt.csv'  #\"/user/user17/mosedata_proj/input/client_bio_dt.csv\"\n",
    "#inputFileCom = '/home/mose/Downloads/CSVS/client_communication_dt.csv' #\"/user/user17/mosedata_proj/input/client_communication_dt.csv\"\n",
    "#inputFileAct = '/home/mose/Downloads/CSVS/client_activities_dt.csv' #\"/user/user17/mosedata_proj/input/client_activities_dt.csv\"\n",
    "#inputFileFact = '/home/mose/Downloads/CSVS/client_fact_ft.csv' #\"/user/user17/mosedata_proj/input/client_fact_ft.csv\"\n",
    "\n",
    " \n",
    "# Create DataFrame from CSV file\n",
    "dfM =  spark.read.format(\"csv\").option(\"sep\", \",\").option(\"inferSchema\", \"true\").option(\"header\", \"true\").load(inputFileHire)\n",
    "#dfR =  spark.read.format(\"csv\").option(\"sep\", \",\").option(\"inferSchema\", \"true\").option(\"header\", \"true\").load(inputFileBio)\n",
    "#dfCom =  spark.read.format(\"csv\").option(\"sep\", \",\").option(\"inferSchema\", \"true\").option(\"header\", \"true\").load(inputFileCom)\n",
    "#dfAct =  spark.read.format(\"csv\").option(\"sep\", \",\").option(\"inferSchema\", \"true\").option(\"header\", \"true\").load(inputFileAct)\n",
    "#dfFact =  spark.read.format(\"csv\").option(\"sep\", \",\").option(\"inferSchema\", \"true\").option(\"header\", \"true\").load(inputFileFact)\n",
    "\n",
    "\n",
    "# Print the schema of the DataFrames\n",
    "dfM.printSchema()\n",
    "dfR.printSchema()\n",
    "dfCom.printSchema()\n",
    "dfAct.printSchema()\n",
    "dfFact.printSchema()\n",
    "\n",
    "\n",
    "#dfR.groupBy('movieId').count().show(50)\n",
    "#dfM.where(\"director='Martin Brest'\").join(dfR,\"movieId\").select(\"movieId\",\"director\",\"user_name\",\"rating\").show(50)\n",
    "\n",
    "\n",
    "# Stop the Spark Session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting dataframes.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile dataframes.py\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"DatawarehouseSpark Application\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "#Input the data\n",
    "inputFileHire ='/user/user17/mosedata_proj/input/client_hiring_dt.csv'\n",
    "inputFileBio ='/user/user17/mosedata_proj/input/client_bio_dt.csv'\n",
    "inputFileCom = '/user/user17/mosedata_proj/input/client_communication_dt.csv'\n",
    "inputFileAct = '/user/user17/mosedata_proj/input/client_activities_dt.csv'\n",
    "inputFileFact = '/user/user17/mosedata_proj/input/client_fact_ft.csv'\n",
    "\n",
    "#Create data frames from sources. Tables were converted to csvs now being converted to dataframes\n",
    "dfHire =  spark.read.format(\"csv\").option(\"sep\", \",\").option(\"inferSchema\", \"true\").option(\"header\", \"true\").load(inputFileHire)\n",
    "dfBio =  spark.read.format(\"csv\").option(\"sep\", \",\").option(\"inferSchema\", \"true\").option(\"header\", \"true\").load(inputFileBio)\n",
    "dfCom =  spark.read.format(\"csv\").option(\"sep\", \",\").option(\"inferSchema\", \"true\").option(\"header\", \"true\").load(inputFileCom)\n",
    "dfAct =  spark.read.format(\"csv\").option(\"sep\", \",\").option(\"inferSchema\", \"true\").option(\"header\", \"true\").load(inputFileAct)\n",
    "dfFact =  spark.read.format(\"csv\").option(\"sep\", \",\").option(\"inferSchema\", \"true\").option(\"header\", \"true\").load(inputFileFact)\n",
    "\n",
    "# Print the schema of the DataFrames\n",
    "dfHire.printSchema() \n",
    "dfBio.printSchema()\n",
    "dfCom.printSchema()\n",
    "dfAct.printSchema()\n",
    "dfFact.printSchema()\n",
    "\n",
    "#Clients from different service branches\n",
    "dfBio.groupBy('service_branch__c').count().show()\n",
    "\n",
    "#Types of jobs the guys got hired in\n",
    "dfHire.groupBy('job_function_hired_in__c').count().show()\n",
    "\n",
    "#Temporary objects from which sql statements are run for each dataframe\n",
    "dfHire.createOrReplaceTempView(\"dfHire_sql\")\n",
    "dfBio.createOrReplaceTempView(\"dfBio_sql\")\n",
    "dfCom.createOrReplaceTempView(\"dfCom_sql\")\n",
    "dfAct.createOrReplaceTempView(\"dfAct_sql\")\n",
    "dfFact.createOrReplaceTempView(\"dfFAct_sql\")\n",
    "\n",
    "#Mixed information from all 5 tables of the database\n",
    "spark.sql(''' Select hires.hired, bio.service_rank__c , \n",
    "              bio.service_branch__c , fact.yearsinservice, fact.reg_afterservice_years,\n",
    "              com.responsive__c, act.finalized_hhusa_revised_resume_on_file__c as resume_done\n",
    "              from dfHire_sql hires\n",
    "              inner join dfBio_sql bio on bio.id = hires.id \n",
    "              inner join dfFAct_sql fact on fact.id = hires.id\n",
    "              inner join dfCom_sql com on com.id = hires.id\n",
    "              inner join dfAct_sql act on act.id = hires.id\n",
    "              where hires.hired = \"1\"\n",
    "              \n",
    "              ''').show()\n",
    "\n",
    "#Account create date by month of the year\n",
    "spark.sql( \"\"\"SELECT count(id) , \n",
    "              Extract(Month From create_ddate) as month \n",
    "              From  dfAct_sql  \n",
    "              group by month \n",
    "              ORDER BY month ASC \"\"\").show()\n",
    "\n",
    "\n",
    "#Registration for services by year\n",
    "spark.sql( \"\"\"SELECT count(id) , \n",
    "Extract(Year From create_ddate) as year_registered \n",
    "              From  dfAct_sql  \n",
    "              group by year_registered\n",
    "              ORDER BY year_registered ASC \"\"\").show()\n",
    "\n",
    "#Hired by Service branch\n",
    "spark.sql(\"\"\" Select count(hires.id) as hired,bio.service_branch__c \n",
    "              From dfHire_sql hires \n",
    "              inner join dfBio_sql bio on bio.id = hires.id\n",
    "              Where hires.hired = 1\n",
    "              group by bio.service_branch__c\n",
    "              order by hired asc\n",
    "              \"\"\").show()\n",
    "\n",
    "#Not hired by Service Branch\n",
    "spark.sql(\"\"\" Select count(hires.id) as Not_hired,bio.service_branch__c \n",
    "              From dfHire_sql hires \n",
    "              inner join dfBio_sql bio on bio.id = hires.id\n",
    "              Where hires.hired = 0\n",
    "              group by bio.service_branch__c\n",
    "              order by Not_hired asc\n",
    "              \"\"\").show()\n",
    "\n",
    "#Hired by rank\n",
    "spark.sql(\"\"\" Select count(hires.id) as hired,bio.service_rank__c\n",
    "              From dfHire_sql hires \n",
    "              inner join dfBio_sql bio on bio.id = hires.id\n",
    "              Where hires.hired = 0\n",
    "              group by bio.service_rank__c\n",
    "              order by hired desc\n",
    "              \"\"\").show()\n",
    "\n",
    "#Prefered method of contact for the hired\n",
    "spark.sql(\"\"\" select count(com.id) as count, com.preferred_method_of_contact__c \n",
    "              from dfCom_sql com \n",
    "              inner join dfHire_sql hires on com.id = hires.id\n",
    "              where hires.hired = 1\n",
    "              group by com.preferred_method_of_contact__c \n",
    "              order by count\n",
    "          \"\"\").show()\n",
    "\n",
    "#Prefered method of contact for the Unhired\n",
    "spark.sql(\"\"\" select count(com.id) as count, com.preferred_method_of_contact__c \n",
    "              from dfCom_sql com \n",
    "              inner join dfHire_sql hires on com.id = hires.id\n",
    "              where hires.hired = 0\n",
    "              group by com.preferred_method_of_contact__c \n",
    "              order by count\n",
    "          \"\"\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"DatawarehouseSpark Application\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "from pyspark.sql.functions import col, regexp_replace, split, udf\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "from pyspark.ml.feature import FeatureHasher\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import IDF\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import PCA\n",
    "from datetime import datetime\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml import Pipeline\n",
    "import re\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputFileHire ='/home/mose/Downloads/CSVS/client_hiring_dt.csv' #\"/user/user17/mosedata_proj/input/client_hiring_dt.csv\"\n",
    "inputFileBio ='/home/mose/Downloads/CSVS/client_bio_dt.csv'  #\"/user/user17/mosedata_proj/input/client_bio_dt.csv\"\n",
    "inputFileCom = '/home/mose/Downloads/CSVS/client_communication_dt.csv' #\"/user/user17/mosedata_proj/input/client_communication_dt.csv\"\n",
    "inputFileAct = '/home/mose/Downloads/CSVS/client_activities_dt.csv' #\"/user/user17/mosedata_proj/input/client_activities_dt.csv\"\n",
    "inputFileFact = '/home/mose/Downloads/CSVS/client_fact_ft.csv' #\"/user/user17/mosedata_proj/input/client_fact_ft.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame from CSV file\n",
    "dfHire =  spark.read.format(\"csv\").option(\"sep\", \",\").option(\"inferSchema\", \"true\").option(\"header\", \"true\").load(inputFileHire)\n",
    "dfBio =  spark.read.format(\"csv\").option(\"sep\", \",\").option(\"inferSchema\", \"true\").option(\"header\", \"true\").load(inputFileBio)\n",
    "dfCom =  spark.read.format(\"csv\").option(\"sep\", \",\").option(\"inferSchema\", \"true\").option(\"header\", \"true\").load(inputFileCom)\n",
    "dfAct =  spark.read.format(\"csv\").option(\"sep\", \",\").option(\"inferSchema\", \"true\").option(\"header\", \"true\").load(inputFileAct)\n",
    "dfFact =  spark.read.format(\"csv\").option(\"sep\", \",\").option(\"inferSchema\", \"true\").option(\"header\", \"true\").load(inputFileFact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- dsubimittedforhire: string (nullable = true)\n",
      " |-- dconfirmedforhire: string (nullable = true)\n",
      " |-- difsubmitconfirmed: integer (nullable = true)\n",
      " |-- startdate: string (nullable = true)\n",
      " |-- datdifconfirstart: integer (nullable = true)\n",
      " |-- job_type__c: string (nullable = true)\n",
      " |-- job_function_hired_in__c: string (nullable = true)\n",
      " |-- hiring_account__c: string (nullable = true)\n",
      " |-- hire_heroes_usa_confirmed_hire__c: integer (nullable = true)\n",
      " |-- hired_location__c: string (nullable = true)\n",
      " |-- hired_zip_code__c: string (nullable = true)\n",
      " |-- industry_hired_in__c: string (nullable = true)\n",
      " |-- hired_but_still_active_and_looking__c: string (nullable = true)\n",
      " |-- hired: string (nullable = true)\n",
      " |-- client_hiring_dt_id: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- client__c: integer (nullable = true)\n",
      " |-- client_type__c: string (nullable = true)\n",
      " |-- service_rank__c: string (nullable = true)\n",
      " |-- service_branch__c: string (nullable = true)\n",
      " |-- primary_military_occupational_specialty__c: string (nullable = true)\n",
      " |-- gender__c: string (nullable = true)\n",
      " |-- highest_level_of_education_completed__c: string (nullable = true)\n",
      " |-- status__c: string (nullable = true)\n",
      " |-- military_spouse_caregiver__c: string (nullable = true)\n",
      " |-- acount_createddate: string (nullable = true)\n",
      " |-- dateofserviceentry: string (nullable = true)\n",
      " |-- dateofserviceseparation: string (nullable = true)\n",
      " |-- client_bio_dt_id: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- mailingstate: string (nullable = true)\n",
      " |-- mailingpostalcode: string (nullable = true)\n",
      " |-- mailingcountry: string (nullable = true)\n",
      " |-- preferred_method_of_contact__c: string (nullable = true)\n",
      " |-- responsive__c: integer (nullable = true)\n",
      " |-- active__c: string (nullable = true)\n",
      " |-- client_communication_dt_id: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- create_ddate: string (nullable = true)\n",
      " |-- dateassignedtohhusa: string (nullable = true)\n",
      " |-- dateassignedtostaff: string (nullable = true)\n",
      " |-- dateinitialassesment: string (nullable = true)\n",
      " |-- dateresumecompleted: string (nullable = true)\n",
      " |-- dsubimittedforhire: string (nullable = true)\n",
      " |-- confirmedhiredate: string (nullable = true)\n",
      " |-- startdate: string (nullable = true)\n",
      " |-- resume_completed_by__c: string (nullable = true)\n",
      " |-- resume_tailoring_tips__c: integer (nullable = true)\n",
      " |-- finalized_hhusa_revised_resume_on_file__c: integer (nullable = true)\n",
      " |-- created_linkedin_account__c: integer (nullable = true)\n",
      " |-- client_act_dt_id: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- yearsinservice: double (nullable = true)\n",
      " |-- ringdna100__email_attempts__c: integer (nullable = true)\n",
      " |-- ringdna100__call_attempts__c: integer (nullable = true)\n",
      " |-- client_act_dt_id: string (nullable = true)\n",
      " |-- client_bio_dt_id: string (nullable = true)\n",
      " |-- client_communication_dt_id: string (nullable = true)\n",
      " |-- client_hiring_dt_id: string (nullable = true)\n",
      " |-- reg_afterservice_months: double (nullable = true)\n",
      " |-- reg_afterservice_years: double (nullable = true)\n",
      " |-- monthsinservice: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the schema of the DataFrames\n",
    "dfHire.printSchema() \n",
    "dfBio.printSchema()\n",
    "dfCom.printSchema()\n",
    "dfAct.printSchema()\n",
    "dfFact.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+\n",
      "|service_branch__c|count|\n",
      "+-----------------+-----+\n",
      "|          Marines|13691|\n",
      "|           Spouse|    2|\n",
      "|  Merchant Marine|    1|\n",
      "|             null|37153|\n",
      "|        Air Force|13416|\n",
      "|             Army|51770|\n",
      "|      Coast Guard|  837|\n",
      "|   Not Applicable|    4|\n",
      "|             Navy|15567|\n",
      "+-----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Clients from different service branches\n",
    "dfBio.groupBy('service_branch__c').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+------+\n",
      "|job_function_hired_in__c| count|\n",
      "+------------------------+------+\n",
      "|    IT - Help Desk/Su...|   287|\n",
      "|    Skilled Labor/Trades|   484|\n",
      "|    Purchasing/Procur...|    73|\n",
      "|                   Sales|   697|\n",
      "|    Firefighter/EMT/E...|    71|\n",
      "|                 Science|    28|\n",
      "|    Training/Instruct...|  1113|\n",
      "|    Supply Chain/Logi...|   892|\n",
      "|             Engineering|   465|\n",
      "|    Media/Journalism/...|    54|\n",
      "|         Banking/Finance|   522|\n",
      "|              Healthcare|   900|\n",
      "|    Quality Assurance...|   238|\n",
      "|              Accounting|   171|\n",
      "|    IT - Computer Sci...|    41|\n",
      "|                    null|109255|\n",
      "|    Business Development|   196|\n",
      "|    Facilities Manage...|   165|\n",
      "|              Consultant|   456|\n",
      "|    Installation/Main...|  1223|\n",
      "+------------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Types of jobs the guys got hired in\n",
    "dfHire.groupBy('job_function_hired_in__c').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Temporary objects from which sql statements are run for each dataframe\n",
    "dfHire.createOrReplaceTempView(\"dfHire_sql\")\n",
    "dfBio.createOrReplaceTempView(\"dfBio_sql\")\n",
    "dfCom.createOrReplaceTempView(\"dfCom_sql\")\n",
    "dfAct.createOrReplaceTempView(\"dfAct_sql\")\n",
    "dfFact.createOrReplaceTempView(\"dfFAct_sql\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------+-----------------+--------------+----------------------+-------------+-----------+\n",
      "|hired|service_rank__c|service_branch__c|yearsinservice|reg_afterservice_years|responsive__c|resume_done|\n",
      "+-----+---------------+-----------------+--------------+----------------------+-------------+-----------+\n",
      "|    1|            O-3|             Army|         11.04|                 -0.33|            1|          1|\n",
      "|    1|            E-9|             Army|         30.18|                 -0.15|            1|          1|\n",
      "|    1|            E-4|             Army|           9.8|                 -0.73|            1|          1|\n",
      "|    1|            E-4|             Army|          2.81|                  0.43|            1|          1|\n",
      "|    1|            E-4|          Marines|          4.47|                  4.45|            1|          1|\n",
      "|    1|            O-2|             Army|          4.38|                 -0.51|            1|          1|\n",
      "|    1|            O-3|             Army|         11.03|                 -0.09|            1|          1|\n",
      "|    1|            E-8|             Army|         20.59|                  2.14|            1|          1|\n",
      "|    1|            E-7|             Army|         22.27|                 -0.42|            1|          1|\n",
      "|    1|            E-7|             Army|         20.01|                 -0.53|            1|          1|\n",
      "|    1|            O-3|             Army|          7.89|                 -0.41|            1|          1|\n",
      "|    1|            E-5|             Navy|          7.94|                 -0.31|            1|          1|\n",
      "|    1|            O-3|             Army|         16.89|                  1.54|            1|          1|\n",
      "|    1|            E-9|        Air Force|         29.82|                 -0.62|            1|          1|\n",
      "|    1|            E-7|        Air Force|         20.06|                  3.47|            1|          1|\n",
      "|    1|            W-1|             Army|         41.99|                -23.56|            1|          1|\n",
      "|    1|            O-3|             Army|         20.83|                 -0.82|            1|          1|\n",
      "|    1|            O-3|             Army|          4.08|                  0.03|            1|          1|\n",
      "|    1|            E-6|             Army|           6.1|                 -0.33|            1|          1|\n",
      "|    1|            E-4|             Navy|          7.09|                  8.58|            1|          1|\n",
      "+-----+---------------+-----------------+--------------+----------------------+-------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Mixed information from all 5 tables of the database\n",
    "spark.sql(''' Select hires.hired, bio.service_rank__c , \n",
    "              bio.service_branch__c , fact.yearsinservice, fact.reg_afterservice_years,\n",
    "              com.responsive__c, act.finalized_hhusa_revised_resume_on_file__c as resume_done\n",
    "              from dfHire_sql hires\n",
    "              inner join dfBio_sql bio on bio.id = hires.id \n",
    "              inner join dfFAct_sql fact on fact.id = hires.id\n",
    "              inner join dfCom_sql com on com.id = hires.id\n",
    "              inner join dfAct_sql act on act.id = hires.id\n",
    "              where hires.hired = \"1\"\n",
    "              \n",
    "              ''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|count(id)|month|\n",
      "+---------+-----+\n",
      "|    11982|    1|\n",
      "|     9626|    2|\n",
      "|    10152|    3|\n",
      "|    12579|    4|\n",
      "|    10978|    5|\n",
      "|    10672|    6|\n",
      "|    10815|    7|\n",
      "|    11912|    8|\n",
      "|    10845|    9|\n",
      "|    11866|   10|\n",
      "|    12022|   11|\n",
      "|     8992|   12|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Account create date by month of the year\n",
    "spark.sql( \"\"\"SELECT count(id) , \n",
    "              Extract(Month From create_ddate) as month \n",
    "              From  dfAct_sql  \n",
    "              group by month \n",
    "              ORDER BY month ASC \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------+\n",
      "|count(id)|year_registered|\n",
      "+---------+---------------+\n",
      "|     2825|           2007|\n",
      "|     2571|           2008|\n",
      "|     1769|           2009|\n",
      "|     2255|           2010|\n",
      "|     2732|           2011|\n",
      "|     4258|           2012|\n",
      "|    13128|           2013|\n",
      "|    13593|           2014|\n",
      "|    13248|           2015|\n",
      "|    25175|           2016|\n",
      "|    22933|           2017|\n",
      "|    26448|           2018|\n",
      "|     1506|           2019|\n",
      "+---------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Registration for services by year\n",
    "spark.sql( \"\"\"SELECT count(id) , \n",
    "Extract(Year From create_ddate) as year_registered \n",
    "              From  dfAct_sql  \n",
    "              group by year_registered\n",
    "              ORDER BY year_registered ASC \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------+\n",
      "|hired|service_branch__c|\n",
      "+-----+-----------------+\n",
      "|    1|   Not Applicable|\n",
      "|  170|      Coast Guard|\n",
      "| 1145|             null|\n",
      "| 2423|          Marines|\n",
      "| 2879|        Air Force|\n",
      "| 3055|             Navy|\n",
      "|10417|             Army|\n",
      "+-----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Hired by Service branch\n",
    "spark.sql(\"\"\" Select count(hires.id) as hired,bio.service_branch__c \n",
    "              From dfHire_sql hires \n",
    "              inner join dfBio_sql bio on bio.id = hires.id\n",
    "              Where hires.hired = 1\n",
    "              group by bio.service_branch__c\n",
    "              order by hired asc\n",
    "              \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------+\n",
      "|Not_hired|service_branch__c|\n",
      "+---------+-----------------+\n",
      "|        2|   Not Applicable|\n",
      "|      667|      Coast Guard|\n",
      "|     9382|             null|\n",
      "|    10524|        Air Force|\n",
      "|    11260|          Marines|\n",
      "|    12500|             Navy|\n",
      "|    41314|             Army|\n",
      "+---------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Not hired by Service Branch\n",
    "spark.sql(\"\"\" Select count(hires.id) as Not_hired,bio.service_branch__c \n",
    "              From dfHire_sql hires \n",
    "              inner join dfBio_sql bio on bio.id = hires.id\n",
    "              Where hires.hired = 0\n",
    "              group by bio.service_branch__c\n",
    "              order by Not_hired asc\n",
    "              \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------+\n",
      "|hired|service_rank__c|\n",
      "+-----+---------------+\n",
      "| 3807|            E-5|\n",
      "| 3683|            E-4|\n",
      "| 2795|            E-7|\n",
      "| 2775|            E-6|\n",
      "| 1281|            E-8|\n",
      "| 1156|           null|\n",
      "| 1142|            O-3|\n",
      "|  648|            E-3|\n",
      "|  564|            O-4|\n",
      "|  510|            O-5|\n",
      "|  463|            E-9|\n",
      "|  294|            O-2|\n",
      "|  186|            O-6|\n",
      "|  178|            W-3|\n",
      "|  148|            W-4|\n",
      "|  137|            W-2|\n",
      "|  126|            E-2|\n",
      "|   79|            O-1|\n",
      "|   75|            E-1|\n",
      "|   27|            W-5|\n",
      "+-----+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Hired by rank\n",
    "spark.sql(\"\"\" Select count(hires.id) as hired,bio.service_rank__c\n",
    "              From dfHire_sql hires \n",
    "              inner join dfBio_sql bio on bio.id = hires.id\n",
    "              Where hires.hired = 1\n",
    "              group by bio.service_rank__c\n",
    "              order by hired desc\n",
    "              \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------------------+\n",
      "|count|preferred_method_of_contact__c|\n",
      "+-----+------------------------------+\n",
      "|    7|                          Mail|\n",
      "|   23|                      LinkedIn|\n",
      "|  158|                          Text|\n",
      "| 1892|                          null|\n",
      "| 2626|                        E-Mail|\n",
      "|15384|                     Telephone|\n",
      "+-----+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Prefered method of contact for the hired\n",
    "spark.sql(\"\"\" select count(com.id) as count, com.preferred_method_of_contact__c \n",
    "              from dfCom_sql com \n",
    "              inner join dfHire_sql hires on com.id = hires.id\n",
    "              where hires.hired = 1\n",
    "              group by com.preferred_method_of_contact__c \n",
    "              order by count\n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------------------+\n",
      "|count|preferred_method_of_contact__c|\n",
      "+-----+------------------------------+\n",
      "|   28|                          Mail|\n",
      "|   33|                      LinkedIn|\n",
      "|  378|                          Text|\n",
      "|12405|                        E-Mail|\n",
      "|31506|                          null|\n",
      "|41299|                     Telephone|\n",
      "+-----+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Prefered method of contact for the Unhired\n",
    "spark.sql(\"\"\" select count(com.id) as count, com.preferred_method_of_contact__c \n",
    "              from dfCom_sql com \n",
    "              inner join dfHire_sql hires on com.id = hires.id\n",
    "              where hires.hired = 0\n",
    "              group by com.preferred_method_of_contact__c \n",
    "              order by count\n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[S_years: double, R_years: double, Resume_Tips: int, Resume_OnFile: int, Client: int, Client_Type: string, Service_R: string, Service_B: string, HHUSA_hire: int, Com_Method: string, responsive__c: int, active__c: string, Created_Linkedin: int, Educ: string, hired: string, Occupation: string]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Select variables that determine whether one is going to be employed or not, store those varibles in a new dataframe\n",
    "dfspark= spark.sql(\"\"\" select fact.yearsinservice as S_years , fact.reg_afterservice_years as R_years,\n",
    "               act.resume_tailoring_tips__c as Resume_Tips,act.finalized_hhusa_revised_resume_on_file__c as Resume_OnFile, bio.client__c as Client, bio.client_type__c as Client_Type, bio.service_rank__c as Service_R\n",
    "              ,bio.service_branch__c as Service_B, hire.hire_heroes_usa_confirmed_hire__c as HHUSA_hire, com.preferred_method_of_contact__c as Com_Method, com.responsive__c\n",
    "              ,com.active__c, act.created_linkedin_account__c as Created_Linkedin, bio.highest_level_of_education_completed__c as Educ,  hire.hired\n",
    "              ,bio.primary_military_occupational_specialty__c as Occupation\n",
    "               \n",
    "              from dfFact_sql fact\n",
    "              \n",
    "              inner join dfAct_sql act on act.id = fact.id\n",
    "              inner join dfBio_sql bio on bio.id = fact.id\n",
    "              inner join dfHire_sql hire on hire.id = fact.id\n",
    "              inner join dfCom_sql com on com.id = fact.id\n",
    "              \n",
    "              where bio.client_type__c != '' and bio.service_branch__c !='' and bio.service_rank__c !='' \n",
    "              and com.preferred_method_of_contact__c !='' and fact.yearsinservice < 53 and fact.yearsinservice > 0 \n",
    "              and fact.reg_afterservice_years >- 32 and fact.reg_afterservice_years < 53 and com.active__c !=''\n",
    "              and bio.highest_level_of_education_completed__c !='' and hire.hired  !='' and hire.hired !='No' and hire.hired != 3\n",
    "\n",
    "          \"\"\")\n",
    "dfspark.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51093"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the number of clients in the description above\n",
    "dfspark.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the data type of the the hired column from string to double\n",
    "dfspark = dfspark.withColumn(\"hired\", dfspark[\"hired\"].cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|hired|\n",
      "+-----+\n",
      "|  0.0|\n",
      "|  1.0|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Confirm the labels in the hired column 0 represents the client was not hired 1 represents they were hired. \n",
    "dfspark.select('hired').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('S_years', 'double'),\n",
       " ('R_years', 'double'),\n",
       " ('Resume_Tips', 'int'),\n",
       " ('Resume_OnFile', 'int'),\n",
       " ('Client', 'int'),\n",
       " ('Client_Type', 'string'),\n",
       " ('Service_R', 'string'),\n",
       " ('Service_B', 'string'),\n",
       " ('HHUSA_hire', 'int'),\n",
       " ('Com_Method', 'string'),\n",
       " ('responsive__c', 'int'),\n",
       " ('active__c', 'string'),\n",
       " ('Created_Linkedin', 'int'),\n",
       " ('Educ', 'string'),\n",
       " ('hired', 'double'),\n",
       " ('Occupation', 'string')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The data types of the data frame \n",
    "dfspark.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----------+-------------+------+-----------------+---------+---------+----------+----------+-------------+---------+----------------+---------------+-----+----------+--------------------+\n",
      "|S_years|R_years|Resume_Tips|Resume_OnFile|Client|      Client_Type|Service_R|Service_B|HHUSA_hire|Com_Method|responsive__c|active__c|Created_Linkedin|           Educ|hired|Occupation|            features|\n",
      "+-------+-------+-----------+-------------+------+-----------------+---------+---------+----------+----------+-------------+---------+----------------+---------------+-----+----------+--------------------+\n",
      "|   8.11|  -0.37|          1|            1|     1|Online Registrant|      E-5|     Army|         0| Telephone|            1|      Yes|               1|High School/GED|  0.0|       91F|(262144,[52387,65...|\n",
      "+-------+-------+-----------+-------------+------+-----------------+---------+---------+----------+----------+-------------+---------+----------------+---------------+-----+----------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Hash the features through a hash function to convert the features into vectors for the dfspark dataframe,\n",
    "# Convert strings to numerical data, the output column is features. The target is not included in this conversion.\n",
    "funchash = FeatureHasher(inputCols= ['S_years','R_years','Resume_Tips','Resume_OnFile','Client','Client_Type','Service_R',\\\n",
    " 'Service_B','HHUSA_hire','Com_Method','responsive__c','active__c','Created_Linkedin','Educ','Occupation'],\n",
    "                       outputCol=\"features\")\n",
    "\n",
    "dffeatures = funchash.transform(dfspark)\n",
    "dffeatures.persist()\n",
    "dffeatures.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(S_years=8.11, R_years=-0.37, Resume_Tips=1, Resume_OnFile=1, Client=1, Client_Type='Online Registrant', Service_R='E-5', Service_B='Army', HHUSA_hire=0, Com_Method='Telephone', responsive__c=1, active__c='Yes', Created_Linkedin=1, Educ='High School/GED', hired=0.0, Occupation='91F', features=SparseVector(262144, {52387: 1.0, 65085: 8.11, 69515: 1.0, 81887: 1.0, 119601: 1.0, 132517: 1.0, 138738: 1.0, 166656: -0.37, 207405: 1.0, 220379: 1.0, 230454: 1.0, 239098: 1.0, 246347: 1.0, 249041: 1.0, 251060: 0.0}))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dffeatures.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o164.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 77.0 failed 1 times, most recent failure: Lost task 0.0 in stage 77.0 (TID 2012, mose-Inspiron-5520.lan, executor driver): java.lang.IllegalArgumentException: axpy only supports adding to a dense vector but got type class org.apache.spark.mllib.linalg.SparseVector.\n\tat org.apache.spark.mllib.linalg.BLAS$.axpy(BLAS.scala:68)\n\tat org.apache.spark.mllib.feature.PCA.$anonfun$fit$3(PCA.scala:52)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n\tat scala.collection.Iterator$SliceIterator.next(Iterator.scala:271)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1429)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1429)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1429)\n\tat org.apache.spark.rdd.RDD.$anonfun$take$2(RDD.scala:1423)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2133)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2133)\n\tat org.apache.spark.rdd.RDD.$anonfun$take$1(RDD.scala:1423)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.RDD.take(RDD.scala:1396)\n\tat org.apache.spark.rdd.RDD.$anonfun$first$1(RDD.scala:1437)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.RDD.first(RDD.scala:1437)\n\tat org.apache.spark.mllib.linalg.distributed.RowMatrix.numCols(RowMatrix.scala:62)\n\tat org.apache.spark.mllib.linalg.distributed.RowMatrix.computePrincipalComponentsAndExplainedVariance(RowMatrix.scala:467)\n\tat org.apache.spark.mllib.feature.PCA.fit(PCA.scala:65)\n\tat org.apache.spark.ml.feature.PCA.fit(PCA.scala:93)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.IllegalArgumentException: axpy only supports adding to a dense vector but got type class org.apache.spark.mllib.linalg.SparseVector.\n\tat org.apache.spark.mllib.linalg.BLAS$.axpy(BLAS.scala:68)\n\tat org.apache.spark.mllib.feature.PCA.$anonfun$fit$3(PCA.scala:52)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n\tat scala.collection.Iterator$SliceIterator.next(Iterator.scala:271)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1429)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1429)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1429)\n\tat org.apache.spark.rdd.RDD.$anonfun$take$2(RDD.scala:1423)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2133)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-c22fa21498fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#pca = PCA(k=6, inputCol=\"features\", outputCol=\"pcaFeatures\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodelpca\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdffeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    127\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    316\u001b[0m         \"\"\"\n\u001b[1;32m    317\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o164.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 77.0 failed 1 times, most recent failure: Lost task 0.0 in stage 77.0 (TID 2012, mose-Inspiron-5520.lan, executor driver): java.lang.IllegalArgumentException: axpy only supports adding to a dense vector but got type class org.apache.spark.mllib.linalg.SparseVector.\n\tat org.apache.spark.mllib.linalg.BLAS$.axpy(BLAS.scala:68)\n\tat org.apache.spark.mllib.feature.PCA.$anonfun$fit$3(PCA.scala:52)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n\tat scala.collection.Iterator$SliceIterator.next(Iterator.scala:271)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1429)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1429)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1429)\n\tat org.apache.spark.rdd.RDD.$anonfun$take$2(RDD.scala:1423)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2133)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2133)\n\tat org.apache.spark.rdd.RDD.$anonfun$take$1(RDD.scala:1423)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.RDD.take(RDD.scala:1396)\n\tat org.apache.spark.rdd.RDD.$anonfun$first$1(RDD.scala:1437)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.RDD.first(RDD.scala:1437)\n\tat org.apache.spark.mllib.linalg.distributed.RowMatrix.numCols(RowMatrix.scala:62)\n\tat org.apache.spark.mllib.linalg.distributed.RowMatrix.computePrincipalComponentsAndExplainedVariance(RowMatrix.scala:467)\n\tat org.apache.spark.mllib.feature.PCA.fit(PCA.scala:65)\n\tat org.apache.spark.ml.feature.PCA.fit(PCA.scala:93)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.IllegalArgumentException: axpy only supports adding to a dense vector but got type class org.apache.spark.mllib.linalg.SparseVector.\n\tat org.apache.spark.mllib.linalg.BLAS$.axpy(BLAS.scala:68)\n\tat org.apache.spark.mllib.feature.PCA.$anonfun$fit$3(PCA.scala:52)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n\tat scala.collection.Iterator$SliceIterator.next(Iterator.scala:271)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1429)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1429)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1429)\n\tat org.apache.spark.rdd.RDD.$anonfun$take$2(RDD.scala:1423)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2133)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "#pca = PCA(k=6, inputCol=\"features\", outputCol=\"pcaFeatures\")\n",
    "modelpca = pca.fit(dffeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['S_years',\n",
       " 'R_years',\n",
       " 'Resume_Tips',\n",
       " 'Resume_OnFile',\n",
       " 'Client',\n",
       " 'Client_Type',\n",
       " 'Service_R',\n",
       " 'Service_B',\n",
       " 'HHUSA_hire',\n",
       " 'Com_Method',\n",
       " 'responsive__c',\n",
       " 'active__c',\n",
       " 'Created_Linkedin',\n",
       " 'Educ',\n",
       " 'hired',\n",
       " 'Occupation',\n",
       " 'features']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dffeatures.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|hired|\n",
      "+--------------------+-----+\n",
      "|(262144,[52387,65...|  0.0|\n",
      "|(262144,[52387,65...|  0.0|\n",
      "|(262144,[65085,69...|  0.0|\n",
      "|(262144,[65085,69...|  0.0|\n",
      "|(262144,[65085,69...|  0.0|\n",
      "|(262144,[64366,65...|  0.0|\n",
      "|(262144,[54411,65...|  0.0|\n",
      "|(262144,[65085,69...|  0.0|\n",
      "|(262144,[65085,69...|  0.0|\n",
      "|(262144,[65085,69...|  0.0|\n",
      "|(262144,[11074,65...|  0.0|\n",
      "|(262144,[52387,65...|  0.0|\n",
      "|(262144,[40267,65...|  0.0|\n",
      "|(262144,[65085,69...|  0.0|\n",
      "|(262144,[52387,65...|  0.0|\n",
      "|(262144,[65085,69...|  0.0|\n",
      "|(262144,[48884,52...|  0.0|\n",
      "|(262144,[52387,65...|  0.0|\n",
      "|(262144,[65085,69...|  0.0|\n",
      "|(262144,[65085,69...|  0.0|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Confirm that the dataframe is active, We are going to use this data to train our machine learning algorithms.\n",
    "dffeatures.select('features','hired').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the data into training and testing data for the training of the algorithms and testing of the algorithms \n",
    "A,B=dffeatures.randomSplit([0.1,0.9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,test_data=A.randomSplit([0.65,0.35])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3328"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "|(262144,[52387,65...|\n",
      "|(262144,[52387,65...|\n",
      "|(262144,[65085,69...|\n",
      "|(262144,[65085,69...|\n",
      "|(262144,[65085,69...|\n",
      "|(262144,[64366,65...|\n",
      "|(262144,[54411,65...|\n",
      "|(262144,[65085,69...|\n",
      "|(262144,[65085,69...|\n",
      "|(262144,[65085,69...|\n",
      "|(262144,[11074,65...|\n",
      "|(262144,[52387,65...|\n",
      "|(262144,[40267,65...|\n",
      "|(262144,[65085,69...|\n",
      "|(262144,[52387,65...|\n",
      "|(262144,[65085,69...|\n",
      "|(262144,[48884,52...|\n",
      "|(262144,[52387,65...|\n",
      "|(262144,[65085,69...|\n",
      "|(262144,[65085,69...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dffeatures.select('features').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyspark.ml.linalg import SparseVector\n",
    "from pyspark.ml.feature import Normalizer\n",
    "\n",
    "Norm = Normalizer(inputCol=\"features\", outputCol=\"features_norm\", p=1)\n",
    "dffeatures_norm = Norm.transform(dffeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|features                                                                                                                                                                        |\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|(262144,[52387,65085,69515,81887,119601,132517,138738,166656,207405,220379,230454,239098,246347,249041,251060],[1.0,8.11,1.0,1.0,1.0,1.0,1.0,-0.37,1.0,1.0,1.0,1.0,1.0,1.0,0.0])|\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dffeatures_norm.select('features').show(1,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['S_years',\n",
       " 'R_years',\n",
       " 'Resume_Tips',\n",
       " 'Resume_OnFile',\n",
       " 'Client',\n",
       " 'Client_Type',\n",
       " 'Service_R',\n",
       " 'Service_B',\n",
       " 'HHUSA_hire',\n",
       " 'Com_Method',\n",
       " 'responsive__c',\n",
       " 'active__c',\n",
       " 'Created_Linkedin',\n",
       " 'Educ',\n",
       " 'hired',\n",
       " 'Occupation',\n",
       " 'features',\n",
       " 'features_norm']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dffeatures_norm.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,test_data=dffeatures_norm.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the decision tree classifier, to predict whether one is hired or not, using the hired column and features. \n",
    "def treedec(label, features):\n",
    "    decision_tree = DecisionTreeClassifier(labelCol=label, featuresCol=features)\n",
    "    return decision_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration it takes to run the algorithm: 0:00:00.000186\n"
     ]
    }
   ],
   "source": [
    "# Now let us train the model, while timing how long it takes run this task. \n",
    "\n",
    "# Start time of training the model \n",
    "start = datetime.now()\n",
    "def traindec_tree(train_data, algo): \n",
    "    # The model \n",
    "    model1 = algo.fit(train_data)\n",
    "    return model1\n",
    "# Print the duration\n",
    "#traindec_tree(train_data, treedec('hired','features'))\n",
    "#End of time of the model \n",
    "end = datetime.now()\n",
    "print('Duration it takes to run the algorithm: {}'.format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions using the test set of the dataset ... Store the predictions in a new dataframe prediction \n",
    "predictions = traindec_tree(train_data, treedec('hired','features_norm')).transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+--------------------+\n",
      "|prediction|hired|            features|\n",
      "+----------+-----+--------------------+\n",
      "|       1.0|  1.0|(262144,[52387,65...|\n",
      "|       0.0|  0.0|(262144,[65085,69...|\n",
      "|       0.0|  0.0|(262144,[65085,69...|\n",
      "|       1.0|  0.0|(262144,[65085,69...|\n",
      "|       0.0|  0.0|(262144,[52387,65...|\n",
      "|       0.0|  0.0|(262144,[48884,65...|\n",
      "|       1.0|  1.0|(262144,[52387,65...|\n",
      "|       0.0|  0.0|(262144,[65085,69...|\n",
      "|       1.0|  1.0|(262144,[65085,69...|\n",
      "|       1.0|  1.0|(262144,[65085,69...|\n",
      "|       1.0|  1.0|(262144,[65085,69...|\n",
      "|       1.0|  1.0|(262144,[65085,69...|\n",
      "|       1.0|  1.0|(262144,[65085,69...|\n",
      "|       0.0|  0.0|(262144,[65085,69...|\n",
      "|       0.0|  0.0|(262144,[65085,68...|\n",
      "|       0.0|  0.0|(262144,[19683,65...|\n",
      "|       0.0|  0.0|(262144,[48884,65...|\n",
      "|       1.0|  1.0|(262144,[65085,69...|\n",
      "|       0.0|  0.0|(262144,[65085,69...|\n",
      "|       1.0|  1.0|(262144,[65085,69...|\n",
      "+----------+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check out the new dataframe predictions to view the prediction, hired and features columns \n",
    "predictions.select(\"prediction\", \"hired\", \"features\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets evaluate the algorithm performance\n",
    "def evaluate_algo(df,label,Algorithm_name  ):\n",
    "    classification_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=label, predictionCol=\"prediction\")\n",
    "    accuracy = classification_evaluator.evaluate(df)\n",
    "    print(Algorithm_name )\n",
    "    print(\"Test Error  = %g \" % (1.0 - accuracy))\n",
    "    print(\"Accuracy   = %g \" % accuracy)\n",
    "\n",
    "#evaluate_algo(predictions,'hired','Decision Tree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration: 0:00:00.140405\n"
     ]
    }
   ],
   "source": [
    "# Train a RandomForest model.\n",
    "from datetime import datetime\n",
    "\n",
    "start_time = datetime.now()\n",
    "# do your work here\n",
    "rf = RandomForestClassifier(labelCol=\"hired\", featuresCol=\"features\", numTrees=17)\n",
    "end_time = datetime.now()\n",
    "\n",
    "print('Duration: {}'.format(end_time - start_time))\n",
    "\n",
    "def randomforest(label, features, numTrees):\n",
    "    \n",
    "    random_forest = RandomForestClassifier(labelCol=label, featuresCol=features, numTrees=numTrees)\n",
    "    \n",
    "    return random_forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration: 0:00:00.000133\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "start_time = datetime.now()\n",
    "# do your work here\n",
    "#modelrf = rf.fit(train_data)\n",
    "\n",
    "def random_forestfit(randf,train_data):\n",
    "    \"\"\" Takes in a dataframe, and a random forest model \"\"\"\n",
    "    model= randf.fit(train_data)\n",
    "    return model\n",
    "    \n",
    "\n",
    "end_time = datetime.now()\n",
    "\n",
    "print('Duration: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions.\n",
    "predictions2 = random_forestfit(randomforest('hired', 'features', 17),train_data).transform(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+--------------------+\n",
      "|prediction|hired|            features|\n",
      "+----------+-----+--------------------+\n",
      "|       0.0|  1.0|(262144,[52387,65...|\n",
      "|       0.0|  0.0|(262144,[65085,69...|\n",
      "|       0.0|  0.0|(262144,[65085,69...|\n",
      "|       0.0|  0.0|(262144,[65085,69...|\n",
      "|       0.0|  0.0|(262144,[52387,65...|\n",
      "|       0.0|  0.0|(262144,[48884,65...|\n",
      "|       0.0|  1.0|(262144,[52387,65...|\n",
      "|       0.0|  0.0|(262144,[65085,69...|\n",
      "|       0.0|  1.0|(262144,[65085,69...|\n",
      "|       0.0|  1.0|(262144,[65085,69...|\n",
      "|       0.0|  1.0|(262144,[65085,69...|\n",
      "|       0.0|  1.0|(262144,[65085,69...|\n",
      "|       0.0|  1.0|(262144,[65085,69...|\n",
      "|       0.0|  0.0|(262144,[65085,69...|\n",
      "|       0.0|  0.0|(262144,[65085,68...|\n",
      "|       0.0|  0.0|(262144,[19683,65...|\n",
      "|       0.0|  0.0|(262144,[48884,65...|\n",
      "|       0.0|  1.0|(262144,[65085,69...|\n",
      "|       0.0|  0.0|(262144,[65085,69...|\n",
      "|       0.0|  1.0|(262144,[65085,69...|\n",
      "+----------+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select example rows to display.\n",
    "predictions2.select('prediction','hired','features').show()  #.select(\"prediction\", \"indexedLabel\", \"features\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Test Error  = 0.454042 \n",
      "Accuracy   = 0.545958 \n"
     ]
    }
   ],
   "source": [
    "evaluate_algo(predictions2,'hired','Random Forest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster(k,df,features):\n",
    "    \"\"\" Takes in k as an int, the dataframe, and features \"\"\"\n",
    "    kmeans = KMeans(k=k, seed=1)  # 2 clusters here\n",
    "    model = kmeans.fit(df.select(features))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformeddf = cluster(4,dffeatures,'features').transform(dffeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|prediction|\n",
      "+----------+\n",
      "|         1|\n",
      "|         3|\n",
      "|         2|\n",
      "|         0|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformeddf.select('prediction').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+-----+----------+\n",
      "|  Service_B|Service_R|hired|prediction|\n",
      "+-----------+---------+-----+----------+\n",
      "|  Air Force|      O-4|  0.0|         1|\n",
      "|       Navy|      E-1|  0.0|         3|\n",
      "|  Air Force|      E-8|  1.0|         3|\n",
      "|       Army|      O-4|  1.0|         1|\n",
      "|Coast Guard|      O-5|  0.0|         1|\n",
      "|  Air Force|      E-5|  0.0|         2|\n",
      "|       Army|      O-7|  0.0|         1|\n",
      "|       Army|     CW-2|  0.0|         1|\n",
      "|       Navy|      O-2|  1.0|         3|\n",
      "|       Army|      W-3|  1.0|         3|\n",
      "|    Marines|      O-2|  1.0|         3|\n",
      "|Coast Guard|      O-3|  1.0|         3|\n",
      "|  Air Force|      O-6|  0.0|         1|\n",
      "|  Air Force|      O-4|  1.0|         3|\n",
      "|       Army|      O-1|  0.0|         1|\n",
      "|       Army|      O-3|  1.0|         1|\n",
      "|  Air Force|      E-3|  0.0|         0|\n",
      "|       Navy|      O-4|  0.0|         2|\n",
      "|  Air Force|      E-5|  1.0|         2|\n",
      "|       Army|      E-9|  0.0|         1|\n",
      "+-----------+---------+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformeddf.select('Service_B','Service_R','hired','prediction').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+\n",
      "|summary|          S_years|           R_years|\n",
      "+-------+-----------------+------------------+\n",
      "|  count|            51093|             51093|\n",
      "|   mean|13.25944708668499|1.9484418609202803|\n",
      "| stddev| 8.55804960106253| 6.346271266872357|\n",
      "|    min|             0.01|            -29.16|\n",
      "|    max|            52.33|             52.58|\n",
      "+-------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Information about individual clusters in term of years service men stay in the army and when they register for services\n",
    "transformeddf.select('S_years','R_years').describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get cluster information Cluster 0 in terms of years service men spend on service. \n",
    "cluster0 = transformeddf.filter(transformeddf.prediction==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+\n",
      "|summary|           S_years|           R_years|\n",
      "+-------+------------------+------------------+\n",
      "|  count|              2925|              2925|\n",
      "|   mean| 5.224564102564105|22.217131623931618|\n",
      "| stddev|2.9455794494751912| 8.277066032403162|\n",
      "|    min|              0.01|             11.33|\n",
      "|    max|             21.19|             52.58|\n",
      "+-------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cluster0.select('S_years','R_years').describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get cluster information Cluster 1\n",
    "cluster1 = transformeddf.filter(transformeddf.prediction==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+-------------------+\n",
      "|summary|          S_years|            R_years|\n",
      "+-------+-----------------+-------------------+\n",
      "|  count|            18935|              18935|\n",
      "|   mean|22.77949405862156|-0.6843913387906007|\n",
      "| stddev|4.482330273547443| 2.2575649893207146|\n",
      "|    min|             14.0|             -29.16|\n",
      "|    max|            52.33|               7.92|\n",
      "+-------+-----------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cluster1.select('S_years','R_years').describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get cluster information Cluster 2\n",
    "cluster2 = transformeddf.filter(transformeddf.prediction==2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------------+\n",
      "|summary|           S_years|          R_years|\n",
      "+-------+------------------+-----------------+\n",
      "|  count|              1929|             1929|\n",
      "|   mean|19.301316744427176|9.179803006739244|\n",
      "| stddev| 4.354761181256989|4.399028957656696|\n",
      "|    min|              8.92|             2.32|\n",
      "|    max|             39.37|            26.82|\n",
      "+-------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cluster2.select('S_years','R_years').describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get cluster information Cluster 3\n",
    "cluster3 = transformeddf.filter(transformeddf.prediction==3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+\n",
      "|summary|           S_years|           R_years|\n",
      "+-------+------------------+------------------+\n",
      "|  count|             27304|             27304|\n",
      "|   mean| 7.091309698212723|1.0920648989159096|\n",
      "| stddev|3.2336080116487533|3.1435676760042863|\n",
      "|    min|              0.01|            -10.07|\n",
      "|    max|             15.08|             11.83|\n",
      "+-------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cluster3.select('S_years','R_years').describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting largescale.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile largescale.py\n",
    "\n",
    "from datetime import datetime\n",
    "# Start time of training the model \n",
    "start = datetime.now()\n",
    "\n",
    "from pyspark.sql.functions import col, regexp_replace, split\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import FeatureHasher\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.sql.types import *\n",
    "import copy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Large Scale Computing Application \") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "# The dataset \n",
    "inputFileHire ='/home/mose/Downloads/CSVS/client_hiring_dt.csv' #\"/user/user17/mosedata_proj/input/client_hiring_dt.csv\"\n",
    "inputFileBio ='/home/mose/Downloads/CSVS/client_bio_dt.csv'  #\"/user/user17/mosedata_proj/input/client_bio_dt.csv\"\n",
    "inputFileCom = '/home/mose/Downloads/CSVS/client_communication_dt.csv' #\"/user/user17/mosedata_proj/input/client_communication_dt.csv\"\n",
    "inputFileAct = '/home/mose/Downloads/CSVS/client_activities_dt.csv' #\"/user/user17/mosedata_proj/input/client_activities_dt.csv\"\n",
    "inputFileFact = '/home/mose/Downloads/CSVS/client_fact_ft.csv' #\"/user/user17/mosedata_proj/input/client_fact_ft.csv\"\n",
    "\n",
    "# Create DataFrames from CSV files\n",
    "dfHire =  spark.read.format(\"csv\").option(\"sep\", \",\").option(\"inferSchema\", \"true\").option(\"header\", \"true\").load(inputFileHire)\n",
    "dfBio =  spark.read.format(\"csv\").option(\"sep\", \",\").option(\"inferSchema\", \"true\").option(\"header\", \"true\").load(inputFileBio)\n",
    "dfCom =  spark.read.format(\"csv\").option(\"sep\", \",\").option(\"inferSchema\", \"true\").option(\"header\", \"true\").load(inputFileCom)\n",
    "dfAct =  spark.read.format(\"csv\").option(\"sep\", \",\").option(\"inferSchema\", \"true\").option(\"header\", \"true\").load(inputFileAct)\n",
    "dfFact =  spark.read.format(\"csv\").option(\"sep\", \",\").option(\"inferSchema\", \"true\").option(\"header\", \"true\").load(inputFileFact)\n",
    "\n",
    "# Print the schemas of the DataFrames\n",
    "dfHire.printSchema() \n",
    "dfBio.printSchema()\n",
    "dfCom.printSchema()\n",
    "dfAct.printSchema()\n",
    "dfFact.printSchema()\n",
    "\n",
    "\n",
    "#Clients from different service branches\n",
    "dfBio.groupBy('service_branch__c').count().show()\n",
    "\n",
    "\n",
    "#Types of jobs the guys got hired in\n",
    "dfHire.groupBy('job_function_hired_in__c').count().show()\n",
    "\n",
    "\n",
    "#Temporary objects from which sql statements can be run for each dataframe\n",
    "dfHire.createOrReplaceTempView(\"dfHire_sql\")\n",
    "dfBio.createOrReplaceTempView(\"dfBio_sql\")\n",
    "dfCom.createOrReplaceTempView(\"dfCom_sql\")\n",
    "dfAct.createOrReplaceTempView(\"dfAct_sql\")\n",
    "dfFact.createOrReplaceTempView(\"dfFAct_sql\")\n",
    "\n",
    "#Account create date by month of the year\n",
    "spark.sql( \"\"\"SELECT count(id) , \n",
    "              Extract(Month From create_ddate) as month \n",
    "              From  dfAct_sql  \n",
    "              group by month \n",
    "              ORDER BY month ASC \"\"\").show()\n",
    "\n",
    "#Hired by Service branch\n",
    "spark.sql(\"\"\" Select count(hires.id) as hired,bio.service_branch__c \n",
    "              From dfHire_sql hires \n",
    "              inner join dfBio_sql bio on bio.id = hires.id\n",
    "              Where hires.hired = 1\n",
    "              group by bio.service_branch__c\n",
    "              order by hired asc\n",
    "              \"\"\").show()\n",
    "\n",
    "#Not hired by Service Branch\n",
    "spark.sql(\"\"\" Select count(hires.id) as Not_hired,bio.service_branch__c \n",
    "              From dfHire_sql hires \n",
    "              inner join dfBio_sql bio on bio.id = hires.id\n",
    "              Where hires.hired = 0\n",
    "              group by bio.service_branch__c\n",
    "              order by Not_hired asc\n",
    "              \"\"\").show()\n",
    "\n",
    "#Hired by rank\n",
    "spark.sql(\"\"\" Select count(hires.id) as hired,bio.service_rank__c\n",
    "              From dfHire_sql hires \n",
    "              inner join dfBio_sql bio on bio.id = hires.id\n",
    "              Where hires.hired = 1\n",
    "              group by bio.service_rank__c\n",
    "              order by hired desc\n",
    "              \"\"\").show()\n",
    "\n",
    "#Prefered method of contact for the hired\n",
    "spark.sql(\"\"\" select count(com.id) as count, com.preferred_method_of_contact__c \n",
    "              from dfCom_sql com \n",
    "              inner join dfHire_sql hires on com.id = hires.id\n",
    "              where hires.hired = 1\n",
    "              group by com.preferred_method_of_contact__c \n",
    "              order by count\n",
    "          \"\"\").show()\n",
    "\n",
    "\n",
    "#Prefered method of contact for the Unhired\n",
    "spark.sql(\"\"\" select count(com.id) as count, com.preferred_method_of_contact__c \n",
    "              from dfCom_sql com \n",
    "              inner join dfHire_sql hires on com.id = hires.id\n",
    "              where hires.hired = 0\n",
    "              group by com.preferred_method_of_contact__c \n",
    "              order by count\n",
    "          \"\"\").show()\n",
    "\n",
    "#Select variables that determine whether one is going to be employed or not, store those varibles in a new dataframe\n",
    "dfspark= spark.sql(\"\"\" select fact.yearsinservice as S_years , fact.reg_afterservice_years as R_years,\n",
    "               act.resume_tailoring_tips__c as Resume_Tips,act.finalized_hhusa_revised_resume_on_file__c as Resume_OnFile, bio.client__c as Client, bio.client_type__c as Client_Type, bio.service_rank__c as Service_R\n",
    "              ,bio.service_branch__c as Service_B, hire.hire_heroes_usa_confirmed_hire__c as HHUSA_hire, com.preferred_method_of_contact__c as Com_Method, com.responsive__c\n",
    "              ,com.active__c, act.created_linkedin_account__c as Created_Linkedin, bio.highest_level_of_education_completed__c as Educ,  hire.hired\n",
    "              ,bio.primary_military_occupational_specialty__c as Occupation\n",
    "               \n",
    "              from dfFact_sql fact\n",
    "              \n",
    "              inner join dfAct_sql act on act.id = fact.id\n",
    "              inner join dfBio_sql bio on bio.id = fact.id\n",
    "              inner join dfHire_sql hire on hire.id = fact.id\n",
    "              inner join dfCom_sql com on com.id = fact.id\n",
    "              \n",
    "              where bio.client_type__c != '' and bio.service_branch__c !='' and bio.service_rank__c !='' \n",
    "              and com.preferred_method_of_contact__c !='' and fact.yearsinservice < 53 and fact.yearsinservice > 0 \n",
    "              and fact.reg_afterservice_years >- 32 and fact.reg_afterservice_years < 53 and com.active__c !=''\n",
    "              and bio.highest_level_of_education_completed__c !='' and hire.hired  !='' and hire.hired !='No' and hire.hired != 3\n",
    "\n",
    "          \"\"\")\n",
    "\n",
    "dfsparkcopy = dfspark\n",
    "\n",
    "# Get the number of clients in the description above\n",
    "dfspark.count()\n",
    "\n",
    "\n",
    "\n",
    "# Change the data type of the the hired column from string to double\n",
    "dfspark = dfspark.withColumn(\"hired\", dfspark[\"hired\"].cast(DoubleType()))\n",
    "\n",
    "# Confirm the labels in the hired column 0 represents the client was not hired 1 represents they were hired. \n",
    "dfspark.select('hired').distinct().show()\n",
    "\n",
    "\n",
    "# The data types of the data frame \n",
    "dfspark.dtypes\n",
    "\n",
    "#Hash the features through a hash function to convert the features into vectors for the dfspark dataframe,\n",
    "# Convert strings to numerical data, the output column is features. The target is not included in this conversion.\n",
    "funchash = FeatureHasher(inputCols= ['S_years','R_years','Resume_Tips','Resume_OnFile','Client','Client_Type','Service_R',\\\n",
    " 'Service_B','HHUSA_hire','Com_Method','responsive__c','active__c','Created_Linkedin','Educ','Occupation'],\n",
    "                       outputCol=\"features\")\n",
    "\n",
    "#Add the new added features to the dataframe\n",
    "dffeatures = funchash.transform(dfspark)\n",
    "\n",
    "\n",
    "#Confirm that the dataframe is active, We are going to use this data to train our machine learning algorithms.\n",
    "dffeatures.select('features','hired').show()\n",
    "\n",
    "#Split the data into training and testing data for the training of the algorithms and testing of the algorithms \n",
    "train_data,test_data=dffeatures.randomSplit([0.75,0.25])\n",
    "\n",
    "\n",
    "# Train the decision tree classifier, to predict whether one is hired or not, using the hired column and features. \n",
    "def treedec(label, features):\n",
    "    decision_tree = DecisionTreeClassifier(labelCol=label, featuresCol=features)\n",
    "    return decision_tree\n",
    "\n",
    "\n",
    "def traindec_tree(train_data, algo): \n",
    "    # The model \n",
    "    model1 = algo.fit(train_data)\n",
    "    return model1\n",
    "\n",
    "# Make predictions using the test set of the dataset ... Store the predictions in a new dataframe prediction \n",
    "predictions = traindec_tree(train_data, treedec('hired','features')).transform(test_data)\n",
    "\n",
    "# Check out the new dataframe predictions to view the prediction, hired and features columns \n",
    "predictions.select(\"prediction\", \"hired\", \"features\").show()\n",
    "\n",
    "# Now lets evaluate the algorithm performance\n",
    "def evaluate_algo(df,label,Algorithm_name  ):\n",
    "    classification_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=label, predictionCol=\"prediction\")\n",
    "    accuracy = classification_evaluator.evaluate(df)\n",
    "    print(Algorithm_name )\n",
    "    print(\"Test Error  = %g \" % (1.0 - accuracy))\n",
    "    print(\"Accuracy   = %g \" % accuracy)\n",
    "\n",
    "print(\" \")\n",
    "evaluate_algo(predictions,'hired','Decision Tree')\n",
    "\n",
    "\n",
    "#Random forest\n",
    "def randomforest(label, features, numTrees):\n",
    "    \"\"\" takes in labels, features and number of trees\"\"\"\n",
    "    random_forest = RandomForestClassifier(labelCol=label, featuresCol=features, numTrees=numTrees)\n",
    "    \n",
    "    return random_forest\n",
    "\n",
    "#Random forest fit function \n",
    "def random_forestfit(randf,train_data):\n",
    "    \"\"\" Takes in a dataframe, and a random forest model \"\"\"\n",
    "    model= randf.fit(train_data)\n",
    "    return model\n",
    " \n",
    "\n",
    "# Make predictions.\n",
    "predictions2 = random_forestfit(randomforest('hired', 'features', 17),train_data).transform(test_data)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions2.select('prediction','hired','features').show()\n",
    "\n",
    "print(\" \")\n",
    "evaluate_algo(predictions2,'hired','Random Forest')\n",
    "\n",
    "#Clustering\n",
    "#Hash the features through a hash function to convert the features into vectors for the dfspark dataframe,\n",
    "# Convert strings to numerical data, the output column is features. The target is not included in this conversion.\n",
    "funchash = FeatureHasher(inputCols= ['S_years','R_years','Resume_Tips','Resume_OnFile','Client','Client_Type','Service_R',\\\n",
    " 'Service_B','HHUSA_hire','Com_Method','responsive__c','active__c','Created_Linkedin','Educ','Occupation'],\n",
    "                       outputCol=\"features\")\n",
    "\n",
    "dffeatures = funchash.transform(dfspark)\n",
    "dffeatures.persist()\n",
    "dffeatures.show(1)\n",
    "def cluster(k,df,features):\n",
    "    \"\"\" Takes in k as an int, the dataframe, and features \"\"\"\n",
    "    kmeans = KMeans(k=k, seed=1)  # 2 clusters here\n",
    "    model = kmeans.fit(df.select(features))\n",
    "    return model\n",
    "\n",
    "transformeddf = cluster(4,dffeatures,'features').transform(dffeatures)\n",
    "\n",
    "#Number of clusters\n",
    "print(\" \")\n",
    "print (\"Number of clusters\")\n",
    "transformeddf.select('prediction').distinct().show()\n",
    "\n",
    "#Service branch with Service Rank, Showing whether hired or not. \n",
    "print(\" \")\n",
    "print(\" Service branches with Service Ranks and cluster as prediction\")\n",
    "transformeddf.select('Service_B','Service_R','hired','prediction').distinct().show()\n",
    "\n",
    "\n",
    "#Information about individual clusters in term of years service men stay in the army and when they register for services\n",
    "print(\" \")\n",
    "print (\"Statistical Information about years in service and Registration for services with HHUSA\")\n",
    "transformeddf.select('S_years','R_years').describe().show()\n",
    "\n",
    "#Get cluster information Cluster 0 in terms of years service men spend on service.\n",
    "print(\" \")\n",
    "print (\"Years clustered with cluster 0 \")\n",
    "cluster0 = transformeddf.filter(transformeddf.prediction==0)\n",
    "cluster0.select('S_years','R_years').describe().show()\n",
    "\n",
    "#Get cluster information Cluster 1\n",
    "print(\" \")\n",
    "print (\"Years clustered with cluster 1 \")\n",
    "cluster1 = transformeddf.filter(transformeddf.prediction==1)\n",
    "cluster1.select('S_years','R_years').describe().show()\n",
    "\n",
    "#Get cluster information Cluster 2\n",
    "print(\" \")\n",
    "print (\"Years clustered with cluster 2\")\n",
    "cluster2 = transformeddf.filter(transformeddf.prediction==2)\n",
    "cluster2.select('S_years','R_years').describe().show()\n",
    "\n",
    "\n",
    "#Get cluster information Cluster 3\n",
    "print(\" \")\n",
    "print (\"Years clustered with cluster 3 \")\n",
    "cluster3 = transformeddf.filter(transformeddf.prediction==3)\n",
    "cluster3.select('S_years','R_years').describe().show()\n",
    "\n",
    "\n",
    "end = datetime.now()\n",
    "\n",
    "print('Duration of script: {}'.format(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Body: string, Id: bigint, Tags: string, Title: string, oneTag: string]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textdf = spark.read.json(\"/home/mose/Downloads/Train_onetag_small.json\")\n",
    "\n",
    "# The persist method saves resources in terms of memory to be used for the dataframe \n",
    "textdf.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Body', 'Id', 'Tags', 'Title', 'oneTag']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Body=\"<p>I'd like to check if an uploaded file is an image file (e.g png, jpg, jpeg, gif, bmp) or another file. The problem is that I'm using Uploadify to upload the files, which changes the mime type and gives a 'text/octal' or something as the mime type, no matter which file type you upload.</p>\\n\\n<p>Is there a way to check if the uploaded file is an image apart from checking the file extension using PHP?</p>\\n\", Id=1, Tags='php image-processing file-upload upload mime-types', Title='How to check if an uploaded file is an image without mime type?', oneTag='php')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textdf.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Coverts sentences into words takes into context regular expression patters \n",
    "RegTokenizer = RegexTokenizer(inputCol=\"Body\", outputCol=\"Words\", pattern=\"\\\\W\")\n",
    "textdf = RegTokenizer.transform(textdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Body=\"<p>I'd like to check if an uploaded file is an image file (e.g png, jpg, jpeg, gif, bmp) or another file. The problem is that I'm using Uploadify to upload the files, which changes the mime type and gives a 'text/octal' or something as the mime type, no matter which file type you upload.</p>\\n\\n<p>Is there a way to check if the uploaded file is an image apart from checking the file extension using PHP?</p>\\n\", Id=1, Tags='php image-processing file-upload upload mime-types', Title='How to check if an uploaded file is an image without mime type?', oneTag='php', Words=['p', 'i', 'd', 'like', 'to', 'check', 'if', 'an', 'uploaded', 'file', 'is', 'an', 'image', 'file', 'e', 'g', 'png', 'jpg', 'jpeg', 'gif', 'bmp', 'or', 'another', 'file', 'the', 'problem', 'is', 'that', 'i', 'm', 'using', 'uploadify', 'to', 'upload', 'the', 'files', 'which', 'changes', 'the', 'mime', 'type', 'and', 'gives', 'a', 'text', 'octal', 'or', 'something', 'as', 'the', 'mime', 'type', 'no', 'matter', 'which', 'file', 'type', 'you', 'upload', 'p', 'p', 'is', 'there', 'a', 'way', 'to', 'check', 'if', 'the', 'uploaded', 'file', 'is', 'an', 'image', 'apart', 'from', 'checking', 'the', 'file', 'extension', 'using', 'php', 'p'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the length of the body or the body \n",
    "lenbody = udf(lambda x:len(x), IntegerType())\n",
    "textdf = textdf.withColumn(\"LenBody\",lenbody(textdf.Words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Body=\"<p>I'd like to check if an uploaded file is an image file (e.g png, jpg, jpeg, gif, bmp) or another file. The problem is that I'm using Uploadify to upload the files, which changes the mime type and gives a 'text/octal' or something as the mime type, no matter which file type you upload.</p>\\n\\n<p>Is there a way to check if the uploaded file is an image apart from checking the file extension using PHP?</p>\\n\", Id=1, Tags='php image-processing file-upload upload mime-types', Title='How to check if an uploaded file is an image without mime type?', oneTag='php', Words=['p', 'i', 'd', 'like', 'to', 'check', 'if', 'an', 'uploaded', 'file', 'is', 'an', 'image', 'file', 'e', 'g', 'png', 'jpg', 'jpeg', 'gif', 'bmp', 'or', 'another', 'file', 'the', 'problem', 'is', 'that', 'i', 'm', 'using', 'uploadify', 'to', 'upload', 'the', 'files', 'which', 'changes', 'the', 'mime', 'type', 'and', 'gives', 'a', 'text', 'octal', 'or', 'something', 'as', 'the', 'mime', 'type', 'no', 'matter', 'which', 'file', 'type', 'you', 'upload', 'p', 'p', 'is', 'there', 'a', 'way', 'to', 'check', 'if', 'the', 'uploaded', 'file', 'is', 'an', 'image', 'apart', 'from', 'checking', 'the', 'file', 'extension', 'using', 'php', 'p'], LenBody=83),\n",
       " Row(Body='<p>In my favorite editor (vim), I regularly use ctrl-w to execute a certain action. Now, it quite often happens to me that firefox is the active window (on windows) while I still look at vim (thinking vim is the active window) and press ctrl-w which closes firefox. This is not what I want. Is there a way to stop ctrl-w from closing firefox?</p>\\n\\n<p>Rene</p>\\n', Id=2, Tags='firefox', Title='How can I prevent firefox from closing when I press ctrl-w', oneTag='firefox', Words=['p', 'in', 'my', 'favorite', 'editor', 'vim', 'i', 'regularly', 'use', 'ctrl', 'w', 'to', 'execute', 'a', 'certain', 'action', 'now', 'it', 'quite', 'often', 'happens', 'to', 'me', 'that', 'firefox', 'is', 'the', 'active', 'window', 'on', 'windows', 'while', 'i', 'still', 'look', 'at', 'vim', 'thinking', 'vim', 'is', 'the', 'active', 'window', 'and', 'press', 'ctrl', 'w', 'which', 'closes', 'firefox', 'this', 'is', 'not', 'what', 'i', 'want', 'is', 'there', 'a', 'way', 'to', 'stop', 'ctrl', 'w', 'from', 'closing', 'firefox', 'p', 'p', 'rene', 'p'], LenBody=71)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textdf.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The re python library finds all regular expresions that are indicated\n",
    "numparagraphs = udf(lambda x: len(re.findall(\"</p>\",x)),IntegerType())\n",
    "numlinks = udf(lambda x: len(re.findall(\"</a>\",x)), IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "textdf = textdf.withColumn(\"numParagraphs\",numparagraphs(textdf.Body))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "textdf = textdf.withColumn(\"Linksnum\", numlinks(textdf.Body))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+\n",
      "|numParagraphs|Linksnum|\n",
      "+-------------+--------+\n",
      "|            2|       0|\n",
      "|            2|       0|\n",
      "|            4|       0|\n",
      "|            7|       1|\n",
      "|            2|       0|\n",
      "|            1|       0|\n",
      "|            9|       0|\n",
      "|            4|       0|\n",
      "|            4|       0|\n",
      "|            3|       0|\n",
      "|            4|       1|\n",
      "|            4|       0|\n",
      "|            1|       0|\n",
      "|            2|       0|\n",
      "|            1|       0|\n",
      "|           12|       0|\n",
      "|            4|       0|\n",
      "|            3|       0|\n",
      "|            4|       1|\n",
      "|            8|       0|\n",
      "+-------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "textdf.select(\"numParagraphs\",\"Linksnum\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Body',\n",
       " 'Id',\n",
       " 'Tags',\n",
       " 'Title',\n",
       " 'oneTag',\n",
       " 'Words',\n",
       " 'LenBody',\n",
       " 'numParagraphs',\n",
       " 'Linksnum']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now put our newly created features into a vector using the vectorizer function\n",
    "assembler = VectorAssembler(inputCols=[\"LenBody\",\"numParagraphs\",\"Linksnum\"],outputCol=\"features\")\n",
    "textdf = assembler.transform(textdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data uniformly\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalerModel = scaler.fit(textdf)\n",
    "textdf = scalerModel.transform(textdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------+\n",
      "|scaledFeatures                              |\n",
      "+--------------------------------------------+\n",
      "|[0.43245858153237177,0.7036973726344203,0.0]|\n",
      "+--------------------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "textdf.select('scaledFeatures').show(1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Body=\"<p>I'd like to check if an uploaded file is an image file (e.g png, jpg, jpeg, gif, bmp) or another file. The problem is that I'm using Uploadify to upload the files, which changes the mime type and gives a 'text/octal' or something as the mime type, no matter which file type you upload.</p>\\n\\n<p>Is there a way to check if the uploaded file is an image apart from checking the file extension using PHP?</p>\\n\", Id=1, Tags='php image-processing file-upload upload mime-types', Title='How to check if an uploaded file is an image without mime type?', oneTag='php', Words=['p', 'i', 'd', 'like', 'to', 'check', 'if', 'an', 'uploaded', 'file', 'is', 'an', 'image', 'file', 'e', 'g', 'png', 'jpg', 'jpeg', 'gif', 'bmp', 'or', 'another', 'file', 'the', 'problem', 'is', 'that', 'i', 'm', 'using', 'uploadify', 'to', 'upload', 'the', 'files', 'which', 'changes', 'the', 'mime', 'type', 'and', 'gives', 'a', 'text', 'octal', 'or', 'something', 'as', 'the', 'mime', 'type', 'no', 'matter', 'which', 'file', 'type', 'you', 'upload', 'p', 'p', 'is', 'there', 'a', 'way', 'to', 'check', 'if', 'the', 'uploaded', 'file', 'is', 'an', 'image', 'apart', 'from', 'checking', 'the', 'file', 'extension', 'using', 'php', 'p'], LenBody=83, numParagraphs=2, Linksnum=0, features=DenseVector([83.0, 2.0, 0.0]), scaledFeatures=DenseVector([0.4325, 0.7037, 0.0]))]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textdf.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the word count and the number of words we want to keep. In this case there are 1500\n",
    "cv = CountVectorizer(inputCol=\"Words\", outputCol=\"WordCount2\", vocabSize=1500)\n",
    "model = cv.fit(textdf)\n",
    "textdf = model.transform(textdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Body',\n",
       " 'Id',\n",
       " 'Tags',\n",
       " 'Title',\n",
       " 'oneTag',\n",
       " 'Words',\n",
       " 'LenBody',\n",
       " 'numParagraphs',\n",
       " 'Linksnum',\n",
       " 'features',\n",
       " 'scaledFeatures',\n",
       " 'WordCount2']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|WordCount2                                                                                                                                                                                                                                                                                                                           |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|(1500,[0,1,2,3,5,8,9,15,21,28,31,35,36,43,45,48,51,57,61,71,78,84,86,94,97,99,100,115,147,152,169,241,283,306,350,490,578,759,832,1003,1257,1358,1484],[4.0,6.0,2.0,3.0,2.0,4.0,1.0,1.0,2.0,1.0,1.0,3.0,1.0,2.0,2.0,1.0,1.0,6.0,2.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0])|\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "textdf.select('WordCount2').show(1,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['p',\n",
       " 'the',\n",
       " 'i',\n",
       " 'to',\n",
       " 'code',\n",
       " 'a',\n",
       " 'gt',\n",
       " 'lt',\n",
       " 'is',\n",
       " 'and',\n",
       " 'pre',\n",
       " 'in',\n",
       " 'this',\n",
       " 'of',\n",
       " 'it',\n",
       " 'that',\n",
       " 'for',\n",
       " '0',\n",
       " '1',\n",
       " 'have',\n",
       " 'my',\n",
       " 'if',\n",
       " 'on',\n",
       " 'but',\n",
       " 'with',\n",
       " 'can',\n",
       " 'not',\n",
       " 'be',\n",
       " 'as',\n",
       " 't',\n",
       " 'li',\n",
       " 'from',\n",
       " '2',\n",
       " 's',\n",
       " 'http',\n",
       " 'an',\n",
       " 'm',\n",
       " 'strong',\n",
       " 'new',\n",
       " 'how',\n",
       " 'do',\n",
       " 'com',\n",
       " 'so',\n",
       " 'or',\n",
       " 'at',\n",
       " 'using',\n",
       " 'when',\n",
       " 'am',\n",
       " 'like',\n",
       " 'class',\n",
       " 'id',\n",
       " 'there',\n",
       " 'get',\n",
       " 'are',\n",
       " 'name',\n",
       " 'what',\n",
       " 'any',\n",
       " 'file',\n",
       " 'string',\n",
       " 'data',\n",
       " 'all',\n",
       " 'which',\n",
       " 'want',\n",
       " 'would',\n",
       " 'amp',\n",
       " 'use',\n",
       " 'java',\n",
       " 'function',\n",
       " 'public',\n",
       " 'some',\n",
       " '3',\n",
       " 'text',\n",
       " 'error',\n",
       " 'android',\n",
       " 'value',\n",
       " 'c',\n",
       " 'x',\n",
       " 'href',\n",
       " 'you',\n",
       " 'one',\n",
       " 'by',\n",
       " 'user',\n",
       " 'me',\n",
       " 'server',\n",
       " 'type',\n",
       " 'here',\n",
       " 'way',\n",
       " 'return',\n",
       " 'int',\n",
       " 'will',\n",
       " 'div',\n",
       " 'need',\n",
       " 'then',\n",
       " 'set',\n",
       " 'e',\n",
       " 'system',\n",
       " 'has',\n",
       " 'problem',\n",
       " 'out',\n",
       " 'php',\n",
       " 'no',\n",
       " 'just',\n",
       " '4',\n",
       " 'org',\n",
       " 'know',\n",
       " 'html',\n",
       " 'only',\n",
       " 'where',\n",
       " 'page',\n",
       " 'application',\n",
       " '5',\n",
       " 'thanks',\n",
       " 'var',\n",
       " 'br',\n",
       " 'we',\n",
       " 'd',\n",
       " 'should',\n",
       " 'does',\n",
       " 'add',\n",
       " 'n',\n",
       " 'true',\n",
       " 've',\n",
       " 'void',\n",
       " 'em',\n",
       " 'was',\n",
       " 'rel',\n",
       " 'work',\n",
       " 'time',\n",
       " 'other',\n",
       " '10',\n",
       " 'app',\n",
       " 'null',\n",
       " 'method',\n",
       " 'b',\n",
       " 'table',\n",
       " 'list',\n",
       " 'now',\n",
       " 'into',\n",
       " 'help',\n",
       " 'end',\n",
       " 'trying',\n",
       " 'following',\n",
       " 'object',\n",
       " 'view',\n",
       " 'nofollow',\n",
       " 'up',\n",
       " 'example',\n",
       " 'image',\n",
       " 'same',\n",
       " 'create',\n",
       " 'also',\n",
       " 'each',\n",
       " 'something',\n",
       " 'www',\n",
       " 'web',\n",
       " 'first',\n",
       " 'array',\n",
       " 'line',\n",
       " 'script',\n",
       " 'find',\n",
       " 'don',\n",
       " 'run',\n",
       " 'could',\n",
       " 'select',\n",
       " 'about',\n",
       " 'test',\n",
       " 'make',\n",
       " 'form',\n",
       " 'r',\n",
       " 'files',\n",
       " 'tried',\n",
       " 'ul',\n",
       " 'net',\n",
       " 'url',\n",
       " 'td',\n",
       " 'self',\n",
       " 'input',\n",
       " 'windows',\n",
       " 'button',\n",
       " 'see',\n",
       " 'blockquote',\n",
       " 'database',\n",
       " 'question',\n",
       " 'content',\n",
       " 'else',\n",
       " 'more',\n",
       " 'works',\n",
       " 'xml',\n",
       " '6',\n",
       " '00',\n",
       " 'two',\n",
       " '8',\n",
       " 'after',\n",
       " 'they',\n",
       " 'possible',\n",
       " 'false',\n",
       " 'right',\n",
       " 'them',\n",
       " 'y',\n",
       " 'working',\n",
       " '7',\n",
       " 'width',\n",
       " 'main',\n",
       " 'src',\n",
       " 'try',\n",
       " 'private',\n",
       " 'however',\n",
       " 'version',\n",
       " 'number',\n",
       " 'f',\n",
       " 'result',\n",
       " 'these',\n",
       " 'because',\n",
       " 'project',\n",
       " 'key',\n",
       " 'message',\n",
       " 'why',\n",
       " 'doesn',\n",
       " 'used',\n",
       " 'please',\n",
       " 'query',\n",
       " 'import',\n",
       " 'size',\n",
       " 'item',\n",
       " 'call',\n",
       " 'show',\n",
       " 'while',\n",
       " 'title',\n",
       " 'found',\n",
       " 'been',\n",
       " 'anyone',\n",
       " 'change',\n",
       " 'post',\n",
       " 'document',\n",
       " 'users',\n",
       " 'different',\n",
       " 'its',\n",
       " 'start',\n",
       " 'able',\n",
       " 'log',\n",
       " 'access',\n",
       " 'another',\n",
       " 'event',\n",
       " 'case',\n",
       " 'request',\n",
       " 'values',\n",
       " 'update',\n",
       " 'client',\n",
       " 'edit',\n",
       " 'index',\n",
       " '9',\n",
       " 'service',\n",
       " 'read',\n",
       " 'without',\n",
       " 'source',\n",
       " 'javascript',\n",
       " 'left',\n",
       " 'style',\n",
       " 'open',\n",
       " 'jquery',\n",
       " 'img',\n",
       " 'running',\n",
       " 'row',\n",
       " 'h',\n",
       " 'display',\n",
       " 'fine',\n",
       " 'write',\n",
       " 'site',\n",
       " 'google',\n",
       " 'seems',\n",
       " 'height',\n",
       " 'click',\n",
       " 'date',\n",
       " '12',\n",
       " 'static',\n",
       " 'etc',\n",
       " 'option',\n",
       " 'path',\n",
       " 'output',\n",
       " 'property',\n",
       " '20',\n",
       " 'doing',\n",
       " 'model',\n",
       " 'g',\n",
       " 'default',\n",
       " 'link',\n",
       " 'than',\n",
       " 'through',\n",
       " 'echo',\n",
       " 'below',\n",
       " 'include',\n",
       " 'even',\n",
       " 'solution',\n",
       " 'lib',\n",
       " '11',\n",
       " 'sql',\n",
       " 'questions',\n",
       " 'still',\n",
       " 'program',\n",
       " 'such',\n",
       " 'library',\n",
       " 'getting',\n",
       " 'exception',\n",
       " 'created',\n",
       " 'simple',\n",
       " 'context',\n",
       " 'png',\n",
       " 'your',\n",
       " 'very',\n",
       " 'before',\n",
       " 'apache',\n",
       " 'both',\n",
       " 'ol',\n",
       " 'sure',\n",
       " '100',\n",
       " 'order',\n",
       " 'asp',\n",
       " 'command',\n",
       " 'field',\n",
       " 'color',\n",
       " 'window',\n",
       " 'images',\n",
       " 'column',\n",
       " 'load',\n",
       " 'having',\n",
       " 'thread',\n",
       " 'background',\n",
       " 'think',\n",
       " 'js',\n",
       " 'wrong',\n",
       " 'go',\n",
       " 'point',\n",
       " 'element',\n",
       " 'process',\n",
       " 'length',\n",
       " 'really',\n",
       " 'tr',\n",
       " 'span',\n",
       " 'being',\n",
       " 'every',\n",
       " 'back',\n",
       " 'current',\n",
       " 'called',\n",
       " 'css',\n",
       " 'label',\n",
       " 'action',\n",
       " 'issue',\n",
       " 'many',\n",
       " 'info',\n",
       " 'stack',\n",
       " 'check',\n",
       " 'got',\n",
       " 'top',\n",
       " 'since',\n",
       " 'connection',\n",
       " 'looking',\n",
       " 'put',\n",
       " 'second',\n",
       " 'search',\n",
       " 'db',\n",
       " 'local',\n",
       " 'over',\n",
       " 'email',\n",
       " 'above',\n",
       " 'password',\n",
       " 'done',\n",
       " 'api',\n",
       " 'response',\n",
       " 'between',\n",
       " 'build',\n",
       " 'cannot',\n",
       " 'alt',\n",
       " 'print',\n",
       " 'j',\n",
       " 'well',\n",
       " 'body',\n",
       " 'directory',\n",
       " 'count',\n",
       " 'description',\n",
       " 'location',\n",
       " 'information',\n",
       " 'next',\n",
       " 'address',\n",
       " 'root',\n",
       " 'good',\n",
       " '01',\n",
       " 'let',\n",
       " 'control',\n",
       " 'o',\n",
       " 'microsoft',\n",
       " 'part',\n",
       " 'map',\n",
       " 'advance',\n",
       " 'our',\n",
       " 'instead',\n",
       " 'v',\n",
       " 'much',\n",
       " 'their',\n",
       " 'best',\n",
       " 'position',\n",
       " '2012',\n",
       " 'idea',\n",
       " 'custom',\n",
       " 'format',\n",
       " 'mysql',\n",
       " 'already',\n",
       " 'say',\n",
       " 'long',\n",
       " 'instance',\n",
       " 'variable',\n",
       " 'send',\n",
       " '13',\n",
       " '16',\n",
       " 'may',\n",
       " 'currently',\n",
       " 'results',\n",
       " 'inside',\n",
       " '15',\n",
       " 'k',\n",
       " 'header',\n",
       " 'enter',\n",
       " 'items',\n",
       " 'correct',\n",
       " 'home',\n",
       " 'controller',\n",
       " 'domain',\n",
       " 'node',\n",
       " 'z',\n",
       " 'override',\n",
       " 'based',\n",
       " '30',\n",
       " 'seem',\n",
       " 'group',\n",
       " 'options',\n",
       " 'last',\n",
       " 'ui',\n",
       " 'added',\n",
       " 'screen',\n",
       " 'someone',\n",
       " 'folder',\n",
       " 'save',\n",
       " 'session',\n",
       " 'website',\n",
       " 'close',\n",
       " 'stackoverflow',\n",
       " 'better',\n",
       " 'username',\n",
       " 'python',\n",
       " 'bit',\n",
       " 'box',\n",
       " 'menu',\n",
       " 'json',\n",
       " 'login',\n",
       " 'console',\n",
       " 'via',\n",
       " 'l',\n",
       " 'usr',\n",
       " 'looks',\n",
       " 'anything',\n",
       " 'def',\n",
       " 'none',\n",
       " 'https',\n",
       " 'char',\n",
       " 'lang',\n",
       " 'within',\n",
       " 'multiple',\n",
       " 'activity',\n",
       " 'everything',\n",
       " 'appreciated',\n",
       " 'tag',\n",
       " 'insert',\n",
       " 'ajax',\n",
       " 'had',\n",
       " 'config',\n",
       " 'again',\n",
       " 'understand',\n",
       " 'parent',\n",
       " 'final',\n",
       " 'install',\n",
       " 'catch',\n",
       " 'browser',\n",
       " 'objects',\n",
       " 'store',\n",
       " 'os',\n",
       " 'ideas',\n",
       " 'double',\n",
       " 'thing',\n",
       " 'contains',\n",
       " 'given',\n",
       " 'look',\n",
       " 'jpg',\n",
       " 'template',\n",
       " 'font',\n",
       " 'reference',\n",
       " 'those',\n",
       " 'answer',\n",
       " 'going',\n",
       " 'connect',\n",
       " 'debug',\n",
       " 'imgur',\n",
       " 'frame',\n",
       " 'rows',\n",
       " 'down',\n",
       " 'ruby',\n",
       " 'foo',\n",
       " 'around',\n",
       " 'always',\n",
       " 'too',\n",
       " 'state',\n",
       " 'remove',\n",
       " 'block',\n",
       " 'status',\n",
       " 'intent',\n",
       " 'yes',\n",
       " 'must',\n",
       " 'errors',\n",
       " 'thank',\n",
       " '08',\n",
       " 'layout',\n",
       " 'en',\n",
       " 'did',\n",
       " 'far',\n",
       " 'core',\n",
       " 'u',\n",
       " 'memory',\n",
       " 'most',\n",
       " 'installed',\n",
       " 'single',\n",
       " 'w',\n",
       " 'framework',\n",
       " 'bar',\n",
       " 'linux',\n",
       " 'machine',\n",
       " 'specific',\n",
       " 'loop',\n",
       " 'give',\n",
       " '14',\n",
       " 'delete',\n",
       " 'begin',\n",
       " 'take',\n",
       " 'ip',\n",
       " 'lot',\n",
       " 'returns',\n",
       " '02',\n",
       " 'facebook',\n",
       " 'device',\n",
       " 're',\n",
       " 'things',\n",
       " 'creating',\n",
       " 'base',\n",
       " 'ok',\n",
       " 'nothing',\n",
       " 'missing',\n",
       " 'module',\n",
       " 'fields',\n",
       " 'host',\n",
       " 'own',\n",
       " 'uses',\n",
       " 'support',\n",
       " 'androidruntime',\n",
       " 'might',\n",
       " 'similar',\n",
       " 'float',\n",
       " 'integer',\n",
       " 'actually',\n",
       " 'figure',\n",
       " '50',\n",
       " 'configuration',\n",
       " '2010',\n",
       " 'alert',\n",
       " 'tell',\n",
       " 'us',\n",
       " 'execute',\n",
       " 'std',\n",
       " 'tostring',\n",
       " 'few',\n",
       " 'empty',\n",
       " 'note',\n",
       " 'changes',\n",
       " 'xmlns',\n",
       " 'pass',\n",
       " 'failed',\n",
       " 'off',\n",
       " 'settings',\n",
       " 'network',\n",
       " 'either',\n",
       " 'package',\n",
       " 'interface',\n",
       " '2011',\n",
       " 'eclipse',\n",
       " 'mode',\n",
       " 'plugin',\n",
       " '25',\n",
       " 'elements',\n",
       " 'methods',\n",
       " 'classes',\n",
       " 'rb',\n",
       " 'head',\n",
       " 'shows',\n",
       " 'properties',\n",
       " '18',\n",
       " 'video',\n",
       " 'const',\n",
       " 'port',\n",
       " 'pages',\n",
       " 'break',\n",
       " 'columns',\n",
       " 'made',\n",
       " 'bin',\n",
       " 'cell',\n",
       " '23',\n",
       " 'functions',\n",
       " 'allow',\n",
       " '21',\n",
       " 'nbsp',\n",
       " 'generated',\n",
       " 'q',\n",
       " 'submit',\n",
       " 'once',\n",
       " 'setup',\n",
       " 'product',\n",
       " 'grid',\n",
       " 'reason',\n",
       " '22',\n",
       " 'copy',\n",
       " 'h2',\n",
       " 'times',\n",
       " 'aspx',\n",
       " 'sub',\n",
       " 'auto',\n",
       " 'target',\n",
       " 'dev',\n",
       " 'available',\n",
       " '04',\n",
       " 'jar',\n",
       " 'keep',\n",
       " 'filter',\n",
       " 'required',\n",
       " 'side',\n",
       " 'convert',\n",
       " 'tables',\n",
       " 'handle',\n",
       " 'center',\n",
       " 'println',\n",
       " 'task',\n",
       " 'selected',\n",
       " 'join',\n",
       " 'setting',\n",
       " 'security',\n",
       " 'gets',\n",
       " 'category',\n",
       " '17',\n",
       " '05',\n",
       " 'lines',\n",
       " 'under',\n",
       " 'though',\n",
       " 'boolean',\n",
       " 'gems',\n",
       " 'entity',\n",
       " 'localhost',\n",
       " 'isn',\n",
       " 'space',\n",
       " 'admin',\n",
       " 'sun',\n",
       " 'fix',\n",
       " 'dll',\n",
       " 'implement',\n",
       " 'internal',\n",
       " 'wondering',\n",
       " 'nil',\n",
       " 'init',\n",
       " 'val',\n",
       " '09',\n",
       " 'parameters',\n",
       " '19',\n",
       " 'binding',\n",
       " 'rails',\n",
       " 'txt',\n",
       " 'maybe',\n",
       " 'level',\n",
       " 'super',\n",
       " 'several',\n",
       " 'child',\n",
       " 'defined',\n",
       " 'non',\n",
       " '03',\n",
       " 'place',\n",
       " 'parameter',\n",
       " '24',\n",
       " 'sort',\n",
       " 'needs',\n",
       " 'language',\n",
       " 'adding',\n",
       " 'py',\n",
       " 'dialog',\n",
       " 'cache',\n",
       " 'statement',\n",
       " 'section',\n",
       " 'kind',\n",
       " 'problems',\n",
       " 'total',\n",
       " 'margin',\n",
       " '27',\n",
       " 'utf',\n",
       " 'filename',\n",
       " 'nsstring',\n",
       " 'writing',\n",
       " 'border',\n",
       " 'account',\n",
       " 'computer',\n",
       " 'changed',\n",
       " 'correctly',\n",
       " 'match',\n",
       " '255',\n",
       " 'tab',\n",
       " 'bundle',\n",
       " 'define',\n",
       " 'didn',\n",
       " 'warning',\n",
       " 'basically',\n",
       " 'solve',\n",
       " 'release',\n",
       " 'protected',\n",
       " 'hr',\n",
       " 'structure',\n",
       " 'replace',\n",
       " 'frac',\n",
       " 'software',\n",
       " 'foreach',\n",
       " 'extends',\n",
       " '29',\n",
       " 'thought',\n",
       " 'full',\n",
       " 'byte',\n",
       " 'word',\n",
       " 'stored',\n",
       " 'ie',\n",
       " '_',\n",
       " 'day',\n",
       " 'sender',\n",
       " 'record',\n",
       " 'clear',\n",
       " 'never',\n",
       " 'services',\n",
       " 'were',\n",
       " 'pdf',\n",
       " '32',\n",
       " '07',\n",
       " 'stop',\n",
       " 'resources',\n",
       " 'models',\n",
       " 'springframework',\n",
       " 'move',\n",
       " 'alloc',\n",
       " 'people',\n",
       " 'started',\n",
       " 'checked',\n",
       " 'virtual',\n",
       " 'attribute',\n",
       " 'gives',\n",
       " 'suggestions',\n",
       " 'container',\n",
       " 'ubuntu',\n",
       " 'numbers',\n",
       " 'old',\n",
       " 'mail',\n",
       " 'sample',\n",
       " 'unknown',\n",
       " 'small',\n",
       " 'bool',\n",
       " 'align',\n",
       " 'collection',\n",
       " 'success',\n",
       " 'visual',\n",
       " 'person',\n",
       " 'compile',\n",
       " 'mean',\n",
       " 'encoding',\n",
       " 'standard',\n",
       " 'iphone',\n",
       " 'param',\n",
       " 'append',\n",
       " 'projects',\n",
       " 'events',\n",
       " 'bytes',\n",
       " 'str',\n",
       " '06',\n",
       " 'rather',\n",
       " 'written',\n",
       " 'reading',\n",
       " 'resource',\n",
       " 'wrap_content',\n",
       " 'exists',\n",
       " 'details',\n",
       " 'certain',\n",
       " 'widget',\n",
       " 'handler',\n",
       " 'download',\n",
       " 'parse',\n",
       " 'stuff',\n",
       " '40',\n",
       " 'until',\n",
       " 'documentation',\n",
       " 'expected',\n",
       " 'variables',\n",
       " 'socket',\n",
       " 'git',\n",
       " 'little',\n",
       " 'servlet',\n",
       " 'hello',\n",
       " 'th',\n",
       " 'modules',\n",
       " 'environment',\n",
       " 'generate',\n",
       " 'says',\n",
       " 'development',\n",
       " 'easy',\n",
       " 'simply',\n",
       " 'large',\n",
       " 'three',\n",
       " '31',\n",
       " 'exe',\n",
       " 'game',\n",
       " 'approach',\n",
       " 'calls',\n",
       " 'args',\n",
       " 'normal',\n",
       " 'bottom',\n",
       " 'global',\n",
       " 'views',\n",
       " 'types',\n",
       " 'javax',\n",
       " 'upload',\n",
       " 'invoke',\n",
       " 'tags',\n",
       " 'hibernate',\n",
       " 'switch',\n",
       " 'stream',\n",
       " 'onclick',\n",
       " 'making',\n",
       " 'remote',\n",
       " 'layout_width',\n",
       " 'layout_height',\n",
       " 'entry',\n",
       " 'im',\n",
       " 'namespace',\n",
       " 'll',\n",
       " 'whole',\n",
       " 'he',\n",
       " 'great',\n",
       " 'original',\n",
       " 'textview',\n",
       " 'hidden',\n",
       " 'io',\n",
       " 'internet',\n",
       " 'component',\n",
       " 'free',\n",
       " 'comment',\n",
       " 'obj',\n",
       " 'links',\n",
       " 'phone',\n",
       " 'loaded',\n",
       " 'happens',\n",
       " 'developer',\n",
       " 'messages',\n",
       " 'testing',\n",
       " 'names',\n",
       " 'automatically',\n",
       " 'whether',\n",
       " 'studio',\n",
       " 'mvc',\n",
       " '2008',\n",
       " 'come',\n",
       " 'play',\n",
       " 'course',\n",
       " 'points',\n",
       " '200',\n",
       " 'chrome',\n",
       " 'xsl',\n",
       " 'appears',\n",
       " 'related',\n",
       " 'pattern',\n",
       " 'loading',\n",
       " 'pretty',\n",
       " 'who',\n",
       " 'basic',\n",
       " 'provide',\n",
       " 'django',\n",
       " 'require',\n",
       " 'range',\n",
       " 'per',\n",
       " 'player',\n",
       " 'means',\n",
       " 'particular',\n",
       " 'padding',\n",
       " 'quite',\n",
       " 'real',\n",
       " 'tools',\n",
       " 'forms',\n",
       " 'util',\n",
       " 'datetime',\n",
       " 'buffer',\n",
       " 'character',\n",
       " 'random',\n",
       " '_post',\n",
       " 'vector',\n",
       " 'itself',\n",
       " 'ms',\n",
       " 'icon',\n",
       " 'valid',\n",
       " 'records',\n",
       " 'cursor',\n",
       " 'implementation',\n",
       " 'active',\n",
       " 'achieve',\n",
       " 'drive',\n",
       " 'yet',\n",
       " 'goes',\n",
       " 'runat',\n",
       " 'calling',\n",
       " 'max',\n",
       " 'maps',\n",
       " 'unable',\n",
       " 'bind',\n",
       " 'params',\n",
       " 'except',\n",
       " 'takes',\n",
       " 'displayed',\n",
       " 'hard',\n",
       " 'report',\n",
       " 'share',\n",
       " 'common',\n",
       " 'makes',\n",
       " 'issues',\n",
       " 'firefox',\n",
       " 'temp',\n",
       " 'runs',\n",
       " 'follows',\n",
       " 'seen',\n",
       " 'limit',\n",
       " 'properly',\n",
       " '64',\n",
       " 'company',\n",
       " 'characters',\n",
       " 'doc',\n",
       " 'applications',\n",
       " 'step',\n",
       " 'tool',\n",
       " 'native',\n",
       " 'h1',\n",
       " 'attr',\n",
       " 'exist',\n",
       " 'sent',\n",
       " 'external',\n",
       " 'duplicate',\n",
       " 'controls',\n",
       " 'complete',\n",
       " 'sum',\n",
       " 'exactly',\n",
       " 'directly',\n",
       " 'schema',\n",
       " 'panel',\n",
       " 'min',\n",
       " 'performance',\n",
       " 'posts',\n",
       " 'job',\n",
       " 'difference',\n",
       " 'dim',\n",
       " 'matrix',\n",
       " 'seconds',\n",
       " 'separate',\n",
       " 'expression',\n",
       " 'docs',\n",
       " 'syntax',\n",
       " 'examples',\n",
       " 'textbox',\n",
       " 'article',\n",
       " 'book',\n",
       " 'render',\n",
       " 'thinking',\n",
       " 'flash',\n",
       " 'mac',\n",
       " 'checkbox',\n",
       " 'printf',\n",
       " 'runtime',\n",
       " 'canvas',\n",
       " 'existing',\n",
       " '28',\n",
       " 'shared',\n",
       " 'servers',\n",
       " 'customer',\n",
       " 'desktop',\n",
       " 'buttons',\n",
       " 'previous',\n",
       " 'math',\n",
       " 'master',\n",
       " '000',\n",
       " 'blog',\n",
       " 'comes',\n",
       " 'wordpress',\n",
       " ...]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The most common words in the document\n",
    "model.vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['provided', 'against', 'expect', 'focus', 'strange']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Least common words \n",
    "model.vocabulary[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tidf is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus.[1]\n",
    "# Word frequency in comparison to the whole document. \n",
    "idf = IDF(inputCol=\"WordCount2\", outputCol=\"tfidf\")\n",
    "idfModel = idf.fit(textdf)\n",
    "textdf = idfModel.transform(textdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Body=\"<p>I'd like to check if an uploaded file is an image file (e.g png, jpg, jpeg, gif, bmp) or another file. The problem is that I'm using Uploadify to upload the files, which changes the mime type and gives a 'text/octal' or something as the mime type, no matter which file type you upload.</p>\\n\\n<p>Is there a way to check if the uploaded file is an image apart from checking the file extension using PHP?</p>\\n\", Id=1, Tags='php image-processing file-upload upload mime-types', Title='How to check if an uploaded file is an image without mime type?', oneTag='php', Words=['p', 'i', 'd', 'like', 'to', 'check', 'if', 'an', 'uploaded', 'file', 'is', 'an', 'image', 'file', 'e', 'g', 'png', 'jpg', 'jpeg', 'gif', 'bmp', 'or', 'another', 'file', 'the', 'problem', 'is', 'that', 'i', 'm', 'using', 'uploadify', 'to', 'upload', 'the', 'files', 'which', 'changes', 'the', 'mime', 'type', 'and', 'gives', 'a', 'text', 'octal', 'or', 'something', 'as', 'the', 'mime', 'type', 'no', 'matter', 'which', 'file', 'type', 'you', 'upload', 'p', 'p', 'is', 'there', 'a', 'way', 'to', 'check', 'if', 'the', 'uploaded', 'file', 'is', 'an', 'image', 'apart', 'from', 'checking', 'the', 'file', 'extension', 'using', 'php', 'p'], LenBody=83, numParagraphs=2, Linksnum=0, features=DenseVector([83.0, 2.0, 0.0]), scaledFeatures=DenseVector([0.4325, 0.7037, 0.0]), WordCount2=SparseVector(1500, {0: 4.0, 1: 6.0, 2: 2.0, 3: 3.0, 5: 2.0, 8: 4.0, 9: 1.0, 15: 1.0, 21: 2.0, 28: 1.0, 31: 1.0, 35: 3.0, 36: 1.0, 43: 2.0, 45: 2.0, 48: 1.0, 51: 1.0, 57: 6.0, 61: 2.0, 71: 1.0, 78: 1.0, 84: 3.0, 86: 1.0, 94: 1.0, 97: 1.0, 99: 1.0, 100: 1.0, 115: 1.0, 147: 2.0, 152: 1.0, 169: 1.0, 241: 1.0, 283: 1.0, 306: 1.0, 350: 2.0, 490: 1.0, 578: 1.0, 759: 1.0, 832: 2.0, 1003: 1.0, 1257: 1.0, 1358: 1.0, 1484: 1.0}), tfidf=SparseVector(1500, {0: 0.0026, 1: 0.7515, 2: 0.1374, 3: 0.3184, 5: 0.3823, 8: 1.0754, 9: 0.3344, 15: 0.5899, 21: 1.8551, 28: 1.1263, 31: 1.1113, 35: 3.3134, 36: 1.2545, 43: 2.3741, 45: 2.3753, 48: 1.2254, 51: 1.1879, 57: 11.0264, 61: 2.8957, 71: 2.1945, 78: 1.6947, 84: 6.5898, 86: 1.6136, 94: 2.3569, 97: 1.8218, 99: 2.6292, 100: 1.9206, 115: 2.3592, 147: 5.4841, 152: 2.1116, 169: 2.6328, 241: 2.5745, 283: 3.2325, 306: 3.2668, 350: 6.2367, 490: 3.8893, 578: 3.6182, 759: 3.7771, 832: 8.8964, 1003: 4.5612, 1257: 4.6607, 1358: 4.4742, 1484: 5.1482}))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Body',\n",
       " 'Id',\n",
       " 'Tags',\n",
       " 'Title',\n",
       " 'oneTag',\n",
       " 'Words',\n",
       " 'LenBody',\n",
       " 'numParagraphs',\n",
       " 'Linksnum',\n",
       " 'features',\n",
       " 'scaledFeatures',\n",
       " 'WordCount2',\n",
       " 'tfidf']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|          oneTag|\n",
      "+----------------+\n",
      "|             php|\n",
      "|         firefox|\n",
      "|               r|\n",
      "|              c#|\n",
      "|             php|\n",
      "|active-directory|\n",
      "|           other|\n",
      "|              c#|\n",
      "|      javascript|\n",
      "|             sql|\n",
      "|            .net|\n",
      "|       algorithm|\n",
      "|           other|\n",
      "|   documentation|\n",
      "|       windows-7|\n",
      "|             php|\n",
      "|               r|\n",
      "|             wpf|\n",
      "|      javascript|\n",
      "|             php|\n",
      "+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#The target, we need to convert it to numerical \n",
    "textdf.select('oneTag').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the oneTag to index\n",
    "indexer = StringIndexer(inputCol='oneTag', outputCol=\"oneTagIndex\")\n",
    "textdf = indexer.fit(textdf).transform(textdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|OneTagIndex|\n",
      "+-----------+\n",
      "|3.0        |\n",
      "|91.0       |\n",
      "|29.0       |\n",
      "|1.0        |\n",
      "|3.0        |\n",
      "+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "textdf.select('OneTagIndex').show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|LenBody|\n",
      "+-------+\n",
      "|     12|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "textdf.filter(textdf.Id==115).select('LenBody').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Body',\n",
       " 'Id',\n",
       " 'Tags',\n",
       " 'Title',\n",
       " 'oneTag',\n",
       " 'Words',\n",
       " 'LenBody',\n",
       " 'numParagraphs',\n",
       " 'Linksnum',\n",
       " 'features',\n",
       " 'scaledFeatures',\n",
       " 'WordCount2',\n",
       " 'tfidf',\n",
       " 'oneTagIndex']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the most important features of the TFIDF \n",
    "#pca = PCA(k=2, inputCol=\"features\", outputCol=\"pca_features\")\n",
    "#model = pca.fit(df)\n",
    "#model.explainedVariance\n",
    "pca = PCA(k=120, inputCol=\"tfidf\", outputCol=\"pcaFeatures\")\n",
    "modelpca = pca.fit(textdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "textdf = modelpca.transform(textdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Body=\"<p>I'd like to check if an uploaded file is an image file (e.g png, jpg, jpeg, gif, bmp) or another file. The problem is that I'm using Uploadify to upload the files, which changes the mime type and gives a 'text/octal' or something as the mime type, no matter which file type you upload.</p>\\n\\n<p>Is there a way to check if the uploaded file is an image apart from checking the file extension using PHP?</p>\\n\", Id=1, Tags='php image-processing file-upload upload mime-types', Title='How to check if an uploaded file is an image without mime type?', oneTag='php', Words=['p', 'i', 'd', 'like', 'to', 'check', 'if', 'an', 'uploaded', 'file', 'is', 'an', 'image', 'file', 'e', 'g', 'png', 'jpg', 'jpeg', 'gif', 'bmp', 'or', 'another', 'file', 'the', 'problem', 'is', 'that', 'i', 'm', 'using', 'uploadify', 'to', 'upload', 'the', 'files', 'which', 'changes', 'the', 'mime', 'type', 'and', 'gives', 'a', 'text', 'octal', 'or', 'something', 'as', 'the', 'mime', 'type', 'no', 'matter', 'which', 'file', 'type', 'you', 'upload', 'p', 'p', 'is', 'there', 'a', 'way', 'to', 'check', 'if', 'the', 'uploaded', 'file', 'is', 'an', 'image', 'apart', 'from', 'checking', 'the', 'file', 'extension', 'using', 'php', 'p'], LenBody=83, numParagraphs=2, Linksnum=0, features=DenseVector([83.0, 2.0, 0.0]), scaledFeatures=DenseVector([0.4325, 0.7037, 0.0]), WordCount2=SparseVector(1500, {0: 4.0, 1: 6.0, 2: 2.0, 3: 3.0, 5: 2.0, 8: 4.0, 9: 1.0, 15: 1.0, 21: 2.0, 28: 1.0, 31: 1.0, 35: 3.0, 36: 1.0, 43: 2.0, 45: 2.0, 48: 1.0, 51: 1.0, 57: 6.0, 61: 2.0, 71: 1.0, 78: 1.0, 84: 3.0, 86: 1.0, 94: 1.0, 97: 1.0, 99: 1.0, 100: 1.0, 115: 1.0, 147: 2.0, 152: 1.0, 169: 1.0, 241: 1.0, 283: 1.0, 306: 1.0, 350: 2.0, 490: 1.0, 578: 1.0, 759: 1.0, 832: 2.0, 1003: 1.0, 1257: 1.0, 1358: 1.0, 1484: 1.0}), tfidf=SparseVector(1500, {0: 0.0026, 1: 0.7515, 2: 0.1374, 3: 0.3184, 5: 0.3823, 8: 1.0754, 9: 0.3344, 15: 0.5899, 21: 1.8551, 28: 1.1263, 31: 1.1113, 35: 3.3134, 36: 1.2545, 43: 2.3741, 45: 2.3753, 48: 1.2254, 51: 1.1879, 57: 11.0264, 61: 2.8957, 71: 2.1945, 78: 1.6947, 84: 6.5898, 86: 1.6136, 94: 2.3569, 97: 1.8218, 99: 2.6292, 100: 1.9206, 115: 2.3592, 147: 5.4841, 152: 2.1116, 169: 2.6328, 241: 2.5745, 283: 3.2325, 306: 3.2668, 350: 6.2367, 490: 3.8893, 578: 3.6182, 759: 3.7771, 832: 8.8964, 1003: 4.5612, 1257: 4.6607, 1358: 4.4742, 1484: 5.1482}), oneTagIndex=3.0, pcaFeatures=DenseVector([-0.5515, -0.8551, -0.2699, 0.0702, 0.0058, -0.1756, -0.1471, -0.1039, 0.5323, -0.1512, 0.7982, -0.3081, 2.019, -2.2702, 0.627, -0.5612, -0.9537, -0.372, 0.0211, 0.7193, -0.2496, 0.8342, 0.0201, 1.7637, -0.3452, 0.6952, -0.1296, 1.5319, 0.3866, 0.1505, 1.3836, -0.8519, 0.0436, 0.3116, -0.0259, 0.6506, 0.0241, 1.1728, -0.9009, -0.7641, -0.7732, 0.2815, -0.3001, 1.3035, 1.6064, -1.3777, 0.3615, -0.5381, 0.1033, -0.8828, 1.0723, 0.2914, -1.0722, -0.6495, 0.5774, 0.0517, 0.5234, -0.6169, 1.6435, 0.1649, -1.6936, -0.3223, -0.4, -1.0087, 0.4448, 0.5565, -0.8148, 0.0169, -0.1976, 0.6536, 0.4421, 0.2553, 0.5447, -0.8777, 0.9855, -0.7105, 1.009, -0.6488, -0.8794, -0.3719, -1.8214, 1.038, -0.092, -0.1404, -0.6326, 0.2177, 1.1506, 1.6385, 0.6145, -1.0655, 0.246, 0.6379, 0.2214, 1.487, -1.3774, 2.2027, 0.0184, 0.2838, -0.3311, -0.3596, -1.6591, 2.4591, -1.5707, -0.6473, 0.133, 0.0425, 1.2729, -0.6126, -0.6668, 0.9455, -0.5654, -2.1052, -0.2365, 0.5156, -1.8132, 0.4742, 0.4086, -0.4022, 0.4052, -0.5793]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+-------------+-------------+\n",
      "|categoryIndex1|categoryIndex2| categoryVec1| categoryVec2|\n",
      "+--------------+--------------+-------------+-------------+\n",
      "|           0.0|           1.0|(2,[0],[1.0])|(2,[1],[1.0])|\n",
      "|           1.0|           0.0|(2,[1],[1.0])|(2,[0],[1.0])|\n",
      "|           2.0|           1.0|    (2,[],[])|(2,[1],[1.0])|\n",
      "|           0.0|           2.0|(2,[0],[1.0])|    (2,[],[])|\n",
      "|           0.0|           1.0|(2,[0],[1.0])|(2,[1],[1.0])|\n",
      "|           2.0|           0.0|    (2,[],[])|(2,[0],[1.0])|\n",
      "+--------------+--------------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    (0.0, 1.0),\n",
    "    (1.0, 0.0),\n",
    "    (2.0, 1.0),\n",
    "    (0.0, 2.0),\n",
    "    (0.0, 1.0),\n",
    "    (2.0, 0.0)\n",
    "], [\"categoryIndex1\", \"categoryIndex2\"])\n",
    "\n",
    "encoder = OneHotEncoder(inputCols=[\"categoryIndex1\", \"categoryIndex2\"],\n",
    "                        outputCols=[\"categoryVec1\", \"categoryVec2\"])\n",
    "model = encoder.fit(df)\n",
    "encoded = model.transform(df)\n",
    "encoded.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[S_years: double, R_years: double, Resume_Tips: int, Resume_OnFile: int, Client: int, Client_Type: string, Service_R: string, Service_B: string, HHUSA_hire: int, Com_Method: string, responsive__c: int, active__c: string, Created_Linkedin: int, Educ: string, hired: double, Occupation: string]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"Client_Type\", outputCol=\"Client_TypeIndex\")\n",
    "dfspark = indexer.fit(dfspark).transform(dfspark)\n",
    "dfspark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder(inputCols=[\"Client_TypeIndex\"],\n",
    "                        outputCols=[\"Client_TypeIndexVec\"])\n",
    "model = encoder.fit(dfspark)\n",
    "dfspark = model.transform(dfspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[S_years: double, R_years: double, Resume_Tips: int, Resume_OnFile: int, Client: int, Client_Type: string, Service_R: string, Service_B: string, HHUSA_hire: int, Com_Method: string, responsive__c: int, active__c: string, Created_Linkedin: int, Educ: string, hired: double, Occupation: string, Client_TypeIndex: double, Client_TypeIndexVec: vector]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Client_Type='Online Registrant', Client_TypeIndexVec=SparseVector(4, {0: 1.0}))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfspark.select('Client_Type','Client_TypeIndexVec').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('S_years', 'double'),\n",
       " ('R_years', 'double'),\n",
       " ('Resume_Tips', 'int'),\n",
       " ('Resume_OnFile', 'int'),\n",
       " ('Client', 'int'),\n",
       " ('Client_Type', 'string'),\n",
       " ('Service_R', 'string'),\n",
       " ('Service_B', 'string'),\n",
       " ('HHUSA_hire', 'int'),\n",
       " ('Com_Method', 'string'),\n",
       " ('responsive__c', 'int'),\n",
       " ('active__c', 'string'),\n",
       " ('Created_Linkedin', 'int'),\n",
       " ('Educ', 'string'),\n",
       " ('hired', 'double'),\n",
       " ('Occupation', 'string'),\n",
       " ('Client_TypeIndex', 'double'),\n",
       " ('Client_TypeIndexVec', 'vector')]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfspark.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfspark = dfspark.drop('Client_TypeIndex')\n",
    "dfspark = dfspark.drop('Client_TypeIndexVec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['S_years',\n",
       " 'R_years',\n",
       " 'Resume_Tips',\n",
       " 'Resume_OnFile',\n",
       " 'Client',\n",
       " 'Client_Type',\n",
       " 'Service_R',\n",
       " 'Service_B',\n",
       " 'HHUSA_hire',\n",
       " 'Com_Method',\n",
       " 'responsive__c',\n",
       " 'active__c',\n",
       " 'Created_Linkedin',\n",
       " 'Educ',\n",
       " 'hired',\n",
       " 'Occupation']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfspark.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "indexers = [StringIndexer(inputCol=i, outputCol=i+\"Index\").fit(dfspark) \\\n",
    "            for i in list(set(dfspark.columns)- set(['S_years','R_years','Resume_Tips','Resume_OnFile','Client','HHUSA_hire','responsive__c','Created_Linkedin','Occupation','Educ'])) ]\n",
    "pipeline = Pipeline(stages=indexers)\n",
    "dfspark = pipeline.fit(dfspark).transform(dfspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[S_years: double, R_years: double, Resume_Tips: int, Resume_OnFile: int, Client: int, Client_Type: string, Service_R: string, Service_B: string, HHUSA_hire: int, Com_Method: string, responsive__c: int, active__c: string, Created_Linkedin: int, Educ: string, hired: double, Occupation: string, Com_MethodIndex: double, hiredIndex: double, active__cIndex: double, Client_TypeIndex: double, Service_BIndex: double, Service_RIndex: double]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('S_years', 'double'),\n",
       " ('R_years', 'double'),\n",
       " ('Resume_Tips', 'int'),\n",
       " ('Resume_OnFile', 'int'),\n",
       " ('Client', 'int'),\n",
       " ('Client_Type', 'string'),\n",
       " ('Service_R', 'string'),\n",
       " ('Service_B', 'string'),\n",
       " ('HHUSA_hire', 'int'),\n",
       " ('Com_Method', 'string'),\n",
       " ('responsive__c', 'int'),\n",
       " ('active__c', 'string'),\n",
       " ('Created_Linkedin', 'int'),\n",
       " ('Educ', 'string'),\n",
       " ('hired', 'double'),\n",
       " ('Occupation', 'string'),\n",
       " ('Service_BIndex', 'double'),\n",
       " ('Com_MethodIndex', 'double'),\n",
       " ('active__cIndex', 'double'),\n",
       " ('Service_RIndex', 'double'),\n",
       " ('Client_TypeIndex', 'double'),\n",
       " ('hiredIndex', 'double')]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfspark.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "encoder = OneHotEncoder(inputCols=[\"Service_BIndex\", \"Com_MethodIndex\",\"active__cIndex\", \"Service_RIndex\", \"Client_TypeIndex\"],\n",
    "                       outputCols=[\"Service_BIndexVec\", \"Com_MethodIndexVec\",\"active__cIndexVec\", \"Service_RIndexVec\", \"Client_TypeIndexVec\"])\n",
    "modelOne = encoder.fit(dfspark)\n",
    "dfspark = modelOne.transform(dfspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learn a mapping from words to Vectors.\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "from pyspark.sql.functions import split, regexp_replace\n",
    "\n",
    "\n",
    "dfspark = dfspark.withColumn(\n",
    "    \"Educ\",\n",
    "    split(regexp_replace(\"Educ\", r\"(^\\[\\[\\[)|(\\]\\]\\]$)\", \"\"), \", \")\n",
    ")\n",
    "\n",
    "#word2Vec = Word2Vec(minCount=0, inputCol=\"Educ\", outputCol=\"EducVec\")\n",
    "#modelw = word2Vec.fit(dfspark)\n",
    "\n",
    "#dfspark = modelw.transform(dfspark)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(S_years=8.11, R_years=-0.37, Resume_Tips=1, Resume_OnFile=1, Client=1, Client_Type='Online Registrant', Service_R='E-5', Service_B='Army', HHUSA_hire=0, Com_Method='Telephone', responsive__c=1, active__c='Yes', Created_Linkedin=1, Educ=['High School/GED'], hired=0.0, Occupation='91F', Com_MethodIndex=0.0, hiredIndex=0.0, active__cIndex=0.0, Client_TypeIndex=0.0, Service_BIndex=0.0, Service_RIndex=1.0, active__cIndexVec=SparseVector(1, {0: 1.0}), Service_RIndexVec=SparseVector(25, {1: 1.0}), Client_TypeIndexVec=SparseVector(4, {0: 1.0}), Com_MethodIndexVec=SparseVector(4, {0: 1.0}), Service_BIndexVec=SparseVector(4, {0: 1.0}))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfspark.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfspark = dfspark.withColumn(\n",
    "    \"Occupation\",\n",
    "    split(regexp_replace(\"Occupation\", r\"(^\\[\\[\\[)|(\\]\\]\\]$)\", \"\"), \", \")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2Vec = Word2Vec(minCount=0, inputCol=\"Educ\", outputCol=\"EducVec\")\n",
    "modelw = word2Vec.fit(dfspark)\n",
    "dfspark = modelw.transform(dfspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('S_years', 'double'),\n",
       " ('R_years', 'double'),\n",
       " ('Resume_Tips', 'int'),\n",
       " ('Resume_OnFile', 'int'),\n",
       " ('Client', 'int'),\n",
       " ('Client_Type', 'string'),\n",
       " ('Service_R', 'string'),\n",
       " ('Service_B', 'string'),\n",
       " ('HHUSA_hire', 'int'),\n",
       " ('Com_Method', 'string'),\n",
       " ('responsive__c', 'int'),\n",
       " ('active__c', 'string'),\n",
       " ('Created_Linkedin', 'int'),\n",
       " ('Educ', 'array<string>'),\n",
       " ('hired', 'double'),\n",
       " ('Occupation', 'array<string>'),\n",
       " ('Service_BIndex', 'double'),\n",
       " ('Com_MethodIndex', 'double'),\n",
       " ('active__cIndex', 'double'),\n",
       " ('Service_RIndex', 'double'),\n",
       " ('Client_TypeIndex', 'double'),\n",
       " ('hiredIndex', 'double'),\n",
       " ('active__cIndexVec', 'vector'),\n",
       " ('Service_RIndexVec', 'vector'),\n",
       " ('Client_TypeIndexVec', 'vector'),\n",
       " ('Com_MethodIndexVec', 'vector'),\n",
       " ('Service_BIndexVec', 'vector'),\n",
       " ('EducVec', 'vector')]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfspark.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now put our newly created features into a vector using the vectorizer function\n",
    "assembler2 = VectorAssembler(inputCols=['Resume_Tips','Resume_OnFile','Client','HHUSA_hire','responsive__c','Created_Linkedin','active__cIndexVec','Service_RIndexVec','Client_TypeIndexVec','Com_MethodIndexVec','Service_BIndexVec','EducVec'],outputCol=\"featurevect\")\n",
    "dfspark = assembler2.transform(dfspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(S_years=8.11, R_years=-0.37, Resume_Tips=1, Resume_OnFile=1, Client=1, Client_Type='Online Registrant', Service_R='E-5', Service_B='Army', HHUSA_hire=0, Com_Method='Telephone', responsive__c=1, active__c='Yes', Created_Linkedin=1, Educ=['High School/GED'], hired='0', Occupation=['91F'], Service_BIndex=0.0, Client_TypeIndex=0.0, Service_RIndex=1.0, active__cIndex=0.0, hiredIndex=0.0, Com_MethodIndex=0.0, active__cIndexVec=SparseVector(1, {0: 1.0}), Service_RIndexVec=SparseVector(25, {1: 1.0}), Client_TypeIndexVec=SparseVector(4, {0: 1.0}), Com_MethodIndexVec=SparseVector(4, {0: 1.0}), Service_BIndexVec=SparseVector(4, {0: 1.0}), EducVec=DenseVector([0.0006, -0.0002, 0.0045, -0.0022, 0.004, 0.0033, 0.0028, 0.003, 0.0011, 0.0044, -0.0049, -0.0005, 0.0039, 0.0, -0.0021, 0.0007, -0.0027, 0.0036, 0.0039, -0.0022, -0.0043, 0.0024, 0.0023, 0.0044, 0.0043, 0.0042, -0.0044, 0.0008, -0.001, 0.0032, 0.0028, -0.0008, -0.0008, 0.0034, 0.0012, 0.0013, 0.0039, -0.0032, 0.0011, 0.0027, 0.001, 0.0011, -0.0036, 0.0038, 0.0029, 0.0028, -0.0036, -0.0043, 0.0031, 0.002, 0.0017, -0.0015, 0.0024, -0.0034, -0.0042, 0.0029, 0.0048, 0.001, -0.0, 0.0031, -0.0038, 0.0014, 0.0028, -0.0, 0.0038, -0.0032, 0.0004, -0.0001, 0.0044, 0.0032, -0.0012, 0.0048, 0.0016, -0.0004, -0.0048, -0.0021, -0.0031, 0.0008, 0.0024, -0.004, 0.0048, -0.0012, 0.0013, 0.0048, 0.0036, 0.0014, 0.0005, -0.0011, 0.0041, 0.0034, -0.0021, 0.0043, 0.0038, -0.0019, -0.0031, 0.0016, -0.0023, -0.0031, -0.0033, 0.0009]), features=DenseVector([1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0006, -0.0002, 0.0045, -0.0022, 0.004, 0.0033, 0.0028, 0.003, 0.0011, 0.0044, -0.0049, -0.0005, 0.0039, 0.0, -0.0021, 0.0007, -0.0027, 0.0036, 0.0039, -0.0022, -0.0043, 0.0024, 0.0023, 0.0044, 0.0043, 0.0042, -0.0044, 0.0008, -0.001, 0.0032, 0.0028, -0.0008, -0.0008, 0.0034, 0.0012, 0.0013, 0.0039, -0.0032, 0.0011, 0.0027, 0.001, 0.0011, -0.0036, 0.0038, 0.0029, 0.0028, -0.0036, -0.0043, 0.0031, 0.002, 0.0017, -0.0015, 0.0024, -0.0034, -0.0042, 0.0029, 0.0048, 0.001, -0.0, 0.0031, -0.0038, 0.0014, 0.0028, -0.0, 0.0038, -0.0032, 0.0004, -0.0001, 0.0044, 0.0032, -0.0012, 0.0048, 0.0016, -0.0004, -0.0048, -0.0021, -0.0031, 0.0008, 0.0024, -0.004, 0.0048, -0.0012, 0.0013, 0.0048, 0.0036, 0.0014, 0.0005, -0.0011, 0.0041, 0.0034, -0.0021, 0.0043, 0.0038, -0.0019, -0.0031, 0.0016, -0.0023, -0.0031, -0.0033, 0.0009]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfspark.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|         featurevect|hired|\n",
      "+--------------------+-----+\n",
      "|[1.0,1.0,1.0,0.0,...|  0.0|\n",
      "|[1.0,1.0,1.0,1.0,...|  0.0|\n",
      "|[1.0,1.0,1.0,0.0,...|  0.0|\n",
      "|[1.0,1.0,1.0,0.0,...|  0.0|\n",
      "|[0.0,1.0,1.0,0.0,...|  0.0|\n",
      "|[1.0,1.0,1.0,0.0,...|  0.0|\n",
      "|[1.0,1.0,1.0,0.0,...|  0.0|\n",
      "|[1.0,1.0,1.0,0.0,...|  0.0|\n",
      "|[1.0,1.0,1.0,0.0,...|  0.0|\n",
      "|[0.0,0.0,1.0,0.0,...|  0.0|\n",
      "|[1.0,1.0,1.0,0.0,...|  0.0|\n",
      "|[1.0,1.0,1.0,0.0,...|  0.0|\n",
      "|[1.0,1.0,1.0,0.0,...|  0.0|\n",
      "|[1.0,1.0,1.0,1.0,...|  0.0|\n",
      "|[1.0,1.0,1.0,0.0,...|  0.0|\n",
      "|[1.0,1.0,1.0,1.0,...|  0.0|\n",
      "|[0.0,1.0,1.0,1.0,...|  0.0|\n",
      "|[1.0,1.0,1.0,0.0,...|  0.0|\n",
      "|[0.0,0.0,1.0,0.0,...|  0.0|\n",
      "|[1.0,1.0,1.0,1.0,...|  0.0|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfspark.select('featurevect','hired').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data uniformly\n",
    "scaler2 = StandardScaler(inputCol=\"featurevect\", outputCol=\"features\")\n",
    "scalerModel = scaler2.fit(dfspark)\n",
    "dfspark = scalerModel.transform(dfspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(hired=0.0, features=DenseVector([2.1702, 2.5864, 0.0, 0.0, 2.8238, 2.0136, 9.2445, 0.0, 2.4497, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.8913, 0.0, 0.0, 0.0, 2.5965, 0.0, 0.0, 0.0, 2.009, 0.0, 0.0, 0.0, 0.0635, -0.0142, 0.0689, 0.0125, 0.0133, -0.0059, -0.0231, -0.0398, -0.0155, 0.0026, -0.0724, 0.0363, 0.0434, 0.0286, -0.0137, 0.0389, 0.0176, 0.0234, -0.0579, 0.0149, 0.0188, -0.0503, -0.0475, -0.0449, -0.0004, 0.0037, -0.0566, 0.0056, 0.011, 0.0443, 0.0202, -0.0016, -0.0067, 0.0171, -0.0024, -0.0271, -0.052, -0.0147, 0.0333, 0.021, -0.0429, -0.0355, 0.0139, -0.0197, 0.0208, -0.0595, -0.0119, 0.0048, -0.0022, 0.0154, -0.0549, 0.001, 0.0804, -0.0033, 0.0231, -0.0235, 0.0158, 0.0047, 0.0052, -0.0126, -0.0358, -0.0079, -0.0491, -0.0562, -0.018, 0.0331, 0.04, 0.0533, -0.0221, 0.0346, -0.0399, 0.0022, -0.0334, 0.0062, -0.0086, 0.0203, -0.0935, -0.0296, 0.0044, 0.17, 0.0133, 0.0212, -0.0142, -0.027, -0.0061, 0.1119, 0.0115, -0.02, 0.1037, 0.0521, 0.0528, -0.006, -0.1269, -0.0394, -0.0117, 0.0263, -0.0023, 0.0327, -0.0274, -0.049]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfspark.select('hired','features').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the data type of the the hired column from string to double\n",
    "dfspark = dfspark.withColumn(\"hired\", dfspark[\"hired\"].cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,test_data=dfspark.select('features','hired').randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree = DecisionTreeClassifier(labelCol='hired', featuresCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = decision_tree.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model1.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(features=DenseVector([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 9.2445, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 11.0881, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.8913, 0.0, 0.0, 0.0, 2.5965, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.9716, -0.2374, -1.8123, 1.1851, 1.0588, 1.6715, 0.8131, -1.0705, -0.473, -0.7603, 0.6095, -1.175, 1.3379, 1.064, 1.102, 0.2272, 1.7169, -1.4341, -1.0796, 0.0412, 1.4236, 1.3448, 0.9022, 0.0683, -1.1246, 1.7862, -1.6821, -2.1687, -1.8141, 1.4642, -2.0638, 0.9235, -0.6705, 0.3531, -0.5954, -0.2825, 1.3547, -0.2076, -0.6091, -0.5387, -1.433, 1.6622, -1.6802, -0.6257, -1.5959, -0.5235, -0.007, -1.0847, 2.0039, 1.5939, 1.364, 1.217, -0.8326, -0.2589, -0.6978, -0.6373, -1.1174, -1.7657, 0.9841, 1.4137, -1.1052, 1.1403, -1.4928, 0.4257, -0.6137, 1.5075, 1.8168, 0.0113, 1.7317, -1.907, 0.5285, 0.8976, -1.5635, 0.843, -0.4441, -0.9456, 0.5735, -1.8887, -1.0578, -0.1059, -2.1784, 1.1683, 0.4057, -0.0122, 1.031, 1.1531, 1.877, -0.0493, -1.2581, -0.8936, 1.7233, -1.1166, -1.3324, 1.3369, 1.0475, -0.0457, 0.5902, -1.1297, -1.9185, -0.1511, -1.1642]), hired=0.0, rawPrediction=DenseVector([19392.0, 15.0]), probability=DenseVector([0.9992, 0.0008]), prediction=0.0)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['features', 'hired', 'rawPrediction', 'probability', 'prediction']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'evaluate_algo' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-f8fae98a4263>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mevaluate_algo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'hired'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Decision Tree'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'evaluate_algo' is not defined"
     ]
    }
   ],
   "source": [
    "evaluate_algo(pred,'hired','Decision Tree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest = RandomForestClassifier(labelCol='hired', featuresCol='features', numTrees=28)\n",
    "model2= random_forest.fit(train_data)\n",
    "predrand = model2.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scaledFeatures', 'hired', 'rawPrediction', 'probability', 'prediction']"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predrand.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Tree\n",
      "Test Error  = 0.274526 \n",
      "Accuracy   = 0.725474 \n"
     ]
    }
   ],
   "source": [
    "evaluate_algo(predrand,'hired','Random Tree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "|[2.17015087637283...|\n",
      "|[2.17015087637283...|\n",
      "|[2.17015087637283...|\n",
      "|[2.17015087637283...|\n",
      "|[0.0,2.5863992700...|\n",
      "|[2.17015087637283...|\n",
      "|[2.17015087637283...|\n",
      "|[2.17015087637283...|\n",
      "|[2.17015087637283...|\n",
      "|[0.0,0.0,0.0,0.0,...|\n",
      "|[2.17015087637283...|\n",
      "|[2.17015087637283...|\n",
      "|[2.17015087637283...|\n",
      "|[2.17015087637283...|\n",
      "|[2.17015087637283...|\n",
      "|[2.17015087637283...|\n",
      "|[0.0,2.5863992700...|\n",
      "|[2.17015087637283...|\n",
      "|[0.0,0.0,0.0,0.0,...|\n",
      "|[2.17015087637283...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfspark.select('features').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the data type of the the hired column from string to double\n",
    "dfspark = dfspark.withColumn(\"hired\", dfspark[\"hired\"].cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "features does not exist. Available: scaledFeatures",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-fe4644baf156>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdfspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'scaledFeatures'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mkmeans2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 2 clusters here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodelk2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkmeans2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m#dfspark = modelk.transform(dfspark)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    127\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    316\u001b[0m         \"\"\"\n\u001b[1;32m    317\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    135\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: features does not exist. Available: scaledFeatures"
     ]
    }
   ],
   "source": [
    "b = dfspark.select('scaledFeatures')\n",
    "kmeans2 = KMeans(k=4, seed=1)  # 2 clusters here\n",
    "modelk2 = kmeans2.fit(b)\n",
    "#dfspark = modelk.transform(dfspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing largescalefinal1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile largescalefinal1.py\n",
    "from datetime import datetime\n",
    "# Start time of training the model \n",
    "\n",
    "start = datetime.now()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer, OneHotEncoder, Word2Vec, FeatureHasher \n",
    "from pyspark.ml.feature import StandardScaler, VectorAssembler \n",
    "from pyspark.ml.clustering import KMeans \n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import split, regexp_replace\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "from pyspark.sql.functions import col, regexp_replace, split, udf\n",
    "from pyspark.ml.classification import DecisionTreeClassifier, RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator \n",
    "#import copy\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Large Scale Project \") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "#Input files \n",
    "inputFileHire =\"/user/user17/mosedata_proj/input/client_hiring_dt.csv\"\n",
    "inputFileBio =\"/user/user17/mosedata_proj/input/client_bio_dt.csv\"\n",
    "inputFileCom = \"/user/user17/mosedata_proj/input/client_communication_dt.csv\"\n",
    "inputFileAct = \"/user/user17/mosedata_proj/input/client_activities_dt.csv\"\n",
    "inputFileFact = \"/user/user17/mosedata_proj/input/client_fact_ft.csv\"\n",
    "\n",
    "\n",
    "# Create DataFrame from CSV file\n",
    "dfHire =  spark.read.format(\"csv\").option(\"sep\", \",\").option(\"inferSchema\", \"true\").option(\"header\", \"true\").load(inputFileHire)\n",
    "dfBio =  spark.read.format(\"csv\").option(\"sep\", \",\").option(\"inferSchema\", \"true\").option(\"header\", \"true\").load(inputFileBio)\n",
    "dfCom =  spark.read.format(\"csv\").option(\"sep\", \",\").option(\"inferSchema\", \"true\").option(\"header\", \"true\").load(inputFileCom)\n",
    "dfAct =  spark.read.format(\"csv\").option(\"sep\", \",\").option(\"inferSchema\", \"true\").option(\"header\", \"true\").load(inputFileAct)\n",
    "dfFact =  spark.read.format(\"csv\").option(\"sep\", \",\").option(\"inferSchema\", \"true\").option(\"header\", \"true\").load(inputFileFact)\n",
    "\n",
    "\n",
    "# Print the schema of the DataFrames\n",
    "dfHire.printSchema() \n",
    "dfBio.printSchema()\n",
    "dfCom.printSchema()\n",
    "dfAct.printSchema()\n",
    "dfFact.printSchema()\n",
    "\n",
    "\n",
    "#Temporary objects from which sql statements can be run for each dataframe\n",
    "dfHire.createOrReplaceTempView(\"dfHire_sql\")\n",
    "dfBio.createOrReplaceTempView(\"dfBio_sql\")\n",
    "dfCom.createOrReplaceTempView(\"dfCom_sql\")\n",
    "dfAct.createOrReplaceTempView(\"dfAct_sql\")\n",
    "dfFact.createOrReplaceTempView(\"dfFAct_sql\")\n",
    "\n",
    "#Hired by Service branch\n",
    "spark.sql(\"\"\" Select count(hires.id) as hired,bio.service_branch__c \n",
    "              From dfHire_sql hires \n",
    "              inner join dfBio_sql bio on bio.id = hires.id\n",
    "              Where hires.hired = 1\n",
    "              group by bio.service_branch__c\n",
    "              order by hired asc\n",
    "              \"\"\").show()\n",
    "\n",
    "#Not hired by Service Branch\n",
    "spark.sql(\"\"\" Select count(hires.id) as Not_hired,bio.service_branch__c \n",
    "              From dfHire_sql hires \n",
    "              inner join dfBio_sql bio on bio.id = hires.id\n",
    "              Where hires.hired = 0\n",
    "              group by bio.service_branch__c\n",
    "              order by Not_hired asc\n",
    "              \"\"\").show()\n",
    "\n",
    "#Hired by rank\n",
    "spark.sql(\"\"\" Select count(hires.id) as hired,bio.service_rank__c\n",
    "              From dfHire_sql hires \n",
    "              inner join dfBio_sql bio on bio.id = hires.id\n",
    "              Where hires.hired = 1\n",
    "              group by bio.service_rank__c\n",
    "              order by hired desc\n",
    "              \"\"\").show()\n",
    "\n",
    "#Prefered method of contact for the hired\n",
    "spark.sql(\"\"\" select count(com.id) as count, com.preferred_method_of_contact__c \n",
    "              from dfCom_sql com \n",
    "              inner join dfHire_sql hires on com.id = hires.id\n",
    "              where hires.hired = 1\n",
    "              group by com.preferred_method_of_contact__c \n",
    "              order by count\n",
    "          \"\"\").show()\n",
    "\n",
    "\n",
    "#Prefered method of contact for the Unhired\n",
    "spark.sql(\"\"\" select count(com.id) as count, com.preferred_method_of_contact__c \n",
    "              from dfCom_sql com \n",
    "              inner join dfHire_sql hires on com.id = hires.id\n",
    "              where hires.hired = 0\n",
    "              group by com.preferred_method_of_contact__c \n",
    "              order by count\n",
    "          \"\"\").show()\n",
    "\n",
    "#Select variables that determine whether one is going to be employed or not, store those varibles in a new dataframe\n",
    "dfspark= spark.sql(\"\"\" select fact.yearsinservice as S_years , fact.reg_afterservice_years as R_years,\n",
    "               act.resume_tailoring_tips__c as Resume_Tips,act.finalized_hhusa_revised_resume_on_file__c as Resume_OnFile, bio.client__c as Client, bio.client_type__c as Client_Type, bio.service_rank__c as Service_R\n",
    "              ,bio.service_branch__c as Service_B, hire.hire_heroes_usa_confirmed_hire__c as HHUSA_hire, com.preferred_method_of_contact__c as Com_Method, com.responsive__c\n",
    "              ,com.active__c, act.created_linkedin_account__c as Created_Linkedin, bio.highest_level_of_education_completed__c as Educ,  hire.hired\n",
    "              ,bio.primary_military_occupational_specialty__c as Occupation\n",
    "               \n",
    "              from dfFact_sql fact\n",
    "              \n",
    "              inner join dfAct_sql act on act.id = fact.id\n",
    "              inner join dfBio_sql bio on bio.id = fact.id\n",
    "              inner join dfHire_sql hire on hire.id = fact.id\n",
    "              inner join dfCom_sql com on com.id = fact.id\n",
    "              \n",
    "              where bio.client_type__c != '' and bio.service_branch__c !='' and bio.service_rank__c !='' \n",
    "              and com.preferred_method_of_contact__c !='' and fact.yearsinservice < 53 and fact.yearsinservice > 0 \n",
    "              and fact.reg_afterservice_years >- 32 and fact.reg_afterservice_years < 53 and com.active__c !=''\n",
    "              and bio.highest_level_of_education_completed__c !='' and hire.hired  !='' and hire.hired !='No' and hire.hired != 3\n",
    "\n",
    "          \"\"\")\n",
    "dfspark.persist()\n",
    "\n",
    "dfsparkcopy = dfspark\n",
    "\n",
    "# Get the number of clients in the description above\n",
    "dfspark.count()\n",
    "\n",
    "# Change the data type of the the hired column from string to double\n",
    "dfspark = dfspark.withColumn(\"hired\", dfspark[\"hired\"].cast(DoubleType()))\n",
    "\n",
    "# Confirm the labels in the hired column 0 represents the client was not hired 1 represents they were hired. \n",
    "dfspark.select('hired').distinct().show()\n",
    "\n",
    "\n",
    "# The data types of the data frame \n",
    "print(\" \")\n",
    "print(\"The data types of the data frame. We need to convert them to indexes \\\n",
    "      vectors and then assemble into features, scale then apply to Machine learning algorithms. They accept only vectors\" )\n",
    "print(\" \") \n",
    "print (dfspark.dtypes)\n",
    "print(\" \")\n",
    "#Transform the data frame categorical columns to indexes using the StringIndexer method. \n",
    "indexers = [StringIndexer(inputCol=i, outputCol=i+\"Index\").fit(dfspark) \\\n",
    "            for i in list(set(dfspark.columns)- set(['S_years','R_years','Resume_Tips','Resume_OnFile','Client','HHUSA_hire','responsive__c','Created_Linkedin','Occupation','Educ'])) ]\n",
    "pipeline = Pipeline(stages=indexers)\n",
    "dfspark = pipeline.fit(dfspark).transform(dfspark)\n",
    "\n",
    "\n",
    "#One hot encode the converted columns\n",
    "encoder = OneHotEncoder(inputCols=[\"Service_BIndex\", \"Com_MethodIndex\",\"active__cIndex\", \"Service_RIndex\", \"Client_TypeIndex\"],\n",
    "                       outputCols=[\"Service_BIndexVec\", \"Com_MethodIndexVec\",\"active__cIndexVec\", \"Service_RIndexVec\", \"Client_TypeIndexVec\"])\n",
    "modelOne = encoder.fit(dfspark)\n",
    "dfspark = modelOne.transform(dfspark)\n",
    "\n",
    "\n",
    "\n",
    "#Convert the Education column from string to array of words using regular expressions. \n",
    "dfspark = dfspark.withColumn(\"Educ\",split(regexp_replace(\"Educ\", r\"(^\\[\\[\\[)|(\\]\\]\\]$)\", \"\"), \", \")\n",
    ")\n",
    "\n",
    "#Convert the words to vector in the Education column\n",
    "word2Vec = Word2Vec(inputCol=\"Educ\", outputCol=\"EducVec\")\n",
    "modelw = word2Vec.fit(dfspark)\n",
    "dfspark = modelw.transform(dfspark)\n",
    "\n",
    "# We can now put our newly created feature vectors  into one feature vector using the vectorizer. \n",
    "assembler2 = VectorAssembler(inputCols=['Resume_Tips','Resume_OnFile','Client','HHUSA_hire','responsive__c','Created_Linkedin','active__cIndexVec','Service_RIndexVec','Client_TypeIndexVec','Com_MethodIndexVec','Service_BIndexVec','EducVec'],outputCol=\"vecfeatures\")\n",
    "dfspark = assembler2.transform(dfspark)\n",
    "\n",
    "\n",
    "# Scale the data uniformly\n",
    "scaler2 = StandardScaler(inputCol=\"vecfeatures\", outputCol=\"features\")\n",
    "scalerModel = scaler2.fit(dfspark)\n",
    "dfspark = scalerModel.transform(dfspark)\n",
    "\n",
    "#Train test split\n",
    "train_data,test_data=dfspark.select('features','hired').randomSplit([0.7,0.3])\n",
    "\n",
    "print(\"Classification with Decision trees and Random Forests. \")\n",
    "#Decision tree classifier will predict whether a client can be hired or not 1 or 0 using the hired and scaledFeatures colums\n",
    "decision_tree = DecisionTreeClassifier(labelCol='hired', featuresCol='features')\n",
    "model1 = decision_tree.fit(train_data)\n",
    "pred = model1.transform(test_data)\n",
    "\n",
    "#Check the new columns of the dataframe \n",
    "pred.columns\n",
    "\n",
    "\n",
    "# Now lets evaluate the algorithm performance\n",
    "def evaluate_algo(df,label,Algorithm_name  ):\n",
    "    classification_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=label, predictionCol=\"prediction\")\n",
    "    accuracy = classification_evaluator.evaluate(df)\n",
    "    print(Algorithm_name )\n",
    "    print(\"Test Error  = %g \" % (1.0 - accuracy))\n",
    "    print(\"Accuracy   = %g \" % accuracy)\n",
    "\n",
    "\n",
    "#Evaluate the algorithm performance \n",
    "evaluate_algo(pred,'hired','Decision Tree')\n",
    "\n",
    "#Compare with another machine learning algorithm Random forests \n",
    "print(\" \")\n",
    "random_forest = RandomForestClassifier(labelCol='hired', featuresCol='features', numTrees=28)\n",
    "model2= random_forest.fit(train_data)\n",
    "predrand = model2.transform(test_data)\n",
    "\n",
    "#Columns predicted for random forests\n",
    "predrand.columns\n",
    "\n",
    "#Evaluate the performance of random forests\n",
    "evaluate_algo(predrand,'hired','Random Forests')\n",
    "\n",
    "\n",
    "#Clustering\n",
    "print(\" \")\n",
    "print(\"Clustering with KMEANS\")\n",
    "funchash = FeatureHasher(inputCols= ['S_years','R_years','Resume_Tips','Resume_OnFile','Client','Client_Type','Service_R',\\\n",
    " 'Service_B','HHUSA_hire','Com_Method','responsive__c','active__c','Created_Linkedin','Educ','Occupation'],\n",
    "                       outputCol=\"features\")\n",
    "\n",
    "dfsparkcopy = funchash.transform(dfsparkcopy)\n",
    "\n",
    "def cluster(k,df,features):\n",
    "    \"\"\" Takes in k as an int, the dataframe, and features \"\"\"\n",
    "    kmeans = KMeans(k=k, seed=1)  # 2 clusters here\n",
    "    model = kmeans.fit(df.select(features))\n",
    "    return model\n",
    "\n",
    "transformeddf = cluster(4,dfsparkcopy,'features').transform(dfsparkcopy)\n",
    "\n",
    "#Number of clusters\n",
    "\n",
    "print (\"Number of clusters from Kmeans\")\n",
    "transformeddf.select('prediction').distinct().show()\n",
    "\n",
    "#Service branch with Service Rank, Showing whether hired or not. \n",
    "print(\" \")\n",
    "print(\" Service branches with Service Ranks and cluster as prediction\")\n",
    "transformeddf.select('Service_B','Service_R','hired','prediction').distinct().show()\n",
    "\n",
    "\n",
    "#Information about individual clusters in term of years service men stay in the army and when they register for services\n",
    "print(\" \")\n",
    "print (\"Statistical Information about years in service and Registration for services with HHUSA\")\n",
    "transformeddf.select('S_years','R_years').describe().show()\n",
    "\n",
    "#Get cluster information Cluster 0 in terms of years service men spend on service.\n",
    "print(\" \")\n",
    "print (\"Years clustered with cluster 0 \")\n",
    "cluster0 = transformeddf.filter(transformeddf.prediction==0)\n",
    "cluster0.select('S_years','R_years').describe().show()\n",
    "\n",
    "#Get cluster information Cluster 1\n",
    "\n",
    "print (\"Years clustered with cluster 1 \")\n",
    "cluster1 = transformeddf.filter(transformeddf.prediction==1)\n",
    "cluster1.select('S_years','R_years').describe().show()\n",
    "\n",
    "#Get cluster information Cluster 2\n",
    "print(\" \")\n",
    "print (\"Years clustered with cluster 2\")\n",
    "cluster2 = transformeddf.filter(transformeddf.prediction==2)\n",
    "cluster2.select('S_years','R_years').describe().show()\n",
    "\n",
    "\n",
    "#Get cluster information Cluster 3\n",
    "print(\" \")\n",
    "print (\"Years clustered with cluster 3 \")\n",
    "cluster3 = transformeddf.filter(transformeddf.prediction==3)\n",
    "cluster3.select('S_years','R_years').describe().show()\n",
    "\n",
    "\n",
    "end = datetime.now()\n",
    "\n",
    "print('Duration of script: {}'.format(end - start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "# Loads data.\n",
    "dataset = dfspark.select('features')\n",
    "\n",
    "# Trains a k-means model.\n",
    "kmeans = KMeans().setK(4).setSeed(1)\n",
    "model = kmeans.fit(dataset)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.transform(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|            features|prediction|\n",
      "+--------------------+----------+\n",
      "|[1.0,1.0,1.0,0.0,...|         3|\n",
      "|[1.0,1.0,1.0,1.0,...|         1|\n",
      "|[1.0,1.0,1.0,0.0,...|         0|\n",
      "|[1.0,1.0,1.0,0.0,...|         2|\n",
      "|[0.0,1.0,1.0,0.0,...|         3|\n",
      "|[1.0,1.0,1.0,0.0,...|         3|\n",
      "|[1.0,1.0,1.0,0.0,...|         0|\n",
      "|[1.0,1.0,1.0,0.0,...|         3|\n",
      "|[1.0,1.0,1.0,0.0,...|         3|\n",
      "|[0.0,0.0,1.0,0.0,...|         3|\n",
      "|[1.0,1.0,1.0,0.0,...|         0|\n",
      "|[1.0,1.0,1.0,0.0,...|         0|\n",
      "|[1.0,1.0,1.0,0.0,...|         3|\n",
      "|[1.0,1.0,1.0,1.0,...|         0|\n",
      "|[1.0,1.0,1.0,0.0,...|         0|\n",
      "|[1.0,1.0,1.0,1.0,...|         0|\n",
      "|[0.0,1.0,1.0,1.0,...|         1|\n",
      "|[1.0,1.0,1.0,0.0,...|         3|\n",
      "|[0.0,0.0,1.0,0.0,...|         2|\n",
      "|[1.0,1.0,1.0,1.0,...|         3|\n",
      "+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clustering \n",
    "def cluster(k,df,features):\n",
    "    \"\"\" Takes in k as an int, the dataframe, and features \"\"\"\n",
    "    kmeans = KMeans(k=k, seed=1)  # 2 clusters here\n",
    "    model = kmeans.fit(df.select(features))\n",
    "    return model\n",
    "\n",
    "transformeddf = cluster(4,dfspark,'features').transform(dfspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|prediction|\n",
      "+----------+\n",
      "|         3|\n",
      "|         1|\n",
      "|         0|\n",
      "|         2|\n",
      "|         3|\n",
      "|         3|\n",
      "|         0|\n",
      "|         3|\n",
      "|         3|\n",
      "|         3|\n",
      "|         0|\n",
      "|         0|\n",
      "|         3|\n",
      "|         0|\n",
      "|         0|\n",
      "|         0|\n",
      "|         1|\n",
      "|         3|\n",
      "|         2|\n",
      "|         3|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformeddf.select('prediction').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

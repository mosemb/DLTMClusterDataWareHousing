{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"DatawarehouseSpark Application\") \\\n",
    "    .getOrCreate()\n",
    " \n",
    "inputFileHire ='/home/mose/Downloads/CSVS/client_hiring_dt.csv' #\"/user/user17/mosedata_proj/input/client_hiring_dt.csv\"\n",
    "#inputFileBio ='/home/mose/Downloads/CSVS/client_bio_dt.csv'  #\"/user/user17/mosedata_proj/input/client_bio_dt.csv\"\n",
    "#inputFileCom = '/home/mose/Downloads/CSVS/client_communication_dt.csv' #\"/user/user17/mosedata_proj/input/client_communication_dt.csv\"\n",
    "#inputFileAct = '/home/mose/Downloads/CSVS/client_activities_dt.csv' #\"/user/user17/mosedata_proj/input/client_activities_dt.csv\"\n",
    "#inputFileFact = '/home/mose/Downloads/CSVS/client_fact_ft.csv' #\"/user/user17/mosedata_proj/input/client_fact_ft.csv\"\n",
    "\n",
    " \n",
    "# Create DataFrame from CSV file\n",
    "dfM =  spark.read.format(\"csv\").option(\"sep\", \",\").option(\"inferSchema\", \"true\").option(\"header\", \"true\").load(inputFileHire)\n",
    "#dfR =  spark.read.format(\"csv\").option(\"sep\", \",\").option(\"inferSchema\", \"true\").option(\"header\", \"true\").load(inputFileBio)\n",
    "#dfCom =  spark.read.format(\"csv\").option(\"sep\", \",\").option(\"inferSchema\", \"true\").option(\"header\", \"true\").load(inputFileCom)\n",
    "#dfAct =  spark.read.format(\"csv\").option(\"sep\", \",\").option(\"inferSchema\", \"true\").option(\"header\", \"true\").load(inputFileAct)\n",
    "#dfFact =  spark.read.format(\"csv\").option(\"sep\", \",\").option(\"inferSchema\", \"true\").option(\"header\", \"true\").load(inputFileFact)\n",
    "\n",
    "\n",
    "# Print the schema of the DataFrames\n",
    "dfM.printSchema()\n",
    "dfR.printSchema()\n",
    "dfCom.printSchema()\n",
    "dfAct.printSchema()\n",
    "dfFact.printSchema()\n",
    "\n",
    "\n",
    "#dfR.groupBy('movieId').count().show(50)\n",
    "#dfM.where(\"director='Martin Brest'\").join(dfR,\"movieId\").select(\"movieId\",\"director\",\"user_name\",\"rating\").show(50)\n",
    "\n",
    "\n",
    "# Stop the Spark Session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting dataframes.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile dataframes.py\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"DatawarehouseSpark Application\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "#Input the data\n",
    "inputFileHire ='/user/user17/mosedata_proj/input/client_hiring_dt.csv'\n",
    "inputFileBio ='/user/user17/mosedata_proj/input/client_bio_dt.csv'\n",
    "inputFileCom = '/user/user17/mosedata_proj/input/client_communication_dt.csv'\n",
    "inputFileAct = '/user/user17/mosedata_proj/input/client_activities_dt.csv'\n",
    "inputFileFact = '/user/user17/mosedata_proj/input/client_fact_ft.csv'\n",
    "\n",
    "#Create data frames from sources. Tables were converted to csvs now being converted to dataframes\n",
    "dfHire =  spark.read.format(\"csv\").option(\"sep\", \",\").option(\"inferSchema\", \"true\").option(\"header\", \"true\").load(inputFileHire)\n",
    "dfBio =  spark.read.format(\"csv\").option(\"sep\", \",\").option(\"inferSchema\", \"true\").option(\"header\", \"true\").load(inputFileBio)\n",
    "dfCom =  spark.read.format(\"csv\").option(\"sep\", \",\").option(\"inferSchema\", \"true\").option(\"header\", \"true\").load(inputFileCom)\n",
    "dfAct =  spark.read.format(\"csv\").option(\"sep\", \",\").option(\"inferSchema\", \"true\").option(\"header\", \"true\").load(inputFileAct)\n",
    "dfFact =  spark.read.format(\"csv\").option(\"sep\", \",\").option(\"inferSchema\", \"true\").option(\"header\", \"true\").load(inputFileFact)\n",
    "\n",
    "# Print the schema of the DataFrames\n",
    "dfHire.printSchema() \n",
    "dfBio.printSchema()\n",
    "dfCom.printSchema()\n",
    "dfAct.printSchema()\n",
    "dfFact.printSchema()\n",
    "\n",
    "#Clients from different service branches\n",
    "dfBio.groupBy('service_branch__c').count().show()\n",
    "\n",
    "#Types of jobs the guys got hired in\n",
    "dfHire.groupBy('job_function_hired_in__c').count().show()\n",
    "\n",
    "#Temporary objects from which sql statements are run for each dataframe\n",
    "dfHire.createOrReplaceTempView(\"dfHire_sql\")\n",
    "dfBio.createOrReplaceTempView(\"dfBio_sql\")\n",
    "dfCom.createOrReplaceTempView(\"dfCom_sql\")\n",
    "dfAct.createOrReplaceTempView(\"dfAct_sql\")\n",
    "dfFact.createOrReplaceTempView(\"dfFAct_sql\")\n",
    "\n",
    "#Mixed information from all 5 tables of the database\n",
    "spark.sql(''' Select hires.hired, bio.service_rank__c , \n",
    "              bio.service_branch__c , fact.yearsinservice, fact.reg_afterservice_years,\n",
    "              com.responsive__c, act.finalized_hhusa_revised_resume_on_file__c as resume_done\n",
    "              from dfHire_sql hires\n",
    "              inner join dfBio_sql bio on bio.id = hires.id \n",
    "              inner join dfFAct_sql fact on fact.id = hires.id\n",
    "              inner join dfCom_sql com on com.id = hires.id\n",
    "              inner join dfAct_sql act on act.id = hires.id\n",
    "              where hires.hired = \"1\"\n",
    "              \n",
    "              ''').show()\n",
    "\n",
    "#Account create date by month of the year\n",
    "spark.sql( \"\"\"SELECT count(id) , \n",
    "              Extract(Month From create_ddate) as month \n",
    "              From  dfAct_sql  \n",
    "              group by month \n",
    "              ORDER BY month ASC \"\"\").show()\n",
    "\n",
    "\n",
    "#Registration for services by year\n",
    "spark.sql( \"\"\"SELECT count(id) , \n",
    "Extract(Year From create_ddate) as year_registered \n",
    "              From  dfAct_sql  \n",
    "              group by year_registered\n",
    "              ORDER BY year_registered ASC \"\"\").show()\n",
    "\n",
    "#Hired by Service branch\n",
    "spark.sql(\"\"\" Select count(hires.id) as hired,bio.service_branch__c \n",
    "              From dfHire_sql hires \n",
    "              inner join dfBio_sql bio on bio.id = hires.id\n",
    "              Where hires.hired = 1\n",
    "              group by bio.service_branch__c\n",
    "              order by hired asc\n",
    "              \"\"\").show()\n",
    "\n",
    "#Not hired by Service Branch\n",
    "spark.sql(\"\"\" Select count(hires.id) as Not_hired,bio.service_branch__c \n",
    "              From dfHire_sql hires \n",
    "              inner join dfBio_sql bio on bio.id = hires.id\n",
    "              Where hires.hired = 0\n",
    "              group by bio.service_branch__c\n",
    "              order by Not_hired asc\n",
    "              \"\"\").show()\n",
    "\n",
    "#Hired by rank\n",
    "spark.sql(\"\"\" Select count(hires.id) as hired,bio.service_rank__c\n",
    "              From dfHire_sql hires \n",
    "              inner join dfBio_sql bio on bio.id = hires.id\n",
    "              Where hires.hired = 0\n",
    "              group by bio.service_rank__c\n",
    "              order by hired desc\n",
    "              \"\"\").show()\n",
    "\n",
    "#Prefered method of contact for the hired\n",
    "spark.sql(\"\"\" select count(com.id) as count, com.preferred_method_of_contact__c \n",
    "              from dfCom_sql com \n",
    "              inner join dfHire_sql hires on com.id = hires.id\n",
    "              where hires.hired = 1\n",
    "              group by com.preferred_method_of_contact__c \n",
    "              order by count\n",
    "          \"\"\").show()\n",
    "\n",
    "#Prefered method of contact for the Unhired\n",
    "spark.sql(\"\"\" select count(com.id) as count, com.preferred_method_of_contact__c \n",
    "              from dfCom_sql com \n",
    "              inner join dfHire_sql hires on com.id = hires.id\n",
    "              where hires.hired = 0\n",
    "              group by com.preferred_method_of_contact__c \n",
    "              order by count\n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"DatawarehouseSpark Application\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import col, regexp_replace, split\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "from pyspark.ml.feature import FeatureHasher\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from datetime import datetime\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml import Pipeline\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputFileHire ='/home/mose/Downloads/CSVS/client_hiring_dt.csv' #\"/user/user17/mosedata_proj/input/client_hiring_dt.csv\"\n",
    "inputFileBio ='/home/mose/Downloads/CSVS/client_bio_dt.csv'  #\"/user/user17/mosedata_proj/input/client_bio_dt.csv\"\n",
    "inputFileCom = '/home/mose/Downloads/CSVS/client_communication_dt.csv' #\"/user/user17/mosedata_proj/input/client_communication_dt.csv\"\n",
    "inputFileAct = '/home/mose/Downloads/CSVS/client_activities_dt.csv' #\"/user/user17/mosedata_proj/input/client_activities_dt.csv\"\n",
    "inputFileFact = '/home/mose/Downloads/CSVS/client_fact_ft.csv' #\"/user/user17/mosedata_proj/input/client_fact_ft.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame from CSV file\n",
    "dfHire =  spark.read.format(\"csv\").option(\"sep\", \",\").option(\"inferSchema\", \"true\").option(\"header\", \"true\").load(inputFileHire)\n",
    "dfBio =  spark.read.format(\"csv\").option(\"sep\", \",\").option(\"inferSchema\", \"true\").option(\"header\", \"true\").load(inputFileBio)\n",
    "dfCom =  spark.read.format(\"csv\").option(\"sep\", \",\").option(\"inferSchema\", \"true\").option(\"header\", \"true\").load(inputFileCom)\n",
    "dfAct =  spark.read.format(\"csv\").option(\"sep\", \",\").option(\"inferSchema\", \"true\").option(\"header\", \"true\").load(inputFileAct)\n",
    "dfFact =  spark.read.format(\"csv\").option(\"sep\", \",\").option(\"inferSchema\", \"true\").option(\"header\", \"true\").load(inputFileFact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- dsubimittedforhire: string (nullable = true)\n",
      " |-- dconfirmedforhire: string (nullable = true)\n",
      " |-- difsubmitconfirmed: integer (nullable = true)\n",
      " |-- startdate: string (nullable = true)\n",
      " |-- datdifconfirstart: integer (nullable = true)\n",
      " |-- job_type__c: string (nullable = true)\n",
      " |-- job_function_hired_in__c: string (nullable = true)\n",
      " |-- hiring_account__c: string (nullable = true)\n",
      " |-- hire_heroes_usa_confirmed_hire__c: integer (nullable = true)\n",
      " |-- hired_location__c: string (nullable = true)\n",
      " |-- hired_zip_code__c: string (nullable = true)\n",
      " |-- industry_hired_in__c: string (nullable = true)\n",
      " |-- hired_but_still_active_and_looking__c: string (nullable = true)\n",
      " |-- hired: string (nullable = true)\n",
      " |-- client_hiring_dt_id: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- client__c: integer (nullable = true)\n",
      " |-- client_type__c: string (nullable = true)\n",
      " |-- service_rank__c: string (nullable = true)\n",
      " |-- service_branch__c: string (nullable = true)\n",
      " |-- primary_military_occupational_specialty__c: string (nullable = true)\n",
      " |-- gender__c: string (nullable = true)\n",
      " |-- highest_level_of_education_completed__c: string (nullable = true)\n",
      " |-- status__c: string (nullable = true)\n",
      " |-- military_spouse_caregiver__c: string (nullable = true)\n",
      " |-- acount_createddate: string (nullable = true)\n",
      " |-- dateofserviceentry: string (nullable = true)\n",
      " |-- dateofserviceseparation: string (nullable = true)\n",
      " |-- client_bio_dt_id: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- mailingstate: string (nullable = true)\n",
      " |-- mailingpostalcode: string (nullable = true)\n",
      " |-- mailingcountry: string (nullable = true)\n",
      " |-- preferred_method_of_contact__c: string (nullable = true)\n",
      " |-- responsive__c: integer (nullable = true)\n",
      " |-- active__c: string (nullable = true)\n",
      " |-- client_communication_dt_id: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- create_ddate: string (nullable = true)\n",
      " |-- dateassignedtohhusa: string (nullable = true)\n",
      " |-- dateassignedtostaff: string (nullable = true)\n",
      " |-- dateinitialassesment: string (nullable = true)\n",
      " |-- dateresumecompleted: string (nullable = true)\n",
      " |-- dsubimittedforhire: string (nullable = true)\n",
      " |-- confirmedhiredate: string (nullable = true)\n",
      " |-- startdate: string (nullable = true)\n",
      " |-- resume_completed_by__c: string (nullable = true)\n",
      " |-- resume_tailoring_tips__c: integer (nullable = true)\n",
      " |-- finalized_hhusa_revised_resume_on_file__c: integer (nullable = true)\n",
      " |-- created_linkedin_account__c: integer (nullable = true)\n",
      " |-- client_act_dt_id: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- yearsinservice: double (nullable = true)\n",
      " |-- ringdna100__email_attempts__c: integer (nullable = true)\n",
      " |-- ringdna100__call_attempts__c: integer (nullable = true)\n",
      " |-- client_act_dt_id: string (nullable = true)\n",
      " |-- client_bio_dt_id: string (nullable = true)\n",
      " |-- client_communication_dt_id: string (nullable = true)\n",
      " |-- client_hiring_dt_id: string (nullable = true)\n",
      " |-- reg_afterservice_months: double (nullable = true)\n",
      " |-- reg_afterservice_years: double (nullable = true)\n",
      " |-- monthsinservice: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the schema of the DataFrames\n",
    "dfHire.printSchema() \n",
    "dfBio.printSchema()\n",
    "dfCom.printSchema()\n",
    "dfAct.printSchema()\n",
    "dfFact.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+\n",
      "|service_branch__c|count|\n",
      "+-----------------+-----+\n",
      "|          Marines|13691|\n",
      "|           Spouse|    2|\n",
      "|  Merchant Marine|    1|\n",
      "|             null|37153|\n",
      "|        Air Force|13416|\n",
      "|             Army|51770|\n",
      "|      Coast Guard|  837|\n",
      "|   Not Applicable|    4|\n",
      "|             Navy|15567|\n",
      "+-----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Clients from different service branches\n",
    "dfBio.groupBy('service_branch__c').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+------+\n",
      "|job_function_hired_in__c| count|\n",
      "+------------------------+------+\n",
      "|    IT - Help Desk/Su...|   287|\n",
      "|    Skilled Labor/Trades|   484|\n",
      "|    Purchasing/Procur...|    73|\n",
      "|                   Sales|   697|\n",
      "|    Firefighter/EMT/E...|    71|\n",
      "|                 Science|    28|\n",
      "|    Training/Instruct...|  1113|\n",
      "|    Supply Chain/Logi...|   892|\n",
      "|             Engineering|   465|\n",
      "|    Media/Journalism/...|    54|\n",
      "|         Banking/Finance|   522|\n",
      "|              Healthcare|   900|\n",
      "|    Quality Assurance...|   238|\n",
      "|              Accounting|   171|\n",
      "|    IT - Computer Sci...|    41|\n",
      "|                    null|109255|\n",
      "|    Business Development|   196|\n",
      "|    Facilities Manage...|   165|\n",
      "|              Consultant|   456|\n",
      "|    Installation/Main...|  1223|\n",
      "+------------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Types of jobs the guys got hired in\n",
    "dfHire.groupBy('job_function_hired_in__c').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Temporary objects from which sql statements are run for each dataframe\n",
    "dfHire.createOrReplaceTempView(\"dfHire_sql\")\n",
    "dfBio.createOrReplaceTempView(\"dfBio_sql\")\n",
    "dfCom.createOrReplaceTempView(\"dfCom_sql\")\n",
    "dfAct.createOrReplaceTempView(\"dfAct_sql\")\n",
    "dfFact.createOrReplaceTempView(\"dfFAct_sql\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------+-----------------+--------------+----------------------+-------------+-----------+\n",
      "|hired|service_rank__c|service_branch__c|yearsinservice|reg_afterservice_years|responsive__c|resume_done|\n",
      "+-----+---------------+-----------------+--------------+----------------------+-------------+-----------+\n",
      "|    1|            O-3|             Army|         11.04|                 -0.33|            1|          1|\n",
      "|    1|            E-9|             Army|         30.18|                 -0.15|            1|          1|\n",
      "|    1|            E-4|             Army|           9.8|                 -0.73|            1|          1|\n",
      "|    1|            E-4|             Army|          2.81|                  0.43|            1|          1|\n",
      "|    1|            E-4|          Marines|          4.47|                  4.45|            1|          1|\n",
      "|    1|            O-2|             Army|          4.38|                 -0.51|            1|          1|\n",
      "|    1|            O-3|             Army|         11.03|                 -0.09|            1|          1|\n",
      "|    1|            E-8|             Army|         20.59|                  2.14|            1|          1|\n",
      "|    1|            E-7|             Army|         22.27|                 -0.42|            1|          1|\n",
      "|    1|            E-7|             Army|         20.01|                 -0.53|            1|          1|\n",
      "|    1|            O-3|             Army|          7.89|                 -0.41|            1|          1|\n",
      "|    1|            E-5|             Navy|          7.94|                 -0.31|            1|          1|\n",
      "|    1|            O-3|             Army|         16.89|                  1.54|            1|          1|\n",
      "|    1|            E-9|        Air Force|         29.82|                 -0.62|            1|          1|\n",
      "|    1|            E-7|        Air Force|         20.06|                  3.47|            1|          1|\n",
      "|    1|            W-1|             Army|         41.99|                -23.56|            1|          1|\n",
      "|    1|            O-3|             Army|         20.83|                 -0.82|            1|          1|\n",
      "|    1|            O-3|             Army|          4.08|                  0.03|            1|          1|\n",
      "|    1|            E-6|             Army|           6.1|                 -0.33|            1|          1|\n",
      "|    1|            E-4|             Navy|          7.09|                  8.58|            1|          1|\n",
      "+-----+---------------+-----------------+--------------+----------------------+-------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Mixed information from all 5 tables of the database\n",
    "spark.sql(''' Select hires.hired, bio.service_rank__c , \n",
    "              bio.service_branch__c , fact.yearsinservice, fact.reg_afterservice_years,\n",
    "              com.responsive__c, act.finalized_hhusa_revised_resume_on_file__c as resume_done\n",
    "              from dfHire_sql hires\n",
    "              inner join dfBio_sql bio on bio.id = hires.id \n",
    "              inner join dfFAct_sql fact on fact.id = hires.id\n",
    "              inner join dfCom_sql com on com.id = hires.id\n",
    "              inner join dfAct_sql act on act.id = hires.id\n",
    "              where hires.hired = \"1\"\n",
    "              \n",
    "              ''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|count(id)|month|\n",
      "+---------+-----+\n",
      "|    11982|    1|\n",
      "|     9626|    2|\n",
      "|    10152|    3|\n",
      "|    12579|    4|\n",
      "|    10978|    5|\n",
      "|    10672|    6|\n",
      "|    10815|    7|\n",
      "|    11912|    8|\n",
      "|    10845|    9|\n",
      "|    11866|   10|\n",
      "|    12022|   11|\n",
      "|     8992|   12|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Account create date by month of the year\n",
    "spark.sql( \"\"\"SELECT count(id) , \n",
    "              Extract(Month From create_ddate) as month \n",
    "              From  dfAct_sql  \n",
    "              group by month \n",
    "              ORDER BY month ASC \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------+\n",
      "|count(id)|year_registered|\n",
      "+---------+---------------+\n",
      "|     2825|           2007|\n",
      "|     2571|           2008|\n",
      "|     1769|           2009|\n",
      "|     2255|           2010|\n",
      "|     2732|           2011|\n",
      "|     4258|           2012|\n",
      "|    13128|           2013|\n",
      "|    13593|           2014|\n",
      "|    13248|           2015|\n",
      "|    25175|           2016|\n",
      "|    22933|           2017|\n",
      "|    26448|           2018|\n",
      "|     1506|           2019|\n",
      "+---------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Registration for services by year\n",
    "spark.sql( \"\"\"SELECT count(id) , \n",
    "Extract(Year From create_ddate) as year_registered \n",
    "              From  dfAct_sql  \n",
    "              group by year_registered\n",
    "              ORDER BY year_registered ASC \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------+\n",
      "|hired|service_branch__c|\n",
      "+-----+-----------------+\n",
      "|    1|   Not Applicable|\n",
      "|  170|      Coast Guard|\n",
      "| 1145|             null|\n",
      "| 2423|          Marines|\n",
      "| 2879|        Air Force|\n",
      "| 3055|             Navy|\n",
      "|10417|             Army|\n",
      "+-----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Hired by Service branch\n",
    "spark.sql(\"\"\" Select count(hires.id) as hired,bio.service_branch__c \n",
    "              From dfHire_sql hires \n",
    "              inner join dfBio_sql bio on bio.id = hires.id\n",
    "              Where hires.hired = 1\n",
    "              group by bio.service_branch__c\n",
    "              order by hired asc\n",
    "              \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------+\n",
      "|Not_hired|service_branch__c|\n",
      "+---------+-----------------+\n",
      "|        2|   Not Applicable|\n",
      "|      667|      Coast Guard|\n",
      "|     9382|             null|\n",
      "|    10524|        Air Force|\n",
      "|    11260|          Marines|\n",
      "|    12500|             Navy|\n",
      "|    41314|             Army|\n",
      "+---------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Not hired by Service Branch\n",
    "spark.sql(\"\"\" Select count(hires.id) as Not_hired,bio.service_branch__c \n",
    "              From dfHire_sql hires \n",
    "              inner join dfBio_sql bio on bio.id = hires.id\n",
    "              Where hires.hired = 0\n",
    "              group by bio.service_branch__c\n",
    "              order by Not_hired asc\n",
    "              \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------+\n",
      "|hired|service_rank__c|\n",
      "+-----+---------------+\n",
      "| 3807|            E-5|\n",
      "| 3683|            E-4|\n",
      "| 2795|            E-7|\n",
      "| 2775|            E-6|\n",
      "| 1281|            E-8|\n",
      "| 1156|           null|\n",
      "| 1142|            O-3|\n",
      "|  648|            E-3|\n",
      "|  564|            O-4|\n",
      "|  510|            O-5|\n",
      "|  463|            E-9|\n",
      "|  294|            O-2|\n",
      "|  186|            O-6|\n",
      "|  178|            W-3|\n",
      "|  148|            W-4|\n",
      "|  137|            W-2|\n",
      "|  126|            E-2|\n",
      "|   79|            O-1|\n",
      "|   75|            E-1|\n",
      "|   27|            W-5|\n",
      "+-----+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Hired by rank\n",
    "spark.sql(\"\"\" Select count(hires.id) as hired,bio.service_rank__c\n",
    "              From dfHire_sql hires \n",
    "              inner join dfBio_sql bio on bio.id = hires.id\n",
    "              Where hires.hired = 1\n",
    "              group by bio.service_rank__c\n",
    "              order by hired desc\n",
    "              \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------------------+\n",
      "|count|preferred_method_of_contact__c|\n",
      "+-----+------------------------------+\n",
      "|    7|                          Mail|\n",
      "|   23|                      LinkedIn|\n",
      "|  158|                          Text|\n",
      "| 1892|                          null|\n",
      "| 2626|                        E-Mail|\n",
      "|15384|                     Telephone|\n",
      "+-----+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Prefered method of contact for the hired\n",
    "spark.sql(\"\"\" select count(com.id) as count, com.preferred_method_of_contact__c \n",
    "              from dfCom_sql com \n",
    "              inner join dfHire_sql hires on com.id = hires.id\n",
    "              where hires.hired = 1\n",
    "              group by com.preferred_method_of_contact__c \n",
    "              order by count\n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------------------+\n",
      "|count|preferred_method_of_contact__c|\n",
      "+-----+------------------------------+\n",
      "|   28|                          Mail|\n",
      "|   33|                      LinkedIn|\n",
      "|  378|                          Text|\n",
      "|12405|                        E-Mail|\n",
      "|31506|                          null|\n",
      "|41299|                     Telephone|\n",
      "+-----+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Prefered method of contact for the Unhired\n",
    "spark.sql(\"\"\" select count(com.id) as count, com.preferred_method_of_contact__c \n",
    "              from dfCom_sql com \n",
    "              inner join dfHire_sql hires on com.id = hires.id\n",
    "              where hires.hired = 0\n",
    "              group by com.preferred_method_of_contact__c \n",
    "              order by count\n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select variables that determine whether one is going to be employed or not, store those varibles in a new dataframe\n",
    "dfspark= spark.sql(\"\"\" select fact.yearsinservice as S_years , fact.reg_afterservice_years as R_years,\n",
    "               act.resume_tailoring_tips__c as Resume_Tips,act.finalized_hhusa_revised_resume_on_file__c as Resume_OnFile, bio.client__c as Client, bio.client_type__c as Client_Type, bio.service_rank__c as Service_R\n",
    "              ,bio.service_branch__c as Service_B, hire.hire_heroes_usa_confirmed_hire__c as HHUSA_hire, com.preferred_method_of_contact__c as Com_Method, com.responsive__c\n",
    "              ,com.active__c, act.created_linkedin_account__c as Created_Linkedin, bio.highest_level_of_education_completed__c as Educ,  hire.hired\n",
    "              ,bio.primary_military_occupational_specialty__c as Occupation\n",
    "               \n",
    "              from dfFact_sql fact\n",
    "              \n",
    "              inner join dfAct_sql act on act.id = fact.id\n",
    "              inner join dfBio_sql bio on bio.id = fact.id\n",
    "              inner join dfHire_sql hire on hire.id = fact.id\n",
    "              inner join dfCom_sql com on com.id = fact.id\n",
    "              \n",
    "              where bio.client_type__c != '' and bio.service_branch__c !='' and bio.service_rank__c !='' \n",
    "              and com.preferred_method_of_contact__c !='' and fact.yearsinservice < 53 and fact.yearsinservice > 0 \n",
    "              and fact.reg_afterservice_years >- 32 and fact.reg_afterservice_years < 53 and com.active__c !=''\n",
    "              and bio.highest_level_of_education_completed__c !='' and hire.hired  !='' and hire.hired !='No' and hire.hired != 3\n",
    "\n",
    "          \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51093"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the number of clients in the description above\n",
    "dfspark.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the data type of the the hired column from string to double\n",
    "dfspark = dfspark.withColumn(\"hired\", dfspark[\"hired\"].cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|hired|\n",
      "+-----+\n",
      "|  0.0|\n",
      "|  1.0|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Confirm the labels in the hired column 0 represents the client was not hired 1 represents they were hired. \n",
    "dfspark.select('hired').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('S_years', 'double'),\n",
       " ('R_years', 'double'),\n",
       " ('Resume_Tips', 'int'),\n",
       " ('Resume_OnFile', 'int'),\n",
       " ('Client', 'int'),\n",
       " ('Client_Type', 'string'),\n",
       " ('Service_R', 'string'),\n",
       " ('Service_B', 'string'),\n",
       " ('HHUSA_hire', 'int'),\n",
       " ('Com_Method', 'string'),\n",
       " ('responsive__c', 'int'),\n",
       " ('active__c', 'string'),\n",
       " ('Created_Linkedin', 'int'),\n",
       " ('Educ', 'string'),\n",
       " ('hired', 'double'),\n",
       " ('Occupation', 'string')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The data types of the data frame \n",
    "dfspark.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----------+-------------+------+-----------------+---------+---------+----------+----------+-------------+---------+----------------+---------------+-----+----------+--------------------+\n",
      "|S_years|R_years|Resume_Tips|Resume_OnFile|Client|      Client_Type|Service_R|Service_B|HHUSA_hire|Com_Method|responsive__c|active__c|Created_Linkedin|           Educ|hired|Occupation|            features|\n",
      "+-------+-------+-----------+-------------+------+-----------------+---------+---------+----------+----------+-------------+---------+----------------+---------------+-----+----------+--------------------+\n",
      "|   8.11|  -0.37|          1|            1|     1|Online Registrant|      E-5|     Army|         0| Telephone|            1|      Yes|               1|High School/GED|  0.0|       91F|(262144,[52387,65...|\n",
      "+-------+-------+-----------+-------------+------+-----------------+---------+---------+----------+----------+-------------+---------+----------------+---------------+-----+----------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Hash the features through a hash function to convert the features into vectors for the dfspark dataframe,\n",
    "# Convert strings to numerical data, the output column is features. The target is not included in this conversion.\n",
    "funchash = FeatureHasher(inputCols= ['S_years','R_years','Resume_Tips','Resume_OnFile','Client','Client_Type','Service_R',\\\n",
    " 'Service_B','HHUSA_hire','Com_Method','responsive__c','active__c','Created_Linkedin','Educ','Occupation'],\n",
    "                       outputCol=\"features\")\n",
    "\n",
    "dffeatures = funchash.transform(dfspark)\n",
    "dffeatures.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|hired|\n",
      "+--------------------+-----+\n",
      "|(262144,[52387,65...|  0.0|\n",
      "|(262144,[52387,65...|  0.0|\n",
      "|(262144,[65085,69...|  0.0|\n",
      "|(262144,[65085,69...|  0.0|\n",
      "|(262144,[65085,69...|  0.0|\n",
      "|(262144,[64366,65...|  0.0|\n",
      "|(262144,[54411,65...|  0.0|\n",
      "|(262144,[65085,69...|  0.0|\n",
      "|(262144,[65085,69...|  0.0|\n",
      "|(262144,[65085,69...|  0.0|\n",
      "|(262144,[11074,65...|  0.0|\n",
      "|(262144,[52387,65...|  0.0|\n",
      "|(262144,[40267,65...|  0.0|\n",
      "|(262144,[65085,69...|  0.0|\n",
      "|(262144,[52387,65...|  0.0|\n",
      "|(262144,[65085,69...|  0.0|\n",
      "|(262144,[48884,52...|  0.0|\n",
      "|(262144,[52387,65...|  0.0|\n",
      "|(262144,[65085,69...|  0.0|\n",
      "|(262144,[65085,69...|  0.0|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Confirm that the dataframe is active, We are going to use this data to train our machine learning algorithm.\n",
    "dffeatures.select('features','hired').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the data into training and testing data for the training of the algorithms and testing of the algorithms \n",
    "train_data,test_data=dffeatures.randomSplit([0.75,0.25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the decision tree classifier, to predict whether one is hired or not, using the hired column and features. \n",
    "def treedec(label, features):\n",
    "    decision_tree = DecisionTreeClassifier(labelCol=label, featuresCol=features)\n",
    "    return decision_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration it takes to run the algorithm: 0:00:00.000285\n"
     ]
    }
   ],
   "source": [
    "# Now let us train the model, while timing how long it takes run this task. \n",
    "\n",
    "# Start time of training the model \n",
    "start = datetime.now()\n",
    "def traindec_tree(train_data, algo): \n",
    "    # The model \n",
    "    model1 = algo.fit(train_data)\n",
    "    return model1\n",
    "# Print the duration\n",
    "#traindec_tree(train_data, treedec('hired','features'))\n",
    "#End of time of the model \n",
    "end = datetime.now()\n",
    "print('Duration it takes to run the algorithm: {}'.format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions using the test set of the dataset ... Store the predictions in a new dataframe prediction \n",
    "predictions = traindec_tree(train_data, treedec('hired','features')).transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+--------------------+\n",
      "|prediction|hired|            features|\n",
      "+----------+-----+--------------------+\n",
      "|       0.0|  0.0|(262144,[65085,81...|\n",
      "|       1.0|  1.0|(262144,[52387,65...|\n",
      "|       1.0|  1.0|(262144,[65085,69...|\n",
      "|       0.0|  0.0|(262144,[65085,69...|\n",
      "|       0.0|  0.0|(262144,[65085,69...|\n",
      "|       0.0|  0.0|(262144,[65085,81...|\n",
      "|       0.0|  0.0|(262144,[65085,69...|\n",
      "|       0.0|  0.0|(262144,[6608,650...|\n",
      "|       0.0|  0.0|(262144,[65085,69...|\n",
      "|       0.0|  0.0|(262144,[65085,69...|\n",
      "|       1.0|  1.0|(262144,[11074,65...|\n",
      "|       0.0|  0.0|(262144,[65085,69...|\n",
      "|       0.0|  0.0|(262144,[52387,65...|\n",
      "|       0.0|  0.0|(262144,[63818,65...|\n",
      "|       0.0|  0.0|(262144,[65085,69...|\n",
      "|       0.0|  0.0|(262144,[65085,69...|\n",
      "|       1.0|  1.0|(262144,[52387,65...|\n",
      "|       1.0|  1.0|(262144,[65085,69...|\n",
      "|       0.0|  0.0|(262144,[65085,69...|\n",
      "|       0.0|  0.0|(262144,[11700,65...|\n",
      "+----------+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check out the new dataframe predictions to view the prediction, hired and features columns \n",
    "predictions.select(\"prediction\", \"hired\", \"features\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "Test Error  = 0.126202 \n",
      "Accuracy   = 0.873798 \n"
     ]
    }
   ],
   "source": [
    "# Now lets evaluate the algorithm performance\n",
    "#predictions\n",
    "def evaluate_algo(df,label,Algorithm_name  ):\n",
    "    classification_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=label, predictionCol=\"prediction\")\n",
    "    accuracy = classification_evaluator.evaluate(df)\n",
    "    print(Algorithm_name )\n",
    "    print(\"Test Error  = %g \" % (1.0 - accuracy))\n",
    "    print(\"Accuracy   = %g \" % accuracy)\n",
    "\n",
    "evaluate_algo(predictions,'hired','Decision Tree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration: 0:00:00.007267\n"
     ]
    }
   ],
   "source": [
    "# Train a RandomForest model.\n",
    "from datetime import datetime\n",
    "\n",
    "start_time = datetime.now()\n",
    "# do your work here\n",
    "rf = RandomForestClassifier(labelCol=\"hired\", featuresCol=\"features\", numTrees=17)\n",
    "end_time = datetime.now()\n",
    "\n",
    "print('Duration: {}'.format(end_time - start_time))\n",
    "\n",
    "def randomforest(label, features, numTrees):\n",
    "    \n",
    "    random_forest = RandomForestClassifier(labelCol=label, featuresCol=features, numTrees=numTrees)\n",
    "    \n",
    "    return random_forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration: 0:00:00.000230\n"
     ]
    }
   ],
   "source": [
    "# Train model.  This also runs the indexers.\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "start_time = datetime.now()\n",
    "# do your work here\n",
    "#modelrf = rf.fit(train_data)\n",
    "\n",
    "def random_forestfit(randf,train_data):\n",
    "    model= randf.fit(train_data)\n",
    "    return model\n",
    "    \n",
    "\n",
    "end_time = datetime.now()\n",
    "\n",
    "print('Duration: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions.\n",
    "predictions2 = random_forestfit(randomforest('hired', 'features', 17),train_data).transform(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+--------------------+\n",
      "|prediction|hired|            features|\n",
      "+----------+-----+--------------------+\n",
      "|       0.0|  0.0|(262144,[65085,81...|\n",
      "|       0.0|  1.0|(262144,[52387,65...|\n",
      "|       0.0|  1.0|(262144,[65085,69...|\n",
      "|       0.0|  0.0|(262144,[65085,69...|\n",
      "|       0.0|  0.0|(262144,[65085,69...|\n",
      "|       0.0|  0.0|(262144,[65085,81...|\n",
      "|       0.0|  0.0|(262144,[65085,69...|\n",
      "|       0.0|  0.0|(262144,[6608,650...|\n",
      "|       0.0|  0.0|(262144,[65085,69...|\n",
      "|       0.0|  0.0|(262144,[65085,69...|\n",
      "|       0.0|  1.0|(262144,[11074,65...|\n",
      "|       0.0|  0.0|(262144,[65085,69...|\n",
      "|       0.0|  0.0|(262144,[52387,65...|\n",
      "|       0.0|  0.0|(262144,[63818,65...|\n",
      "|       0.0|  0.0|(262144,[65085,69...|\n",
      "|       0.0|  0.0|(262144,[65085,69...|\n",
      "|       0.0|  1.0|(262144,[52387,65...|\n",
      "|       0.0|  1.0|(262144,[65085,69...|\n",
      "|       0.0|  0.0|(262144,[65085,69...|\n",
      "|       0.0|  0.0|(262144,[11700,65...|\n",
      "+----------+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select example rows to display.\n",
    "predictions2.select('prediction','hired','features').show()  #.select(\"prediction\", \"indexedLabel\", \"features\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Test Error  = 0.452746 \n",
      "Accuracy   = 0.547254 \n"
     ]
    }
   ],
   "source": [
    "evaluate_algo(predictions2,'hired','Random Forest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster(k,df,features):\n",
    "    kmeans = KMeans(k=k, seed=1)  # 2 clusters here\n",
    "    model = kmeans.fit(df.select(features))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformeddf = cluster(4,dffeatures,'features').transform(dffeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|prediction|\n",
      "+----------+\n",
      "|         1|\n",
      "|         3|\n",
      "|         2|\n",
      "|         0|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformeddf.select('prediction').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+\n",
      "|summary|          S_years|           R_years|\n",
      "+-------+-----------------+------------------+\n",
      "|  count|            51093|             51093|\n",
      "|   mean|13.25944708668499|1.9484418609202803|\n",
      "| stddev| 8.55804960106253| 6.346271266872357|\n",
      "|    min|             0.01|            -29.16|\n",
      "|    max|            52.33|             52.58|\n",
      "+-------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Information about individual clusters in term of years service men stay in the army and when they register for services\n",
    "transformeddf.select('S_years','R_years').describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get cluster information Cluster 0 in terms of years service men spend on service. \n",
    "cluster0 = transformeddf.filter(transformeddf.prediction==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+\n",
      "|summary|           S_years|           R_years|\n",
      "+-------+------------------+------------------+\n",
      "|  count|              2925|              2925|\n",
      "|   mean| 5.224564102564105|22.217131623931618|\n",
      "| stddev|2.9455794494751912| 8.277066032403162|\n",
      "|    min|              0.01|             11.33|\n",
      "|    max|             21.19|             52.58|\n",
      "+-------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cluster0.select('S_years','R_years').describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get cluster information Cluster 1\n",
    "cluster1 = transformeddf.filter(transformeddf.prediction==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+-------------------+\n",
      "|summary|          S_years|            R_years|\n",
      "+-------+-----------------+-------------------+\n",
      "|  count|            18935|              18935|\n",
      "|   mean|22.77949405862156|-0.6843913387906007|\n",
      "| stddev|4.482330273547443| 2.2575649893207146|\n",
      "|    min|             14.0|             -29.16|\n",
      "|    max|            52.33|               7.92|\n",
      "+-------+-----------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cluster1.select('S_years','R_years').describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get cluster information Cluster 2\n",
    "cluster2 = transformeddf.filter(transformeddf.prediction==2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------------+\n",
      "|summary|           S_years|          R_years|\n",
      "+-------+------------------+-----------------+\n",
      "|  count|              1929|             1929|\n",
      "|   mean|19.301316744427176|9.179803006739244|\n",
      "| stddev| 4.354761181256989|4.399028957656696|\n",
      "|    min|              8.92|             2.32|\n",
      "|    max|             39.37|            26.82|\n",
      "+-------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cluster2.select('S_years','R_years').describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get cluster information Cluster 3\n",
    "cluster3 = transformeddf.filter(transformeddf.prediction==3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+\n",
      "|summary|           S_years|           R_years|\n",
      "+-------+------------------+------------------+\n",
      "|  count|             27304|             27304|\n",
      "|   mean| 7.091309698212723|1.0920648989159096|\n",
      "| stddev|3.2336080116487533|3.1435676760042863|\n",
      "|    min|              0.01|            -10.07|\n",
      "|    max|             15.08|             11.83|\n",
      "+-------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cluster3.select('S_years','R_years').describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

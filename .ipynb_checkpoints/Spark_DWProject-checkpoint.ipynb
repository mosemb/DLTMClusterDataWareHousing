{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"DatawarehouseSpark Application\") \\\n",
    "    .getOrCreate()\n",
    " \n",
    "inputFileHire ='/home/mose/Downloads/CSVS/client_hiring_dt.csv' #\"/user/user17/mosedata_proj/input/client_hiring_dt.csv\"\n",
    "#inputFileBio ='/home/mose/Downloads/CSVS/client_bio_dt.csv'  #\"/user/user17/mosedata_proj/input/client_bio_dt.csv\"\n",
    "#inputFileCom = '/home/mose/Downloads/CSVS/client_communication_dt.csv' #\"/user/user17/mosedata_proj/input/client_communication_dt.csv\"\n",
    "#inputFileAct = '/home/mose/Downloads/CSVS/client_activities_dt.csv' #\"/user/user17/mosedata_proj/input/client_activities_dt.csv\"\n",
    "#inputFileFact = '/home/mose/Downloads/CSVS/client_fact_ft.csv' #\"/user/user17/mosedata_proj/input/client_fact_ft.csv\"\n",
    "\n",
    " \n",
    "# Create DataFrame from CSV file\n",
    "dfM =  spark.read.format(\"csv\").option(\"sep\", \",\").option(\"inferSchema\", \"true\").option(\"header\", \"true\").load(inputFileHire)\n",
    "#dfR =  spark.read.format(\"csv\").option(\"sep\", \",\").option(\"inferSchema\", \"true\").option(\"header\", \"true\").load(inputFileBio)\n",
    "#dfCom =  spark.read.format(\"csv\").option(\"sep\", \",\").option(\"inferSchema\", \"true\").option(\"header\", \"true\").load(inputFileCom)\n",
    "#dfAct =  spark.read.format(\"csv\").option(\"sep\", \",\").option(\"inferSchema\", \"true\").option(\"header\", \"true\").load(inputFileAct)\n",
    "#dfFact =  spark.read.format(\"csv\").option(\"sep\", \",\").option(\"inferSchema\", \"true\").option(\"header\", \"true\").load(inputFileFact)\n",
    "\n",
    "\n",
    "# Print the schema of the DataFrames\n",
    "dfM.printSchema()\n",
    "dfR.printSchema()\n",
    "dfCom.printSchema()\n",
    "dfAct.printSchema()\n",
    "dfFact.printSchema()\n",
    "\n",
    "\n",
    "#dfR.groupBy('movieId').count().show(50)\n",
    "#dfM.where(\"director='Martin Brest'\").join(dfR,\"movieId\").select(\"movieId\",\"director\",\"user_name\",\"rating\").show(50)\n",
    "\n",
    "\n",
    "# Stop the Spark Session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting dataframes.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile dataframes.py\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"DatawarehouseSpark Application\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "#Input the data\n",
    "inputFileHire ='/user/user17/mosedata_proj/input/client_hiring_dt.csv'\n",
    "inputFileBio ='/user/user17/mosedata_proj/input/client_bio_dt.csv'\n",
    "inputFileCom = '/user/user17/mosedata_proj/input/client_communication_dt.csv'\n",
    "inputFileAct = '/user/user17/mosedata_proj/input/client_activities_dt.csv'\n",
    "inputFileFact = '/user/user17/mosedata_proj/input/client_fact_ft.csv'\n",
    "\n",
    "#Create data frames from sources. Tables were converted to csvs now being converted to dataframes\n",
    "dfHire =  spark.read.format(\"csv\").option(\"sep\", \",\").option(\"inferSchema\", \"true\").option(\"header\", \"true\").load(inputFileHire)\n",
    "dfBio =  spark.read.format(\"csv\").option(\"sep\", \",\").option(\"inferSchema\", \"true\").option(\"header\", \"true\").load(inputFileBio)\n",
    "dfCom =  spark.read.format(\"csv\").option(\"sep\", \",\").option(\"inferSchema\", \"true\").option(\"header\", \"true\").load(inputFileCom)\n",
    "dfAct =  spark.read.format(\"csv\").option(\"sep\", \",\").option(\"inferSchema\", \"true\").option(\"header\", \"true\").load(inputFileAct)\n",
    "dfFact =  spark.read.format(\"csv\").option(\"sep\", \",\").option(\"inferSchema\", \"true\").option(\"header\", \"true\").load(inputFileFact)\n",
    "\n",
    "# Print the schema of the DataFrames\n",
    "dfHire.printSchema() \n",
    "dfBio.printSchema()\n",
    "dfCom.printSchema()\n",
    "dfAct.printSchema()\n",
    "dfFact.printSchema()\n",
    "\n",
    "#Clients from different service branches\n",
    "dfBio.groupBy('service_branch__c').count().show()\n",
    "\n",
    "#Types of jobs the guys got hired in\n",
    "dfHire.groupBy('job_function_hired_in__c').count().show()\n",
    "\n",
    "#Temporary objects from which sql statements are run for each dataframe\n",
    "dfHire.createOrReplaceTempView(\"dfHire_sql\")\n",
    "dfBio.createOrReplaceTempView(\"dfBio_sql\")\n",
    "dfCom.createOrReplaceTempView(\"dfCom_sql\")\n",
    "dfAct.createOrReplaceTempView(\"dfAct_sql\")\n",
    "dfFact.createOrReplaceTempView(\"dfFAct_sql\")\n",
    "\n",
    "#Mixed information from all 5 tables of the database\n",
    "spark.sql(''' Select hires.hired, bio.service_rank__c , \n",
    "              bio.service_branch__c , fact.yearsinservice, fact.reg_afterservice_years,\n",
    "              com.responsive__c, act.finalized_hhusa_revised_resume_on_file__c as resume_done\n",
    "              from dfHire_sql hires\n",
    "              inner join dfBio_sql bio on bio.id = hires.id \n",
    "              inner join dfFAct_sql fact on fact.id = hires.id\n",
    "              inner join dfCom_sql com on com.id = hires.id\n",
    "              inner join dfAct_sql act on act.id = hires.id\n",
    "              where hires.hired = \"1\"\n",
    "              \n",
    "              ''').show()\n",
    "\n",
    "#Account create date by month of the year\n",
    "spark.sql( \"\"\"SELECT count(id) , \n",
    "              Extract(Month From create_ddate) as month \n",
    "              From  dfAct_sql  \n",
    "              group by month \n",
    "              ORDER BY month ASC \"\"\").show()\n",
    "\n",
    "\n",
    "#Registration for services by year\n",
    "spark.sql( \"\"\"SELECT count(id) , \n",
    "Extract(Year From create_ddate) as year_registered \n",
    "              From  dfAct_sql  \n",
    "              group by year_registered\n",
    "              ORDER BY year_registered ASC \"\"\").show()\n",
    "\n",
    "#Hired by Service branch\n",
    "spark.sql(\"\"\" Select count(hires.id) as hired,bio.service_branch__c \n",
    "              From dfHire_sql hires \n",
    "              inner join dfBio_sql bio on bio.id = hires.id\n",
    "              Where hires.hired = 1\n",
    "              group by bio.service_branch__c\n",
    "              order by hired asc\n",
    "              \"\"\").show()\n",
    "\n",
    "#Not hired by Service Branch\n",
    "spark.sql(\"\"\" Select count(hires.id) as Not_hired,bio.service_branch__c \n",
    "              From dfHire_sql hires \n",
    "              inner join dfBio_sql bio on bio.id = hires.id\n",
    "              Where hires.hired = 0\n",
    "              group by bio.service_branch__c\n",
    "              order by Not_hired asc\n",
    "              \"\"\").show()\n",
    "\n",
    "#Hired by rank\n",
    "spark.sql(\"\"\" Select count(hires.id) as hired,bio.service_rank__c\n",
    "              From dfHire_sql hires \n",
    "              inner join dfBio_sql bio on bio.id = hires.id\n",
    "              Where hires.hired = 0\n",
    "              group by bio.service_rank__c\n",
    "              order by hired desc\n",
    "              \"\"\").show()\n",
    "\n",
    "#Prefered method of contact for the hired\n",
    "spark.sql(\"\"\" select count(com.id) as count, com.preferred_method_of_contact__c \n",
    "              from dfCom_sql com \n",
    "              inner join dfHire_sql hires on com.id = hires.id\n",
    "              where hires.hired = 1\n",
    "              group by com.preferred_method_of_contact__c \n",
    "              order by count\n",
    "          \"\"\").show()\n",
    "\n",
    "#Prefered method of contact for the Unhired\n",
    "spark.sql(\"\"\" select count(com.id) as count, com.preferred_method_of_contact__c \n",
    "              from dfCom_sql com \n",
    "              inner join dfHire_sql hires on com.id = hires.id\n",
    "              where hires.hired = 0\n",
    "              group by com.preferred_method_of_contact__c \n",
    "              order by count\n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"DatawarehouseSpark Application\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, VectorIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputFileHire ='/home/mose/Downloads/CSVS/client_hiring_dt.csv' #\"/user/user17/mosedata_proj/input/client_hiring_dt.csv\"\n",
    "inputFileBio ='/home/mose/Downloads/CSVS/client_bio_dt.csv'  #\"/user/user17/mosedata_proj/input/client_bio_dt.csv\"\n",
    "inputFileCom = '/home/mose/Downloads/CSVS/client_communication_dt.csv' #\"/user/user17/mosedata_proj/input/client_communication_dt.csv\"\n",
    "inputFileAct = '/home/mose/Downloads/CSVS/client_activities_dt.csv' #\"/user/user17/mosedata_proj/input/client_activities_dt.csv\"\n",
    "inputFileFact = '/home/mose/Downloads/CSVS/client_fact_ft.csv' #\"/user/user17/mosedata_proj/input/client_fact_ft.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame from CSV file\n",
    "dfHire =  spark.read.format(\"csv\").option(\"sep\", \",\").option(\"inferSchema\", \"true\").option(\"header\", \"true\").load(inputFileHire)\n",
    "dfBio =  spark.read.format(\"csv\").option(\"sep\", \",\").option(\"inferSchema\", \"true\").option(\"header\", \"true\").load(inputFileBio)\n",
    "dfCom =  spark.read.format(\"csv\").option(\"sep\", \",\").option(\"inferSchema\", \"true\").option(\"header\", \"true\").load(inputFileCom)\n",
    "dfAct =  spark.read.format(\"csv\").option(\"sep\", \",\").option(\"inferSchema\", \"true\").option(\"header\", \"true\").load(inputFileAct)\n",
    "dfFact =  spark.read.format(\"csv\").option(\"sep\", \",\").option(\"inferSchema\", \"true\").option(\"header\", \"true\").load(inputFileFact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- dsubimittedforhire: string (nullable = true)\n",
      " |-- dconfirmedforhire: string (nullable = true)\n",
      " |-- difsubmitconfirmed: integer (nullable = true)\n",
      " |-- startdate: string (nullable = true)\n",
      " |-- datdifconfirstart: integer (nullable = true)\n",
      " |-- job_type__c: string (nullable = true)\n",
      " |-- job_function_hired_in__c: string (nullable = true)\n",
      " |-- hiring_account__c: string (nullable = true)\n",
      " |-- hire_heroes_usa_confirmed_hire__c: integer (nullable = true)\n",
      " |-- hired_location__c: string (nullable = true)\n",
      " |-- hired_zip_code__c: string (nullable = true)\n",
      " |-- industry_hired_in__c: string (nullable = true)\n",
      " |-- hired_but_still_active_and_looking__c: string (nullable = true)\n",
      " |-- hired: string (nullable = true)\n",
      " |-- client_hiring_dt_id: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- client__c: integer (nullable = true)\n",
      " |-- client_type__c: string (nullable = true)\n",
      " |-- service_rank__c: string (nullable = true)\n",
      " |-- service_branch__c: string (nullable = true)\n",
      " |-- primary_military_occupational_specialty__c: string (nullable = true)\n",
      " |-- gender__c: string (nullable = true)\n",
      " |-- highest_level_of_education_completed__c: string (nullable = true)\n",
      " |-- status__c: string (nullable = true)\n",
      " |-- military_spouse_caregiver__c: string (nullable = true)\n",
      " |-- acount_createddate: string (nullable = true)\n",
      " |-- dateofserviceentry: string (nullable = true)\n",
      " |-- dateofserviceseparation: string (nullable = true)\n",
      " |-- client_bio_dt_id: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- mailingstate: string (nullable = true)\n",
      " |-- mailingpostalcode: string (nullable = true)\n",
      " |-- mailingcountry: string (nullable = true)\n",
      " |-- preferred_method_of_contact__c: string (nullable = true)\n",
      " |-- responsive__c: integer (nullable = true)\n",
      " |-- active__c: string (nullable = true)\n",
      " |-- client_communication_dt_id: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- create_ddate: string (nullable = true)\n",
      " |-- dateassignedtohhusa: string (nullable = true)\n",
      " |-- dateassignedtostaff: string (nullable = true)\n",
      " |-- dateinitialassesment: string (nullable = true)\n",
      " |-- dateresumecompleted: string (nullable = true)\n",
      " |-- dsubimittedforhire: string (nullable = true)\n",
      " |-- confirmedhiredate: string (nullable = true)\n",
      " |-- startdate: string (nullable = true)\n",
      " |-- resume_completed_by__c: string (nullable = true)\n",
      " |-- resume_tailoring_tips__c: integer (nullable = true)\n",
      " |-- finalized_hhusa_revised_resume_on_file__c: integer (nullable = true)\n",
      " |-- created_linkedin_account__c: integer (nullable = true)\n",
      " |-- client_act_dt_id: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- yearsinservice: double (nullable = true)\n",
      " |-- ringdna100__email_attempts__c: integer (nullable = true)\n",
      " |-- ringdna100__call_attempts__c: integer (nullable = true)\n",
      " |-- client_act_dt_id: string (nullable = true)\n",
      " |-- client_bio_dt_id: string (nullable = true)\n",
      " |-- client_communication_dt_id: string (nullable = true)\n",
      " |-- client_hiring_dt_id: string (nullable = true)\n",
      " |-- reg_afterservice_months: double (nullable = true)\n",
      " |-- reg_afterservice_years: double (nullable = true)\n",
      " |-- monthsinservice: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the schema of the DataFrames\n",
    "dfHire.printSchema() \n",
    "dfBio.printSchema()\n",
    "dfCom.printSchema()\n",
    "dfAct.printSchema()\n",
    "dfFact.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+\n",
      "|service_branch__c|count|\n",
      "+-----------------+-----+\n",
      "|          Marines|13691|\n",
      "|           Spouse|    2|\n",
      "|  Merchant Marine|    1|\n",
      "|             null|37153|\n",
      "|        Air Force|13416|\n",
      "|             Army|51770|\n",
      "|      Coast Guard|  837|\n",
      "|   Not Applicable|    4|\n",
      "|             Navy|15567|\n",
      "+-----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Clients from different service branches\n",
    "dfBio.groupBy('service_branch__c').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+------+\n",
      "|job_function_hired_in__c| count|\n",
      "+------------------------+------+\n",
      "|    IT - Help Desk/Su...|   287|\n",
      "|    Skilled Labor/Trades|   484|\n",
      "|    Purchasing/Procur...|    73|\n",
      "|                   Sales|   697|\n",
      "|    Firefighter/EMT/E...|    71|\n",
      "|                 Science|    28|\n",
      "|    Training/Instruct...|  1113|\n",
      "|    Supply Chain/Logi...|   892|\n",
      "|             Engineering|   465|\n",
      "|    Media/Journalism/...|    54|\n",
      "|         Banking/Finance|   522|\n",
      "|              Healthcare|   900|\n",
      "|    Quality Assurance...|   238|\n",
      "|              Accounting|   171|\n",
      "|    IT - Computer Sci...|    41|\n",
      "|                    null|109255|\n",
      "|    Business Development|   196|\n",
      "|    Facilities Manage...|   165|\n",
      "|              Consultant|   456|\n",
      "|    Installation/Main...|  1223|\n",
      "+------------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Types of jobs the guys got hired in\n",
    "dfHire.groupBy('job_function_hired_in__c').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Temporary objects from which sql statements are run for each dataframe\n",
    "dfHire.createOrReplaceTempView(\"dfHire_sql\")\n",
    "dfBio.createOrReplaceTempView(\"dfBio_sql\")\n",
    "dfCom.createOrReplaceTempView(\"dfCom_sql\")\n",
    "dfAct.createOrReplaceTempView(\"dfAct_sql\")\n",
    "dfFact.createOrReplaceTempView(\"dfFAct_sql\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------+-----------------+--------------+----------------------+-------------+-----------+\n",
      "|hired|service_rank__c|service_branch__c|yearsinservice|reg_afterservice_years|responsive__c|resume_done|\n",
      "+-----+---------------+-----------------+--------------+----------------------+-------------+-----------+\n",
      "|    1|            O-3|             Army|         11.04|                 -0.33|            1|          1|\n",
      "|    1|            E-9|             Army|         30.18|                 -0.15|            1|          1|\n",
      "|    1|            E-4|             Army|           9.8|                 -0.73|            1|          1|\n",
      "|    1|            E-4|             Army|          2.81|                  0.43|            1|          1|\n",
      "|    1|            E-4|          Marines|          4.47|                  4.45|            1|          1|\n",
      "|    1|            O-2|             Army|          4.38|                 -0.51|            1|          1|\n",
      "|    1|            O-3|             Army|         11.03|                 -0.09|            1|          1|\n",
      "|    1|            E-8|             Army|         20.59|                  2.14|            1|          1|\n",
      "|    1|            E-7|             Army|         22.27|                 -0.42|            1|          1|\n",
      "|    1|            E-7|             Army|         20.01|                 -0.53|            1|          1|\n",
      "|    1|            O-3|             Army|          7.89|                 -0.41|            1|          1|\n",
      "|    1|            E-5|             Navy|          7.94|                 -0.31|            1|          1|\n",
      "|    1|            O-3|             Army|         16.89|                  1.54|            1|          1|\n",
      "|    1|            E-9|        Air Force|         29.82|                 -0.62|            1|          1|\n",
      "|    1|            E-7|        Air Force|         20.06|                  3.47|            1|          1|\n",
      "|    1|            W-1|             Army|         41.99|                -23.56|            1|          1|\n",
      "|    1|            O-3|             Army|         20.83|                 -0.82|            1|          1|\n",
      "|    1|            O-3|             Army|          4.08|                  0.03|            1|          1|\n",
      "|    1|            E-6|             Army|           6.1|                 -0.33|            1|          1|\n",
      "|    1|            E-4|             Navy|          7.09|                  8.58|            1|          1|\n",
      "+-----+---------------+-----------------+--------------+----------------------+-------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Mixed information from all 5 tables of the database\n",
    "spark.sql(''' Select hires.hired, bio.service_rank__c , \n",
    "              bio.service_branch__c , fact.yearsinservice, fact.reg_afterservice_years,\n",
    "              com.responsive__c, act.finalized_hhusa_revised_resume_on_file__c as resume_done\n",
    "              from dfHire_sql hires\n",
    "              inner join dfBio_sql bio on bio.id = hires.id \n",
    "              inner join dfFAct_sql fact on fact.id = hires.id\n",
    "              inner join dfCom_sql com on com.id = hires.id\n",
    "              inner join dfAct_sql act on act.id = hires.id\n",
    "              where hires.hired = \"1\"\n",
    "              \n",
    "              ''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|count(id)|month|\n",
      "+---------+-----+\n",
      "|    11982|    1|\n",
      "|     9626|    2|\n",
      "|    10152|    3|\n",
      "|    12579|    4|\n",
      "|    10978|    5|\n",
      "|    10672|    6|\n",
      "|    10815|    7|\n",
      "|    11912|    8|\n",
      "|    10845|    9|\n",
      "|    11866|   10|\n",
      "|    12022|   11|\n",
      "|     8992|   12|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Account create date by month of the year\n",
    "spark.sql( \"\"\"SELECT count(id) , \n",
    "              Extract(Month From create_ddate) as month \n",
    "              From  dfAct_sql  \n",
    "              group by month \n",
    "              ORDER BY month ASC \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------+\n",
      "|count(id)|year_registered|\n",
      "+---------+---------------+\n",
      "|     2825|           2007|\n",
      "|     2571|           2008|\n",
      "|     1769|           2009|\n",
      "|     2255|           2010|\n",
      "|     2732|           2011|\n",
      "|     4258|           2012|\n",
      "|    13128|           2013|\n",
      "|    13593|           2014|\n",
      "|    13248|           2015|\n",
      "|    25175|           2016|\n",
      "|    22933|           2017|\n",
      "|    26448|           2018|\n",
      "|     1506|           2019|\n",
      "+---------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Registration for services by year\n",
    "spark.sql( \"\"\"SELECT count(id) , \n",
    "Extract(Year From create_ddate) as year_registered \n",
    "              From  dfAct_sql  \n",
    "              group by year_registered\n",
    "              ORDER BY year_registered ASC \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------+\n",
      "|hired|service_branch__c|\n",
      "+-----+-----------------+\n",
      "|    1|   Not Applicable|\n",
      "|  170|      Coast Guard|\n",
      "| 1145|             null|\n",
      "| 2423|          Marines|\n",
      "| 2879|        Air Force|\n",
      "| 3055|             Navy|\n",
      "|10417|             Army|\n",
      "+-----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Hired by Service branch\n",
    "spark.sql(\"\"\" Select count(hires.id) as hired,bio.service_branch__c \n",
    "              From dfHire_sql hires \n",
    "              inner join dfBio_sql bio on bio.id = hires.id\n",
    "              Where hires.hired = 1\n",
    "              group by bio.service_branch__c\n",
    "              order by hired asc\n",
    "              \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------+\n",
      "|Not_hired|service_branch__c|\n",
      "+---------+-----------------+\n",
      "|        2|   Not Applicable|\n",
      "|      667|      Coast Guard|\n",
      "|     9382|             null|\n",
      "|    10524|        Air Force|\n",
      "|    11260|          Marines|\n",
      "|    12500|             Navy|\n",
      "|    41314|             Army|\n",
      "+---------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Not hired by Service Branch\n",
    "spark.sql(\"\"\" Select count(hires.id) as Not_hired,bio.service_branch__c \n",
    "              From dfHire_sql hires \n",
    "              inner join dfBio_sql bio on bio.id = hires.id\n",
    "              Where hires.hired = 0\n",
    "              group by bio.service_branch__c\n",
    "              order by Not_hired asc\n",
    "              \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------+\n",
      "|hired|service_rank__c|\n",
      "+-----+---------------+\n",
      "| 3807|            E-5|\n",
      "| 3683|            E-4|\n",
      "| 2795|            E-7|\n",
      "| 2775|            E-6|\n",
      "| 1281|            E-8|\n",
      "| 1156|           null|\n",
      "| 1142|            O-3|\n",
      "|  648|            E-3|\n",
      "|  564|            O-4|\n",
      "|  510|            O-5|\n",
      "|  463|            E-9|\n",
      "|  294|            O-2|\n",
      "|  186|            O-6|\n",
      "|  178|            W-3|\n",
      "|  148|            W-4|\n",
      "|  137|            W-2|\n",
      "|  126|            E-2|\n",
      "|   79|            O-1|\n",
      "|   75|            E-1|\n",
      "|   27|            W-5|\n",
      "+-----+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Hired by rank\n",
    "spark.sql(\"\"\" Select count(hires.id) as hired,bio.service_rank__c\n",
    "              From dfHire_sql hires \n",
    "              inner join dfBio_sql bio on bio.id = hires.id\n",
    "              Where hires.hired = 1\n",
    "              group by bio.service_rank__c\n",
    "              order by hired desc\n",
    "              \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------------------+\n",
      "|count|preferred_method_of_contact__c|\n",
      "+-----+------------------------------+\n",
      "|    7|                          Mail|\n",
      "|   23|                      LinkedIn|\n",
      "|  158|                          Text|\n",
      "| 1892|                          null|\n",
      "| 2626|                        E-Mail|\n",
      "|15384|                     Telephone|\n",
      "+-----+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Prefered method of contact for the hired\n",
    "spark.sql(\"\"\" select count(com.id) as count, com.preferred_method_of_contact__c \n",
    "              from dfCom_sql com \n",
    "              inner join dfHire_sql hires on com.id = hires.id\n",
    "              where hires.hired = 1\n",
    "              group by com.preferred_method_of_contact__c \n",
    "              order by count\n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------------------+\n",
      "|count|preferred_method_of_contact__c|\n",
      "+-----+------------------------------+\n",
      "|   28|                          Mail|\n",
      "|   33|                      LinkedIn|\n",
      "|  378|                          Text|\n",
      "|12405|                        E-Mail|\n",
      "|31506|                          null|\n",
      "|41299|                     Telephone|\n",
      "+-----+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Prefered method of contact for the Unhired\n",
    "spark.sql(\"\"\" select count(com.id) as count, com.preferred_method_of_contact__c \n",
    "              from dfCom_sql com \n",
    "              inner join dfHire_sql hires on com.id = hires.id\n",
    "              where hires.hired = 0\n",
    "              group by com.preferred_method_of_contact__c \n",
    "              order by count\n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(client__c, client_type__c , service_rank__c , service_branch__c) - dfBio_sql\n",
    "# (hire_heroes_usa_confirmed_hire__c,preferred_method_of_contact__c,responsive__c, active__c)   #Target (hired) #dfHire_sql\n",
    "# (resume_tailoring_tips__c, finalized_hhusa_revised_resume_on_file__c, created_linkedin_account__c) dfAct_sql\n",
    "#(yearsinservice,ringdna100__email_attempts__c,ringdna100__call_attempts__c, reg_afterservice_years) dfFact_sql\n",
    "#Prefered method of contact for the Unhired\n",
    "dfspark= spark.sql(\"\"\" select fact.yearsinservice as S_years , fact.reg_afterservice_years as R_years,\n",
    "               act.resume_tailoring_tips__c as Resume_Tips,act.finalized_hhusa_revised_resume_on_file__c as Resume_OnFile, bio.client__c as Client, bio.client_type__c as Client_Type, bio.service_rank__c as Service_R\n",
    "              ,bio.service_branch__c as Service_B, hire.hire_heroes_usa_confirmed_hire__c as HHUSA_hire, com.preferred_method_of_contact__c as Com_Method, com.responsive__c\n",
    "              ,com.active__c, act.created_linkedin_account__c as Created_Linkedin, hire.hired\n",
    "               \n",
    "              from dfFact_sql fact\n",
    "              \n",
    "              inner join dfAct_sql act on act.id = fact.id\n",
    "              inner join dfBio_sql bio on bio.id = fact.id\n",
    "              inner join dfHire_sql hire on hire.id = fact.id\n",
    "              inner join dfCom_sql com on com.id = fact.id\n",
    "              \n",
    "              where bio.client_type__c != '' and bio.service_branch__c !='' and bio.service_rank__c !='' \n",
    "              and com.preferred_method_of_contact__c !='' and fact.yearsinservice < 53 and fact.yearsinservice > 0 \n",
    "              and fact.reg_afterservice_years >- 32 and fact.reg_afterservice_years < 53 and com.active__c !=''\n",
    "\n",
    "          \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53979"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfspark.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----------+-------------+------+-----------------+---------+---------+----------+----------+-------------+---------+----------------+-----+\n",
      "|S_years|R_years|Resume_Tips|Resume_OnFile|Client|      Client_Type|Service_R|Service_B|HHUSA_hire|Com_Method|responsive__c|active__c|Created_Linkedin|hired|\n",
      "+-------+-------+-----------+-------------+------+-----------------+---------+---------+----------+----------+-------------+---------+----------------+-----+\n",
      "|   8.11|  -0.37|          1|            1|     1|Online Registrant|      E-5|     Army|         0| Telephone|            1|      Yes|               1|    0|\n",
      "+-------+-------+-----------+-------------+------+-----------------+---------+---------+----------+----------+-------------+---------+----------------+-----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfspark.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(S_years=8.11, R_years=-0.37, Resume_Tips=1, Resume_OnFile=1, Client=1, Client_Type='Online Registrant', Service_R='E-5', Service_B='Army', HHUSA_hire=0, Com_Method='Telephone', responsive__c=1, active__c='Yes', Created_Linkedin=1, hired='0')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfspark.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelIndexer = StringIndexer(inputCol=\"Client_Type\", outputCol=\"Client_Type_indexed\").fit(dfspark).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('S_years', 'double'),\n",
       " ('R_years', 'double'),\n",
       " ('Resume_Tips', 'int'),\n",
       " ('Resume_OnFile', 'int'),\n",
       " ('Client', 'int'),\n",
       " ('Client_Type', 'string'),\n",
       " ('Service_R', 'string'),\n",
       " ('Service_B', 'string'),\n",
       " ('HHUSA_hire', 'int'),\n",
       " ('Com_Method', 'string'),\n",
       " ('responsive__c', 'int'),\n",
       " ('active__c', 'string'),\n",
       " ('Created_Linkedin', 'int'),\n",
       " ('hired', 'string')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfspark.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('S_years', 'double'),\n",
       " ('R_years', 'double'),\n",
       " ('Resume_Tips', 'int'),\n",
       " ('Resume_OnFile', 'int'),\n",
       " ('Client', 'int'),\n",
       " ('Client_Type', 'string'),\n",
       " ('Service_R', 'string'),\n",
       " ('Service_B', 'string'),\n",
       " ('HHUSA_hire', 'int'),\n",
       " ('Com_Method', 'string'),\n",
       " ('responsive__c', 'int'),\n",
       " ('active__c', 'string'),\n",
       " ('Created_Linkedin', 'int'),\n",
       " ('hired', 'string')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfspark.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "requirement failed: Column Client_Type must be of type struct<type:tinyint,size:int,indices:array<int>,values:array<double>> but was actually string.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-61827f131d4b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfeatureIndexer\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m         \u001b[0mVectorIndexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Client_Type'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Client_Type_Index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfspark\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    127\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    316\u001b[0m         \"\"\"\n\u001b[1;32m    317\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    135\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: requirement failed: Column Client_Type must be of type struct<type:tinyint,size:int,indices:array<int>,values:array<double>> but was actually string."
     ]
    }
   ],
   "source": [
    "featureIndexer =\\\n",
    "        VectorIndexer(inputCol='Client_Type', outputCol=\"Client_Type_Index\").fit(dfspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfspark.filter(dfspark.Created_Linkedin.isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|         Client_Type|\n",
      "+--------------------+\n",
      "|Online Training C...|\n",
      "|   Online Registrant|\n",
      "|Onward To Opportu...|\n",
      "|Workshop Participant|\n",
      "|                 AVR|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfspark.select(\"Client_Type\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----------+-------------+------+-----------------+---------+---------+----------+----------+-------------+---------+----------------+-----+---------------------+\n",
      "|S_years|R_years|Resume_Tips|Resume_OnFile|Client|      Client_Type|Service_R|Service_B|HHUSA_hire|Com_Method|responsive__c|active__c|Created_Linkedin|hired|Client_Type_Numerical|\n",
      "+-------+-------+-----------+-------------+------+-----------------+---------+---------+----------+----------+-------------+---------+----------------+-----+---------------------+\n",
      "|   8.11|  -0.37|          1|            1|     1|Online Registrant|      E-5|     Army|         0| Telephone|            1|      Yes|               1|    0|                  0.0|\n",
      "|  10.24|    0.4|          1|            1|     1|Online Registrant|      E-5|     Navy|         1| Telephone|            1|      Yes|               1|    0|                  0.0|\n",
      "|   4.69|   6.34|          1|            1|     1|Online Registrant|      E-4|     Army|         0| Telephone|            1|      Yes|               1|    0|                  0.0|\n",
      "|  24.86|    3.1|          1|            1|     1|Online Registrant|      E-9|     Army|         0| Telephone|            1|      Yes|               1|    0|                  0.0|\n",
      "|   5.43|   0.49|          0|            1|     1|Online Registrant|      E-4|     Navy|         0| Telephone|            1|      Yes|               0|    0|                  0.0|\n",
      "|    4.0|  -0.22|          1|            1|     1|Online Registrant|      E-4|  Marines|         0| Telephone|            1|      Yes|               1|    0|                  0.0|\n",
      "|  20.01|  -0.74|          1|            1|     1|Online Registrant|      E-6|Air Force|         0| Telephone|            1|      Yes|               0|    0|                  0.0|\n",
      "|  30.16|  -0.24|          1|            1|     1|Online Registrant|      O-4|     Navy|         0| Telephone|            1|      Yes|               1|    0|                  0.0|\n",
      "|   14.5|   2.48|          1|            1|     1|Online Registrant|      E-4|     Army|         0| Telephone|            1|      Yes|               1|    0|                  0.0|\n",
      "|  23.21|  -0.07|          0|            0|     1|Online Registrant|      O-5|     Army|         0|    E-Mail|            1|      Yes|               0|    0|                  0.0|\n",
      "|    6.7|  -0.49|          1|            1|     1|Online Registrant|      O-3|  Marines|         0| Telephone|            1|      Yes|               0|    0|                  0.0|\n",
      "|  15.04|  22.14|          1|            1|     1|Online Registrant|      E-5|     Army|         0| Telephone|            1|      Yes|               1|    0|                  0.0|\n",
      "|  21.66|  10.44|          1|            1|     1|Online Registrant|      E-8|  Marines|         0| Telephone|            1|      Yes|               1|    0|                  0.0|\n",
      "|   17.0|  -5.08|          1|            1|     1|Online Registrant|      O-4|Air Force|         1| Telephone|            1|      Yes|               1|    0|                  0.0|\n",
      "|   8.58|   6.85|          1|            1|     1|Online Registrant|      E-5|     Navy|         0| Telephone|            1|      Yes|               1|    0|                  0.0|\n",
      "|  14.82|   1.36|          1|            1|     1|Online Registrant|      E-6|     Army|         1| Telephone|            1|      Yes|               0|    0|                  0.0|\n",
      "|   7.76|   2.11|          0|            1|     1|Online Registrant|      E-5|     Army|         1| Telephone|            1|      Yes|               0|    0|                  0.0|\n",
      "|   5.36|  -0.15|          1|            1|     1|Online Registrant|      E-5|     Army|         0| Telephone|            1|      Yes|               1|    0|                  0.0|\n",
      "|  21.67|   2.43|          0|            0|     1|Online Registrant|      E-7|     Army|         0| Telephone|            1|      Yes|               0|    0|                  0.0|\n",
      "|   6.19|   3.86|          1|            1|     1|Online Registrant|      E-4|Air Force|         1| Telephone|            1|      Yes|               1|    0|                  0.0|\n",
      "+-------+-------+-----------+-------------+------+-----------------+---------+---------+----------+----------+-------------+---------+----------------+-----+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"Client_Type\", outputCol=\"Client_Type_Numerical\")\n",
    "indexed = indexer.fit(dfspark).transform(dfspark)\n",
    "#df = sqlContext.createDataFrame(\n",
    " #   [(0, \"a\"), (1, \"b\"), (2, \"c\"), (3, \"a\"), (4, \"a\"), (5, \"c\")],\n",
    "#    [\"id\", \"category\"])\n",
    "indexed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|Client_Type_Numerical|\n",
      "+---------------------+\n",
      "|                  0.0|\n",
      "|                  1.0|\n",
      "|                  4.0|\n",
      "|                  3.0|\n",
      "|                  2.0|\n",
      "+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(result=DenseVector([0.0271, 0.0145, 0.032]))\n",
      "Row(result=DenseVector([-0.0257, 0.0251, 0.0292]))\n",
      "Row(result=DenseVector([0.013, 0.063, -0.0221]))\n",
      "+--------------------+\n",
      "|                text|\n",
      "+--------------------+\n",
      "|[Hi, I, heard, ab...|\n",
      "|[I, wish, Java, c...|\n",
      "|[Logistic, regres...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "sc = SparkContext.getOrCreate()\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "\n",
    "# Input data: Each row is a bag of words from a sentence or document.\n",
    "documentDF = sqlContext.createDataFrame([\n",
    "  (\"Hi I heard about Spark\".split(\" \"), ),\n",
    "  (\"I wish Java could use case classes\".split(\" \"), ),\n",
    "  (\"Logistic regression models are neat\".split(\" \"), )\n",
    "], [\"text\"])\n",
    "# Learn a mapping from words to Vectors.\n",
    "word2Vec = Word2Vec(vectorSize=3, minCount=0, inputCol=\"text\", outputCol=\"result\")\n",
    "model = word2Vec.fit(documentDF)\n",
    "result = model.transform(documentDF)\n",
    "for feature in result.select(\"result\").take(3):\n",
    "    print(feature)\n",
    "\n",
    "documentDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(features=SparseVector(20, {6: 0.2877, 8: 0.6931, 13: 0.2877, 16: 0.5754}), label=0)\n",
      "Row(features=SparseVector(20, {0: 0.6931, 2: 0.6931, 7: 1.3863, 13: 0.2877, 15: 0.6931, 16: 0.2877}), label=0)\n",
      "Row(features=SparseVector(20, {3: 0.6931, 4: 0.6931, 6: 0.2877, 11: 0.6931, 19: 0.6931}), label=1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "\n",
    "sentenceData = sqlContext.createDataFrame([\n",
    "  (0, \"Hi I heard about Spark\"),\n",
    "  (0, \"I wish Java could use case classes\"),\n",
    "  (1, \"Logistic regression models are neat\")\n",
    "], [\"label\", \"sentence\"])\n",
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
    "wordsData = tokenizer.transform(sentenceData)\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=20)\n",
    "featurizedData = hashingTF.transform(wordsData)\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "rescaledData = idfModel.transform(featurizedData)\n",
    "for features_label in rescaledData.select(\"features\", \"label\").take(3):\n",
    "    print(features_label)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---------+------+------+--------------------------------------------------------+\n",
      "|real|bool |stringNum|string|target|features                                                |\n",
      "+----+-----+---------+------+------+--------------------------------------------------------+\n",
      "|2.2 |true |1        |foo   |1     |(262144,[174475,247670,257907,262126],[2.2,1.0,1.0,1.0])|\n",
      "|3.3 |false|2        |bar   |0     |(262144,[70644,89673,173866,174475],[1.0,1.0,1.0,3.3])  |\n",
      "|4.4 |false|3        |baz   |0     |(262144,[22406,70644,174475,187923],[1.0,1.0,4.4,1.0])  |\n",
      "|5.5 |false|4        |foo   |1     |(262144,[70644,101499,174475,257907],[1.0,1.0,5.5,1.0]) |\n",
      "+----+-----+---------+------+------+--------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import FeatureHasher\n",
    "\n",
    "dataset = spark.createDataFrame([\n",
    "    (2.2, True, \"1\", \"foo\", 1),\n",
    "    (3.3, False, \"2\", \"bar\", 0),\n",
    "    (4.4, False, \"3\", \"baz\", 0),\n",
    "    (5.5, False, \"4\", \"foo\", 1)\n",
    "], [\"real\", \"bool\", \"stringNum\", \"string\", \"target\"])\n",
    "\n",
    "hasher = FeatureHasher(inputCols=[\"real\", \"bool\", \"stringNum\", \"string\"],\n",
    "                       outputCol=\"features\")\n",
    "\n",
    "featurized = hasher.transform(dataset)\n",
    "featurized.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dfspark.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['real', 'bool', 'stringNum', 'string', 'features']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featurized.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "|(262144,[174475,2...|\n",
      "|(262144,[70644,89...|\n",
      "|(262144,[22406,70...|\n",
      "|(262144,[70644,10...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "featurized.select(\"features\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

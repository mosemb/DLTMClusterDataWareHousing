{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"DatawarehouseSpark Application\") \\\n",
    "    .getOrCreate()\n",
    " \n",
    "inputFileHire ='/home/mose/Downloads/CSVS/client_hiring_dt.csv' #\"/user/user17/mosedata_proj/input/client_hiring_dt.csv\"\n",
    "#inputFileBio ='/home/mose/Downloads/CSVS/client_bio_dt.csv'  #\"/user/user17/mosedata_proj/input/client_bio_dt.csv\"\n",
    "#inputFileCom = '/home/mose/Downloads/CSVS/client_communication_dt.csv' #\"/user/user17/mosedata_proj/input/client_communication_dt.csv\"\n",
    "#inputFileAct = '/home/mose/Downloads/CSVS/client_activities_dt.csv' #\"/user/user17/mosedata_proj/input/client_activities_dt.csv\"\n",
    "#inputFileFact = '/home/mose/Downloads/CSVS/client_fact_ft.csv' #\"/user/user17/mosedata_proj/input/client_fact_ft.csv\"\n",
    "\n",
    " \n",
    "# Create DataFrame from CSV file\n",
    "dfM =  spark.read.format(\"csv\").option(\"sep\", \",\").option(\"inferSchema\", \"true\").option(\"header\", \"true\").load(inputFileHire)\n",
    "#dfR =  spark.read.format(\"csv\").option(\"sep\", \",\").option(\"inferSchema\", \"true\").option(\"header\", \"true\").load(inputFileBio)\n",
    "#dfCom =  spark.read.format(\"csv\").option(\"sep\", \",\").option(\"inferSchema\", \"true\").option(\"header\", \"true\").load(inputFileCom)\n",
    "#dfAct =  spark.read.format(\"csv\").option(\"sep\", \",\").option(\"inferSchema\", \"true\").option(\"header\", \"true\").load(inputFileAct)\n",
    "#dfFact =  spark.read.format(\"csv\").option(\"sep\", \",\").option(\"inferSchema\", \"true\").option(\"header\", \"true\").load(inputFileFact)\n",
    "\n",
    "\n",
    "# Print the schema of the DataFrames\n",
    "dfM.printSchema()\n",
    "dfR.printSchema()\n",
    "dfCom.printSchema()\n",
    "dfAct.printSchema()\n",
    "dfFact.printSchema()\n",
    "\n",
    "\n",
    "#dfR.groupBy('movieId').count().show(50)\n",
    "#dfM.where(\"director='Martin Brest'\").join(dfR,\"movieId\").select(\"movieId\",\"director\",\"user_name\",\"rating\").show(50)\n",
    "\n",
    "\n",
    "# Stop the Spark Session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting dataframes.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile dataframes.py\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"DatawarehouseSpark Application\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "#Input the data\n",
    "inputFileHire ='/user/user17/mosedata_proj/input/client_hiring_dt.csv'\n",
    "inputFileBio ='/user/user17/mosedata_proj/input/client_bio_dt.csv'\n",
    "inputFileCom = '/user/user17/mosedata_proj/input/client_communication_dt.csv'\n",
    "inputFileAct = '/user/user17/mosedata_proj/input/client_activities_dt.csv'\n",
    "inputFileFact = '/user/user17/mosedata_proj/input/client_fact_ft.csv'\n",
    "\n",
    "#Create data frames from sources. Tables were converted to csvs now being converted to dataframes\n",
    "dfHire =  spark.read.format(\"csv\").option(\"sep\", \",\").option(\"inferSchema\", \"true\").option(\"header\", \"true\").load(inputFileHire)\n",
    "dfBio =  spark.read.format(\"csv\").option(\"sep\", \",\").option(\"inferSchema\", \"true\").option(\"header\", \"true\").load(inputFileBio)\n",
    "dfCom =  spark.read.format(\"csv\").option(\"sep\", \",\").option(\"inferSchema\", \"true\").option(\"header\", \"true\").load(inputFileCom)\n",
    "dfAct =  spark.read.format(\"csv\").option(\"sep\", \",\").option(\"inferSchema\", \"true\").option(\"header\", \"true\").load(inputFileAct)\n",
    "dfFact =  spark.read.format(\"csv\").option(\"sep\", \",\").option(\"inferSchema\", \"true\").option(\"header\", \"true\").load(inputFileFact)\n",
    "\n",
    "# Print the schema of the DataFrames\n",
    "dfHire.printSchema() \n",
    "dfBio.printSchema()\n",
    "dfCom.printSchema()\n",
    "dfAct.printSchema()\n",
    "dfFact.printSchema()\n",
    "\n",
    "#Clients from different service branches\n",
    "dfBio.groupBy('service_branch__c').count().show()\n",
    "\n",
    "#Types of jobs the guys got hired in\n",
    "dfHire.groupBy('job_function_hired_in__c').count().show()\n",
    "\n",
    "#Temporary objects from which sql statements are run for each dataframe\n",
    "dfHire.createOrReplaceTempView(\"dfHire_sql\")\n",
    "dfBio.createOrReplaceTempView(\"dfBio_sql\")\n",
    "dfCom.createOrReplaceTempView(\"dfCom_sql\")\n",
    "dfAct.createOrReplaceTempView(\"dfAct_sql\")\n",
    "dfFact.createOrReplaceTempView(\"dfFAct_sql\")\n",
    "\n",
    "#Mixed information from all 5 tables of the database\n",
    "spark.sql(''' Select hires.hired, bio.service_rank__c , \n",
    "              bio.service_branch__c , fact.yearsinservice, fact.reg_afterservice_years,\n",
    "              com.responsive__c, act.finalized_hhusa_revised_resume_on_file__c as resume_done\n",
    "              from dfHire_sql hires\n",
    "              inner join dfBio_sql bio on bio.id = hires.id \n",
    "              inner join dfFAct_sql fact on fact.id = hires.id\n",
    "              inner join dfCom_sql com on com.id = hires.id\n",
    "              inner join dfAct_sql act on act.id = hires.id\n",
    "              where hires.hired = \"1\"\n",
    "              \n",
    "              ''').show()\n",
    "\n",
    "#Account create date by month of the year\n",
    "spark.sql( \"\"\"SELECT count(id) , \n",
    "              Extract(Month From create_ddate) as month \n",
    "              From  dfAct_sql  \n",
    "              group by month \n",
    "              ORDER BY month ASC \"\"\").show()\n",
    "\n",
    "\n",
    "#Registration for services by year\n",
    "spark.sql( \"\"\"SELECT count(id) , \n",
    "Extract(Year From create_ddate) as year_registered \n",
    "              From  dfAct_sql  \n",
    "              group by year_registered\n",
    "              ORDER BY year_registered ASC \"\"\").show()\n",
    "\n",
    "#Hired by Service branch\n",
    "spark.sql(\"\"\" Select count(hires.id) as hired,bio.service_branch__c \n",
    "              From dfHire_sql hires \n",
    "              inner join dfBio_sql bio on bio.id = hires.id\n",
    "              Where hires.hired = 1\n",
    "              group by bio.service_branch__c\n",
    "              order by hired asc\n",
    "              \"\"\").show()\n",
    "\n",
    "#Not hired by Service Branch\n",
    "spark.sql(\"\"\" Select count(hires.id) as Not_hired,bio.service_branch__c \n",
    "              From dfHire_sql hires \n",
    "              inner join dfBio_sql bio on bio.id = hires.id\n",
    "              Where hires.hired = 0\n",
    "              group by bio.service_branch__c\n",
    "              order by Not_hired asc\n",
    "              \"\"\").show()\n",
    "\n",
    "#Hired by rank\n",
    "spark.sql(\"\"\" Select count(hires.id) as hired,bio.service_rank__c\n",
    "              From dfHire_sql hires \n",
    "              inner join dfBio_sql bio on bio.id = hires.id\n",
    "              Where hires.hired = 0\n",
    "              group by bio.service_rank__c\n",
    "              order by hired desc\n",
    "              \"\"\").show()\n",
    "\n",
    "#Prefered method of contact for the hired\n",
    "spark.sql(\"\"\" select count(com.id) as count, com.preferred_method_of_contact__c \n",
    "              from dfCom_sql com \n",
    "              inner join dfHire_sql hires on com.id = hires.id\n",
    "              where hires.hired = 1\n",
    "              group by com.preferred_method_of_contact__c \n",
    "              order by count\n",
    "          \"\"\").show()\n",
    "\n",
    "#Prefered method of contact for the Unhired\n",
    "spark.sql(\"\"\" select count(com.id) as count, com.preferred_method_of_contact__c \n",
    "              from dfCom_sql com \n",
    "              inner join dfHire_sql hires on com.id = hires.id\n",
    "              where hires.hired = 0\n",
    "              group by com.preferred_method_of_contact__c \n",
    "              order by count\n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"DatawarehouseSpark Application\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import col, regexp_replace, split\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "from pyspark.ml.feature import FeatureHasher\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputFileHire ='/home/mose/Downloads/CSVS/client_hiring_dt.csv' #\"/user/user17/mosedata_proj/input/client_hiring_dt.csv\"\n",
    "inputFileBio ='/home/mose/Downloads/CSVS/client_bio_dt.csv'  #\"/user/user17/mosedata_proj/input/client_bio_dt.csv\"\n",
    "inputFileCom = '/home/mose/Downloads/CSVS/client_communication_dt.csv' #\"/user/user17/mosedata_proj/input/client_communication_dt.csv\"\n",
    "inputFileAct = '/home/mose/Downloads/CSVS/client_activities_dt.csv' #\"/user/user17/mosedata_proj/input/client_activities_dt.csv\"\n",
    "inputFileFact = '/home/mose/Downloads/CSVS/client_fact_ft.csv' #\"/user/user17/mosedata_proj/input/client_fact_ft.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame from CSV file\n",
    "dfHire =  spark.read.format(\"csv\").option(\"sep\", \",\").option(\"inferSchema\", \"true\").option(\"header\", \"true\").load(inputFileHire)\n",
    "dfBio =  spark.read.format(\"csv\").option(\"sep\", \",\").option(\"inferSchema\", \"true\").option(\"header\", \"true\").load(inputFileBio)\n",
    "dfCom =  spark.read.format(\"csv\").option(\"sep\", \",\").option(\"inferSchema\", \"true\").option(\"header\", \"true\").load(inputFileCom)\n",
    "dfAct =  spark.read.format(\"csv\").option(\"sep\", \",\").option(\"inferSchema\", \"true\").option(\"header\", \"true\").load(inputFileAct)\n",
    "dfFact =  spark.read.format(\"csv\").option(\"sep\", \",\").option(\"inferSchema\", \"true\").option(\"header\", \"true\").load(inputFileFact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- dsubimittedforhire: string (nullable = true)\n",
      " |-- dconfirmedforhire: string (nullable = true)\n",
      " |-- difsubmitconfirmed: integer (nullable = true)\n",
      " |-- startdate: string (nullable = true)\n",
      " |-- datdifconfirstart: integer (nullable = true)\n",
      " |-- job_type__c: string (nullable = true)\n",
      " |-- job_function_hired_in__c: string (nullable = true)\n",
      " |-- hiring_account__c: string (nullable = true)\n",
      " |-- hire_heroes_usa_confirmed_hire__c: integer (nullable = true)\n",
      " |-- hired_location__c: string (nullable = true)\n",
      " |-- hired_zip_code__c: string (nullable = true)\n",
      " |-- industry_hired_in__c: string (nullable = true)\n",
      " |-- hired_but_still_active_and_looking__c: string (nullable = true)\n",
      " |-- hired: string (nullable = true)\n",
      " |-- client_hiring_dt_id: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- client__c: integer (nullable = true)\n",
      " |-- client_type__c: string (nullable = true)\n",
      " |-- service_rank__c: string (nullable = true)\n",
      " |-- service_branch__c: string (nullable = true)\n",
      " |-- primary_military_occupational_specialty__c: string (nullable = true)\n",
      " |-- gender__c: string (nullable = true)\n",
      " |-- highest_level_of_education_completed__c: string (nullable = true)\n",
      " |-- status__c: string (nullable = true)\n",
      " |-- military_spouse_caregiver__c: string (nullable = true)\n",
      " |-- acount_createddate: string (nullable = true)\n",
      " |-- dateofserviceentry: string (nullable = true)\n",
      " |-- dateofserviceseparation: string (nullable = true)\n",
      " |-- client_bio_dt_id: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- mailingstate: string (nullable = true)\n",
      " |-- mailingpostalcode: string (nullable = true)\n",
      " |-- mailingcountry: string (nullable = true)\n",
      " |-- preferred_method_of_contact__c: string (nullable = true)\n",
      " |-- responsive__c: integer (nullable = true)\n",
      " |-- active__c: string (nullable = true)\n",
      " |-- client_communication_dt_id: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- create_ddate: string (nullable = true)\n",
      " |-- dateassignedtohhusa: string (nullable = true)\n",
      " |-- dateassignedtostaff: string (nullable = true)\n",
      " |-- dateinitialassesment: string (nullable = true)\n",
      " |-- dateresumecompleted: string (nullable = true)\n",
      " |-- dsubimittedforhire: string (nullable = true)\n",
      " |-- confirmedhiredate: string (nullable = true)\n",
      " |-- startdate: string (nullable = true)\n",
      " |-- resume_completed_by__c: string (nullable = true)\n",
      " |-- resume_tailoring_tips__c: integer (nullable = true)\n",
      " |-- finalized_hhusa_revised_resume_on_file__c: integer (nullable = true)\n",
      " |-- created_linkedin_account__c: integer (nullable = true)\n",
      " |-- client_act_dt_id: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- yearsinservice: double (nullable = true)\n",
      " |-- ringdna100__email_attempts__c: integer (nullable = true)\n",
      " |-- ringdna100__call_attempts__c: integer (nullable = true)\n",
      " |-- client_act_dt_id: string (nullable = true)\n",
      " |-- client_bio_dt_id: string (nullable = true)\n",
      " |-- client_communication_dt_id: string (nullable = true)\n",
      " |-- client_hiring_dt_id: string (nullable = true)\n",
      " |-- reg_afterservice_months: double (nullable = true)\n",
      " |-- reg_afterservice_years: double (nullable = true)\n",
      " |-- monthsinservice: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the schema of the DataFrames\n",
    "dfHire.printSchema() \n",
    "dfBio.printSchema()\n",
    "dfCom.printSchema()\n",
    "dfAct.printSchema()\n",
    "dfFact.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+\n",
      "|service_branch__c|count|\n",
      "+-----------------+-----+\n",
      "|          Marines|13691|\n",
      "|           Spouse|    2|\n",
      "|  Merchant Marine|    1|\n",
      "|             null|37153|\n",
      "|        Air Force|13416|\n",
      "|             Army|51770|\n",
      "|      Coast Guard|  837|\n",
      "|   Not Applicable|    4|\n",
      "|             Navy|15567|\n",
      "+-----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Clients from different service branches\n",
    "dfBio.groupBy('service_branch__c').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+------+\n",
      "|job_function_hired_in__c| count|\n",
      "+------------------------+------+\n",
      "|    IT - Help Desk/Su...|   287|\n",
      "|    Skilled Labor/Trades|   484|\n",
      "|    Purchasing/Procur...|    73|\n",
      "|                   Sales|   697|\n",
      "|    Firefighter/EMT/E...|    71|\n",
      "|                 Science|    28|\n",
      "|    Training/Instruct...|  1113|\n",
      "|    Supply Chain/Logi...|   892|\n",
      "|             Engineering|   465|\n",
      "|    Media/Journalism/...|    54|\n",
      "|         Banking/Finance|   522|\n",
      "|              Healthcare|   900|\n",
      "|    Quality Assurance...|   238|\n",
      "|              Accounting|   171|\n",
      "|    IT - Computer Sci...|    41|\n",
      "|                    null|109255|\n",
      "|    Business Development|   196|\n",
      "|    Facilities Manage...|   165|\n",
      "|              Consultant|   456|\n",
      "|    Installation/Main...|  1223|\n",
      "+------------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Types of jobs the guys got hired in\n",
    "dfHire.groupBy('job_function_hired_in__c').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Temporary objects from which sql statements are run for each dataframe\n",
    "dfHire.createOrReplaceTempView(\"dfHire_sql\")\n",
    "dfBio.createOrReplaceTempView(\"dfBio_sql\")\n",
    "dfCom.createOrReplaceTempView(\"dfCom_sql\")\n",
    "dfAct.createOrReplaceTempView(\"dfAct_sql\")\n",
    "dfFact.createOrReplaceTempView(\"dfFAct_sql\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------+-----------------+--------------+----------------------+-------------+-----------+\n",
      "|hired|service_rank__c|service_branch__c|yearsinservice|reg_afterservice_years|responsive__c|resume_done|\n",
      "+-----+---------------+-----------------+--------------+----------------------+-------------+-----------+\n",
      "|    1|            O-3|             Army|         11.04|                 -0.33|            1|          1|\n",
      "|    1|            E-9|             Army|         30.18|                 -0.15|            1|          1|\n",
      "|    1|            E-4|             Army|           9.8|                 -0.73|            1|          1|\n",
      "|    1|            E-4|             Army|          2.81|                  0.43|            1|          1|\n",
      "|    1|            E-4|          Marines|          4.47|                  4.45|            1|          1|\n",
      "|    1|            O-2|             Army|          4.38|                 -0.51|            1|          1|\n",
      "|    1|            O-3|             Army|         11.03|                 -0.09|            1|          1|\n",
      "|    1|            E-8|             Army|         20.59|                  2.14|            1|          1|\n",
      "|    1|            E-7|             Army|         22.27|                 -0.42|            1|          1|\n",
      "|    1|            E-7|             Army|         20.01|                 -0.53|            1|          1|\n",
      "|    1|            O-3|             Army|          7.89|                 -0.41|            1|          1|\n",
      "|    1|            E-5|             Navy|          7.94|                 -0.31|            1|          1|\n",
      "|    1|            O-3|             Army|         16.89|                  1.54|            1|          1|\n",
      "|    1|            E-9|        Air Force|         29.82|                 -0.62|            1|          1|\n",
      "|    1|            E-7|        Air Force|         20.06|                  3.47|            1|          1|\n",
      "|    1|            W-1|             Army|         41.99|                -23.56|            1|          1|\n",
      "|    1|            O-3|             Army|         20.83|                 -0.82|            1|          1|\n",
      "|    1|            O-3|             Army|          4.08|                  0.03|            1|          1|\n",
      "|    1|            E-6|             Army|           6.1|                 -0.33|            1|          1|\n",
      "|    1|            E-4|             Navy|          7.09|                  8.58|            1|          1|\n",
      "+-----+---------------+-----------------+--------------+----------------------+-------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Mixed information from all 5 tables of the database\n",
    "spark.sql(''' Select hires.hired, bio.service_rank__c , \n",
    "              bio.service_branch__c , fact.yearsinservice, fact.reg_afterservice_years,\n",
    "              com.responsive__c, act.finalized_hhusa_revised_resume_on_file__c as resume_done\n",
    "              from dfHire_sql hires\n",
    "              inner join dfBio_sql bio on bio.id = hires.id \n",
    "              inner join dfFAct_sql fact on fact.id = hires.id\n",
    "              inner join dfCom_sql com on com.id = hires.id\n",
    "              inner join dfAct_sql act on act.id = hires.id\n",
    "              where hires.hired = \"1\"\n",
    "              \n",
    "              ''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|count(id)|month|\n",
      "+---------+-----+\n",
      "|    11982|    1|\n",
      "|     9626|    2|\n",
      "|    10152|    3|\n",
      "|    12579|    4|\n",
      "|    10978|    5|\n",
      "|    10672|    6|\n",
      "|    10815|    7|\n",
      "|    11912|    8|\n",
      "|    10845|    9|\n",
      "|    11866|   10|\n",
      "|    12022|   11|\n",
      "|     8992|   12|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Account create date by month of the year\n",
    "spark.sql( \"\"\"SELECT count(id) , \n",
    "              Extract(Month From create_ddate) as month \n",
    "              From  dfAct_sql  \n",
    "              group by month \n",
    "              ORDER BY month ASC \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------+\n",
      "|count(id)|year_registered|\n",
      "+---------+---------------+\n",
      "|     2825|           2007|\n",
      "|     2571|           2008|\n",
      "|     1769|           2009|\n",
      "|     2255|           2010|\n",
      "|     2732|           2011|\n",
      "|     4258|           2012|\n",
      "|    13128|           2013|\n",
      "|    13593|           2014|\n",
      "|    13248|           2015|\n",
      "|    25175|           2016|\n",
      "|    22933|           2017|\n",
      "|    26448|           2018|\n",
      "|     1506|           2019|\n",
      "+---------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Registration for services by year\n",
    "spark.sql( \"\"\"SELECT count(id) , \n",
    "Extract(Year From create_ddate) as year_registered \n",
    "              From  dfAct_sql  \n",
    "              group by year_registered\n",
    "              ORDER BY year_registered ASC \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------+\n",
      "|hired|service_branch__c|\n",
      "+-----+-----------------+\n",
      "|    1|   Not Applicable|\n",
      "|  170|      Coast Guard|\n",
      "| 1145|             null|\n",
      "| 2423|          Marines|\n",
      "| 2879|        Air Force|\n",
      "| 3055|             Navy|\n",
      "|10417|             Army|\n",
      "+-----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Hired by Service branch\n",
    "spark.sql(\"\"\" Select count(hires.id) as hired,bio.service_branch__c \n",
    "              From dfHire_sql hires \n",
    "              inner join dfBio_sql bio on bio.id = hires.id\n",
    "              Where hires.hired = 1\n",
    "              group by bio.service_branch__c\n",
    "              order by hired asc\n",
    "              \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------+\n",
      "|Not_hired|service_branch__c|\n",
      "+---------+-----------------+\n",
      "|        2|   Not Applicable|\n",
      "|      667|      Coast Guard|\n",
      "|     9382|             null|\n",
      "|    10524|        Air Force|\n",
      "|    11260|          Marines|\n",
      "|    12500|             Navy|\n",
      "|    41314|             Army|\n",
      "+---------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Not hired by Service Branch\n",
    "spark.sql(\"\"\" Select count(hires.id) as Not_hired,bio.service_branch__c \n",
    "              From dfHire_sql hires \n",
    "              inner join dfBio_sql bio on bio.id = hires.id\n",
    "              Where hires.hired = 0\n",
    "              group by bio.service_branch__c\n",
    "              order by Not_hired asc\n",
    "              \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------+\n",
      "|hired|service_rank__c|\n",
      "+-----+---------------+\n",
      "| 3807|            E-5|\n",
      "| 3683|            E-4|\n",
      "| 2795|            E-7|\n",
      "| 2775|            E-6|\n",
      "| 1281|            E-8|\n",
      "| 1156|           null|\n",
      "| 1142|            O-3|\n",
      "|  648|            E-3|\n",
      "|  564|            O-4|\n",
      "|  510|            O-5|\n",
      "|  463|            E-9|\n",
      "|  294|            O-2|\n",
      "|  186|            O-6|\n",
      "|  178|            W-3|\n",
      "|  148|            W-4|\n",
      "|  137|            W-2|\n",
      "|  126|            E-2|\n",
      "|   79|            O-1|\n",
      "|   75|            E-1|\n",
      "|   27|            W-5|\n",
      "+-----+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Hired by rank\n",
    "spark.sql(\"\"\" Select count(hires.id) as hired,bio.service_rank__c\n",
    "              From dfHire_sql hires \n",
    "              inner join dfBio_sql bio on bio.id = hires.id\n",
    "              Where hires.hired = 1\n",
    "              group by bio.service_rank__c\n",
    "              order by hired desc\n",
    "              \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------------------+\n",
      "|count|preferred_method_of_contact__c|\n",
      "+-----+------------------------------+\n",
      "|    7|                          Mail|\n",
      "|   23|                      LinkedIn|\n",
      "|  158|                          Text|\n",
      "| 1892|                          null|\n",
      "| 2626|                        E-Mail|\n",
      "|15384|                     Telephone|\n",
      "+-----+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Prefered method of contact for the hired\n",
    "spark.sql(\"\"\" select count(com.id) as count, com.preferred_method_of_contact__c \n",
    "              from dfCom_sql com \n",
    "              inner join dfHire_sql hires on com.id = hires.id\n",
    "              where hires.hired = 1\n",
    "              group by com.preferred_method_of_contact__c \n",
    "              order by count\n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------------------+\n",
      "|count|preferred_method_of_contact__c|\n",
      "+-----+------------------------------+\n",
      "|   28|                          Mail|\n",
      "|   33|                      LinkedIn|\n",
      "|  378|                          Text|\n",
      "|12405|                        E-Mail|\n",
      "|31506|                          null|\n",
      "|41299|                     Telephone|\n",
      "+-----+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Prefered method of contact for the Unhired\n",
    "spark.sql(\"\"\" select count(com.id) as count, com.preferred_method_of_contact__c \n",
    "              from dfCom_sql com \n",
    "              inner join dfHire_sql hires on com.id = hires.id\n",
    "              where hires.hired = 0\n",
    "              group by com.preferred_method_of_contact__c \n",
    "              order by count\n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(client__c, client_type__c , service_rank__c , service_branch__c) - dfBio_sql\n",
    "# (hire_heroes_usa_confirmed_hire__c,preferred_method_of_contact__c,responsive__c, active__c)   #Target (hired) #dfHire_sql\n",
    "# (resume_tailoring_tips__c, finalized_hhusa_revised_resume_on_file__c, created_linkedin_account__c) dfAct_sql\n",
    "#(yearsinservice,ringdna100__email_attempts__c,ringdna100__call_attempts__c, reg_afterservice_years) dfFact_sql\n",
    "#Prefered method of contact for the Unhired\n",
    "dfspark= spark.sql(\"\"\" select fact.yearsinservice as S_years , fact.reg_afterservice_years as R_years,\n",
    "               act.resume_tailoring_tips__c as Resume_Tips,act.finalized_hhusa_revised_resume_on_file__c as Resume_OnFile, bio.client__c as Client, bio.client_type__c as Client_Type, bio.service_rank__c as Service_R\n",
    "              ,bio.service_branch__c as Service_B, hire.hire_heroes_usa_confirmed_hire__c as HHUSA_hire, com.preferred_method_of_contact__c as Com_Method, com.responsive__c\n",
    "              ,com.active__c, act.created_linkedin_account__c as Created_Linkedin, bio.highest_level_of_education_completed__c as Educ,  hire.hired\n",
    "              ,bio.primary_military_occupational_specialty__c as Occupation\n",
    "               \n",
    "              from dfFact_sql fact\n",
    "              \n",
    "              inner join dfAct_sql act on act.id = fact.id\n",
    "              inner join dfBio_sql bio on bio.id = fact.id\n",
    "              inner join dfHire_sql hire on hire.id = fact.id\n",
    "              inner join dfCom_sql com on com.id = fact.id\n",
    "              \n",
    "              where bio.client_type__c != '' and bio.service_branch__c !='' and bio.service_rank__c !='' \n",
    "              and com.preferred_method_of_contact__c !='' and fact.yearsinservice < 53 and fact.yearsinservice > 0 \n",
    "              and fact.reg_afterservice_years >- 32 and fact.reg_afterservice_years < 53 and com.active__c !=''\n",
    "              and bio.highest_level_of_education_completed__c !='' and hire.hired  !='' and hire.hired !='No' and hire.hired != 3\n",
    "\n",
    "          \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|count(id)|hired|\n",
      "+---------+-----+\n",
      "|    26701|    3|\n",
      "|    85649|    0|\n",
      "|        1|   No|\n",
      "|    20090|    1|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\" select count(id),  hired from dfHire_sql  group by hired\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51093"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfspark.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfspark = dfspark.withColumn(\"myColumn\", df[\"myColumn\"].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|hired|\n",
      "+-----+\n",
      "|    0|\n",
      "|    1|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfspark.select('hired').distinct().show()\n",
    "#dfspark.select(\"Client_Type\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Input data: Each row is a bag of words from a sentence or document.\n",
    "documentDF = spark.createDataFrame([\n",
    "    (\"Hi I heard about Spark\".split(\" \"), ),\n",
    "    (\"I wish Java could use case classes\".split(\" \"), ),\n",
    "    (\"Logistic regression models are neat\".split(\" \"), )\n",
    "], [\"text\"])\n",
    "\n",
    "# Learn a mapping from words to Vectors.\n",
    "word2Vec = Word2Vec(vectorSize=4, minCount=0, inputCol=\"text\", outputCol=\"result\")\n",
    "model = word2Vec.fit(documentDF)\n",
    "\n",
    "result = model.transform(documentDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "_dfspark = dfspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('S_years', 'double'),\n",
       " ('R_years', 'double'),\n",
       " ('Resume_Tips', 'int'),\n",
       " ('Resume_OnFile', 'int'),\n",
       " ('Client', 'int'),\n",
       " ('Client_Type', 'string'),\n",
       " ('Service_R', 'string'),\n",
       " ('Service_B', 'string'),\n",
       " ('HHUSA_hire', 'int'),\n",
       " ('Com_Method', 'string'),\n",
       " ('responsive__c', 'int'),\n",
       " ('active__c', 'string'),\n",
       " ('Created_Linkedin', 'int'),\n",
       " ('Educ', 'string'),\n",
       " ('hired', 'double'),\n",
       " ('Occupation', 'string')]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_dfspark.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----------+-------------+------+-----------------+---------+---------+----------+----------+-------------+---------+----------------+---------------------------------------+-----+---------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|S_years|R_years|Resume_Tips|Resume_OnFile|Client|Client_Type      |Service_R|Service_B|HHUSA_hire|Com_Method|responsive__c|active__c|Created_Linkedin|Educ                                   |hired|Occupation                                         |features                                                                                                                                                                          |\n",
      "+-------+-------+-----------+-------------+------+-----------------+---------+---------+----------+----------+-------------+---------+----------------+---------------------------------------+-----+---------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|8.11   |-0.37  |1          |1            |1     |Online Registrant|E-5      |Army     |0         |Telephone |1            |Yes      |1               |High School/GED                        |0.0  |91F                                                |(262144,[52387,65085,69515,81887,119601,132517,138738,166656,207405,220379,230454,239098,246347,249041,251060],[1.0,8.11,1.0,1.0,1.0,1.0,1.0,-0.37,1.0,1.0,1.0,1.0,1.0,1.0,0.0])  |\n",
      "|10.24  |0.4    |1          |1            |1     |Online Registrant|E-5      |Navy     |1         |Telephone |1            |Yes      |1               |2 Year Degree (AA, AS, etc.)           |0.0  |LS 2 logistics specialist                          |(262144,[52387,65085,69515,81887,132517,138738,153520,166656,174237,214526,220379,230454,239098,249041,251060],[1.0,10.24,1.0,1.0,1.0,1.0,1.0,0.4,1.0,1.0,1.0,1.0,1.0,1.0,1.0])   |\n",
      "|4.69   |6.34   |1          |1            |1     |Online Registrant|E-4      |Army     |0         |Telephone |1            |Yes      |1               |4 Year Degree (BA, BS, etc.)           |0.0  |11B Infantry                                       |(262144,[65085,69515,81887,124597,132517,138738,164100,166656,207405,216334,220379,230454,239098,249041,251060],[4.69,1.0,1.0,1.0,1.0,1.0,1.0,6.34,1.0,1.0,1.0,1.0,1.0,1.0,0.0])  |\n",
      "|24.86  |3.1    |1          |1            |1     |Online Registrant|E-9      |Army     |0         |Telephone |1            |Yes      |1               |Post-Graduate Degree (MA, MS, JD, etc.)|0.0  |null                                               |(262144,[65085,69515,81887,90614,132517,138738,154591,166656,207405,220379,230454,239098,249041,251060],[24.86,1.0,1.0,1.0,1.0,1.0,1.0,3.1,1.0,1.0,1.0,1.0,1.0,0.0])              |\n",
      "|5.43   |0.49   |0          |1            |1     |Online Registrant|E-4      |Navy     |0         |Telephone |1            |Yes      |0               |High School/GED                        |0.0  |Nuclear Machinist                                  |(262144,[65085,69515,81887,124597,132517,138738,166656,174237,220379,228006,230454,239098,246347,249041,251060],[5.43,1.0,1.0,1.0,1.0,0.0,0.49,1.0,0.0,1.0,1.0,1.0,1.0,1.0,0.0])  |\n",
      "|4.0    |-0.22  |1          |1            |1     |Online Registrant|E-4      |Marines  |0         |Telephone |1            |Yes      |1               |High School/GED                        |0.0  |Network Admin Specialist                           |(262144,[64366,65085,69515,81887,123310,124597,132517,138738,166656,220379,230454,239098,246347,249041,251060],[1.0,4.0,1.0,1.0,1.0,1.0,1.0,1.0,-0.22,1.0,1.0,1.0,1.0,1.0,0.0])   |\n",
      "|20.01  |-0.74  |1          |1            |1     |Online Registrant|E-6      |Air Force|0         |Telephone |1            |Yes      |0               |4 Year Degree (BA, BS, etc.)           |0.0  |HR                                                 |(262144,[54411,65085,69515,81887,132517,138738,155676,166656,204127,216334,220379,230454,239098,249041,251060],[1.0,20.01,1.0,1.0,1.0,1.0,1.0,-0.74,1.0,1.0,0.0,1.0,1.0,1.0,0.0]) |\n",
      "|30.16  |-0.24  |1          |1            |1     |Online Registrant|O-4      |Navy     |0         |Telephone |1            |Yes      |1               |High School/GED                        |0.0  |Administration Officer                             |(262144,[65085,69515,81887,132517,138738,166656,174237,220379,230454,239098,239268,243998,246347,249041,251060],[30.16,1.0,1.0,1.0,1.0,-0.24,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0])|\n",
      "|14.5   |2.48   |1          |1            |1     |Online Registrant|E-4      |Army     |0         |Telephone |1            |Yes      |1               |High School/GED                        |0.0  |null                                               |(262144,[65085,69515,81887,124597,132517,138738,166656,207405,220379,230454,239098,246347,249041,251060],[14.5,1.0,1.0,1.0,1.0,1.0,2.48,1.0,1.0,1.0,1.0,1.0,1.0,0.0])             |\n",
      "|23.21  |-0.07  |0          |0            |1     |Online Registrant|O-5      |Army     |0         |E-Mail    |1            |Yes      |0               |Doctorate (PhD, MD, etc.)              |0.0  |null                                               |(262144,[65085,69515,79785,81887,138738,166656,177121,191315,207405,220379,230454,239098,249041,251060],[23.21,1.0,1.0,1.0,0.0,-0.07,1.0,1.0,1.0,0.0,1.0,1.0,0.0,0.0])            |\n",
      "|6.7    |-0.49  |1          |1            |1     |Online Registrant|O-3      |Marines  |0         |Telephone |1            |Yes      |0               |4 Year Degree (BA, BS, etc.)           |0.0  |null                                               |(262144,[11074,65085,69515,81887,123310,132517,138738,166656,216334,220379,230454,239098,249041,251060],[1.0,6.7,1.0,1.0,1.0,1.0,1.0,-0.49,1.0,0.0,1.0,1.0,1.0,0.0])              |\n",
      "|15.04  |22.14  |1          |1            |1     |Online Registrant|E-5      |Army     |0         |Telephone |1            |Yes      |1               |4 Year Degree (BA, BS, etc.)           |0.0  |null                                               |(262144,[52387,65085,69515,81887,132517,138738,166656,207405,216334,220379,230454,239098,249041,251060],[1.0,15.04,1.0,1.0,1.0,1.0,22.14,1.0,1.0,1.0,1.0,1.0,1.0,0.0])            |\n",
      "|21.66  |10.44  |1          |1            |1     |Online Registrant|E-8      |Marines  |0         |Telephone |1            |Yes      |1               |High School/GED                        |0.0  |0511 Marine Air Ground Taskforce Planning Logistics|(262144,[40267,65085,69515,81887,99567,123310,132517,138738,166656,220379,230454,239098,246347,249041,251060],[1.0,21.66,1.0,1.0,1.0,1.0,1.0,1.0,10.44,1.0,1.0,1.0,1.0,1.0,0.0])  |\n",
      "|17.0   |-5.08  |1          |1            |1     |Online Registrant|O-4      |Air Force|1         |Telephone |1            |Yes      |1               |4 Year Degree (BA, BS, etc.)           |0.0  |null                                               |(262144,[65085,69515,81887,132517,138738,166656,204127,216334,220379,230454,239098,243998,249041,251060],[17.0,1.0,1.0,1.0,1.0,-5.08,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])            |\n",
      "|8.58   |6.85   |1          |1            |1     |Online Registrant|E-5      |Navy     |0         |Telephone |1            |Yes      |1               |4 Year Degree (BA, BS, etc.)           |0.0  |null                                               |(262144,[52387,65085,69515,81887,132517,138738,166656,174237,216334,220379,230454,239098,249041,251060],[1.0,8.58,1.0,1.0,1.0,1.0,6.85,1.0,1.0,1.0,1.0,1.0,1.0,0.0])              |\n",
      "|14.82  |1.36   |1          |1            |1     |Online Registrant|E-6      |Army     |1         |Telephone |1            |Yes      |0               |4 Year Degree (BA, BS, etc.)           |0.0  |Electrical Engineer, IS Technician                 |(262144,[65085,69515,81887,132517,138738,147720,155676,166656,207405,216334,220379,230454,239098,249041,251060],[14.82,1.0,1.0,1.0,1.0,1.0,1.0,1.36,1.0,1.0,0.0,1.0,1.0,1.0,1.0]) |\n",
      "|7.76   |2.11   |0          |1            |1     |Online Registrant|E-5      |Army     |1         |Telephone |1            |Yes      |0               |2 Year Degree (AA, AS, etc.)           |0.0  |11B                                                |(262144,[48884,52387,65085,69515,81887,132517,138738,153520,166656,207405,220379,230454,239098,249041,251060],[1.0,1.0,7.76,1.0,1.0,1.0,0.0,1.0,2.11,1.0,0.0,1.0,1.0,1.0,1.0])    |\n",
      "|5.36   |-0.15  |1          |1            |1     |Online Registrant|E-5      |Army     |0         |Telephone |1            |Yes      |1               |High School/GED                        |0.0  |null                                               |(262144,[52387,65085,69515,81887,132517,138738,166656,207405,220379,230454,239098,246347,249041,251060],[1.0,5.36,1.0,1.0,1.0,1.0,-0.15,1.0,1.0,1.0,1.0,1.0,1.0,0.0])             |\n",
      "|21.67  |2.43   |0          |0            |1     |Online Registrant|E-7      |Army     |0         |Telephone |1            |Yes      |0               |Post-Graduate Degree (MA, MS, JD, etc.)|0.0  |25B40 IT Specialist                                |(262144,[65085,69515,81887,132517,132869,138738,148576,154591,166656,207405,220379,230454,239098,249041,251060],[21.67,1.0,1.0,1.0,1.0,0.0,1.0,1.0,2.43,1.0,0.0,1.0,1.0,0.0,0.0]) |\n",
      "|6.19   |3.86   |1          |1            |1     |Online Registrant|E-4      |Air Force|1         |Telephone |1            |Yes      |1               |High School/GED                        |0.0  |Aerospace Ground Equipment Journeyman              |(262144,[65085,69515,81887,124597,132517,138738,166656,203738,204127,220379,230454,239098,246347,249041,251060],[6.19,1.0,1.0,1.0,1.0,1.0,3.86,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])  |\n",
      "+-------+-------+-----------+-------------+------+-----------------+---------+---------+----------+----------+-------------+---------+----------------+---------------------------------------+-----+---------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hasher = FeatureHasher(inputCols= ['S_years','R_years','Resume_Tips','Resume_OnFile','Client','Client_Type','Service_R',\\\n",
    " 'Service_B','HHUSA_hire','Com_Method','responsive__c','active__c','Created_Linkedin','Educ','Occupation'],\n",
    "                       outputCol=\"features\")\n",
    "\n",
    "featurizedf = hasher.transform(dfspark)\n",
    "featurizedf.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|hired|\n",
      "+--------------------+-----+\n",
      "|(262144,[52387,65...|  0.0|\n",
      "|(262144,[52387,65...|  0.0|\n",
      "|(262144,[65085,69...|  0.0|\n",
      "|(262144,[65085,69...|  0.0|\n",
      "|(262144,[65085,69...|  0.0|\n",
      "|(262144,[64366,65...|  0.0|\n",
      "|(262144,[54411,65...|  0.0|\n",
      "|(262144,[65085,69...|  0.0|\n",
      "|(262144,[65085,69...|  0.0|\n",
      "|(262144,[65085,69...|  0.0|\n",
      "|(262144,[11074,65...|  0.0|\n",
      "|(262144,[52387,65...|  0.0|\n",
      "|(262144,[40267,65...|  0.0|\n",
      "|(262144,[65085,69...|  0.0|\n",
      "|(262144,[52387,65...|  0.0|\n",
      "|(262144,[65085,69...|  0.0|\n",
      "|(262144,[48884,52...|  0.0|\n",
      "|(262144,[52387,65...|  0.0|\n",
      "|(262144,[65085,69...|  0.0|\n",
      "|(262144,[65085,69...|  0.0|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "featurizedf.select('features','hired').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|hired|\n",
      "+-----+\n",
      "|    3|\n",
      "|    0|\n",
      "|   No|\n",
      "|    1|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfspark.select('hired').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfspark= dfspark.withColumn(\"Client_Type\", split(regexp_replace(col(\"Client_Type\"), r\"(^\\[)|(\\]$)|(')\", \"\"), \", \"))\n",
    "dfspark= dfspark.withColumn(\"Educ\", split(regexp_replace(col(\"Educ\"), r\"(^\\[)|(\\]$)|(')\", \"\"), \", \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('S_years', 'double'),\n",
       " ('R_years', 'double'),\n",
       " ('Resume_Tips', 'int'),\n",
       " ('Resume_OnFile', 'int'),\n",
       " ('Client', 'int'),\n",
       " ('Client_Type', 'array<string>'),\n",
       " ('Service_R', 'string'),\n",
       " ('Service_B', 'string'),\n",
       " ('HHUSA_hire', 'int'),\n",
       " ('Com_Method', 'string'),\n",
       " ('responsive__c', 'int'),\n",
       " ('active__c', 'string'),\n",
       " ('Created_Linkedin', 'int'),\n",
       " ('Educ', 'array<string>'),\n",
       " ('hired', 'string')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfspark.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[S_years: double, R_years: double, Resume_Tips: int, Resume_OnFile: int, Client: int, Client_Type: array<string>, Service_R: string, Service_B: string, HHUSA_hire: int, Com_Method: string, responsive__c: int, active__c: string, Created_Linkedin: int, Educ: array<string>, hired: string, EducVec: vector, Client_Type_Vec: vector]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = ['Educ','Client_Type']\n",
    "vecs = ['EducVec', 'Client_Type_Vec'] \n",
    "def createvecs(cols,vecs,df):\n",
    "    \"\"\" Takes in 2 lists to return a new transformed df \"\"\"\n",
    "    for f, b in zip(cols, vecs):\n",
    "        word2Vec2 = Word2Vec(vectorSize=4, minCount=0, inputCol=f, outputCol=b)\n",
    "        model = word2Vec2.fit(df)\n",
    "        df = model.transform(df)\n",
    "    return df\n",
    "#dfspark.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfspark = createvecs(cols,vecs,dfspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------------------+--------------------+\n",
      "|                Educ|             EducVec|        Client_Type|     Client_Type_Vec|\n",
      "+--------------------+--------------------+-------------------+--------------------+\n",
      "|   [High School/GED]|[-0.0665499120950...|[Online Registrant]|[0.05286745727062...|\n",
      "|[2 Year Degree (A...|[0.09756429990132...|[Online Registrant]|[0.05286745727062...|\n",
      "|[4 Year Degree (B...|[-0.8293966402610...|[Online Registrant]|[0.05286745727062...|\n",
      "|[Post-Graduate De...|[0.45273831486701...|[Online Registrant]|[0.05286745727062...|\n",
      "|   [High School/GED]|[-0.0665499120950...|[Online Registrant]|[0.05286745727062...|\n",
      "|   [High School/GED]|[-0.0665499120950...|[Online Registrant]|[0.05286745727062...|\n",
      "|[4 Year Degree (B...|[-0.8293966402610...|[Online Registrant]|[0.05286745727062...|\n",
      "|   [High School/GED]|[-0.0665499120950...|[Online Registrant]|[0.05286745727062...|\n",
      "|   [High School/GED]|[-0.0665499120950...|[Online Registrant]|[0.05286745727062...|\n",
      "|[Doctorate (PhD, ...|[1.55330220858256...|[Online Registrant]|[0.05286745727062...|\n",
      "|[4 Year Degree (B...|[-0.8293966402610...|[Online Registrant]|[0.05286745727062...|\n",
      "|[4 Year Degree (B...|[-0.8293966402610...|[Online Registrant]|[0.05286745727062...|\n",
      "|   [High School/GED]|[-0.0665499120950...|[Online Registrant]|[0.05286745727062...|\n",
      "|[4 Year Degree (B...|[-0.8293966402610...|[Online Registrant]|[0.05286745727062...|\n",
      "|[4 Year Degree (B...|[-0.8293966402610...|[Online Registrant]|[0.05286745727062...|\n",
      "|[4 Year Degree (B...|[-0.8293966402610...|[Online Registrant]|[0.05286745727062...|\n",
      "|[2 Year Degree (A...|[0.09756429990132...|[Online Registrant]|[0.05286745727062...|\n",
      "|   [High School/GED]|[-0.0665499120950...|[Online Registrant]|[0.05286745727062...|\n",
      "|[Post-Graduate De...|[0.45273831486701...|[Online Registrant]|[0.05286745727062...|\n",
      "|   [High School/GED]|[-0.0665499120950...|[Online Registrant]|[0.05286745727062...|\n",
      "+--------------------+--------------------+-------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfspark.select('Educ','EducVec','Client_Type','Client_Type_Vec').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['S_years',\n",
       " 'R_years',\n",
       " 'Resume_Tips',\n",
       " 'Resume_OnFile',\n",
       " 'Client',\n",
       " 'Client_Type',\n",
       " 'Service_R',\n",
       " 'Service_B',\n",
       " 'HHUSA_hire',\n",
       " 'Com_Method',\n",
       " 'responsive__c',\n",
       " 'active__c',\n",
       " 'Created_Linkedin',\n",
       " 'Educ',\n",
       " 'hired',\n",
       " 'Occupation']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfspark.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "hasher = FeatureHasher(inputCols= ['S_years','R_years','Resume_Tips','Resume_OnFile','Client','Client_Type','Service_R',\\\n",
    " 'Service_B','HHUSA_hire','Com_Method','responsive__c','active__c','Created_Linkedin','Educ','hired','Occupation'],\n",
    "                       outputCol=\"features\")\n",
    "\n",
    "featurizedf = hasher.transform(_dfspark)\n",
    "#featurizedf.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "USING column `features` cannot be resolved on the right side of the join. The right-side columns: [hired];",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-19d4be174616>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeaturizedf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'features'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_dfspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'hired'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'features'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'outer'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#df.show()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, other, on, how)\u001b[0m\n\u001b[1;32m   1146\u001b[0m                 \u001b[0mon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jseq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"how should be basestring\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1148\u001b[0;31m             \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1149\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    135\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: USING column `features` cannot be resolved on the right side of the join. The right-side columns: [hired];"
     ]
    }
   ],
   "source": [
    "df = featurizedf.select('features').join(_dfspark.select('hired'), on=['features'], how='outer')\n",
    "#df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "|(262144,[41931,52...|\n",
      "+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "featurizedf.select('features').show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|hired|\n",
      "+-----+\n",
      "|  0.0|\n",
      "|  0.0|\n",
      "|  0.0|\n",
      "|  0.0|\n",
      "|  0.0|\n",
      "|  0.0|\n",
      "+-----+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_dfspark.select('hired').show(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = StringIndexer(inputCol=\"hired\", outputCol=\"hiredindex\").fit(_dfspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'StringIndexerModel' object has no attribute 'show'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-2e57857ea7fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'StringIndexerModel' object has no attribute 'show'"
     ]
    }
   ],
   "source": [
    "\n",
    "labels.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                Educ|             EducVec|\n",
      "+--------------------+--------------------+\n",
      "|   [High School/GED]|[0.09593699872493...|\n",
      "|[2 Year Degree (A...|[-0.2880331476529...|\n",
      "|[4 Year Degree (B...|[-0.4206845362981...|\n",
      "|[Post-Graduate De...|[-0.1366704897955...|\n",
      "|   [High School/GED]|[0.09593699872493...|\n",
      "|   [High School/GED]|[0.09593699872493...|\n",
      "|[4 Year Degree (B...|[-0.4206845362981...|\n",
      "|   [High School/GED]|[0.09593699872493...|\n",
      "|   [High School/GED]|[0.09593699872493...|\n",
      "|[Doctorate (PhD, ...|[-1.2942123810450...|\n",
      "|[4 Year Degree (B...|[-0.4206845362981...|\n",
      "|[4 Year Degree (B...|[-0.4206845362981...|\n",
      "|   [High School/GED]|[0.09593699872493...|\n",
      "|[4 Year Degree (B...|[-0.4206845362981...|\n",
      "|[4 Year Degree (B...|[-0.4206845362981...|\n",
      "|[4 Year Degree (B...|[-0.4206845362981...|\n",
      "|[2 Year Degree (A...|[-0.2880331476529...|\n",
      "|   [High School/GED]|[0.09593699872493...|\n",
      "|[Post-Graduate De...|[-0.1366704897955...|\n",
      "|   [High School/GED]|[0.09593699872493...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "resultdfspark.select('Educ','EducVec').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----------+-------------+------+-----------------+---------+---------+----------+----------+-------------+---------+----------------+---------------+-----+\n",
      "|S_years|R_years|Resume_Tips|Resume_OnFile|Client|      Client_Type|Service_R|Service_B|HHUSA_hire|Com_Method|responsive__c|active__c|Created_Linkedin|           Educ|hired|\n",
      "+-------+-------+-----------+-------------+------+-----------------+---------+---------+----------+----------+-------------+---------+----------------+---------------+-----+\n",
      "|   8.11|  -0.37|          1|            1|     1|Online Registrant|      E-5|     Army|         0| Telephone|            1|      Yes|               1|High School/GED|    0|\n",
      "+-------+-------+-----------+-------------+------+-----------------+---------+---------+----------+----------+-------------+---------+----------------+---------------+-----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfspark.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(S_years=8.11, R_years=-0.37, Resume_Tips=1, Resume_OnFile=1, Client=1, Client_Type='Online Registrant', Service_R='E-5', Service_B='Army', HHUSA_hire=0, Com_Method='Telephone', responsive__c=1, active__c='Yes', Created_Linkedin=1, Educ='High School/GED', hired='0')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfspark.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labelIndexer = StringIndexer(inputCol=\"Client_Type\", outputCol=\"Client_Type_indexed\")\n",
    "#Indexed = labelIndexer.fit(dfspark).transform(dfspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(S_years=8.11, R_years=-0.37, Resume_Tips=1, Resume_OnFile=1, Client=1, Client_Type='Online Registrant', Service_R='E-5', Service_B='Army', HHUSA_hire=0, Com_Method='Telephone', responsive__c=1, active__c='Yes', Created_Linkedin=1, hired='0', Client_Type_indexed=0.0)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfspark.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dfspark.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('S_years', 'double'),\n",
       " ('R_years', 'double'),\n",
       " ('Resume_Tips', 'int'),\n",
       " ('Resume_OnFile', 'int'),\n",
       " ('Client', 'int'),\n",
       " ('Client_Type', 'string'),\n",
       " ('Service_R', 'string'),\n",
       " ('Service_B', 'string'),\n",
       " ('HHUSA_hire', 'int'),\n",
       " ('Com_Method', 'string'),\n",
       " ('responsive__c', 'int'),\n",
       " ('active__c', 'string'),\n",
       " ('Created_Linkedin', 'int'),\n",
       " ('hired', 'string')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfspark.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "requirement failed: Column Client_Type must be of type struct<type:tinyint,size:int,indices:array<int>,values:array<double>> but was actually string.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-61827f131d4b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfeatureIndexer\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m         \u001b[0mVectorIndexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Client_Type'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Client_Type_Index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfspark\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    127\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    316\u001b[0m         \"\"\"\n\u001b[1;32m    317\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    135\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: requirement failed: Column Client_Type must be of type struct<type:tinyint,size:int,indices:array<int>,values:array<double>> but was actually string."
     ]
    }
   ],
   "source": [
    "featureIndexer =\\\n",
    "        VectorIndexer(inputCol='Client_', outputCol=\"Client_Type_Index\").fit(Indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2885"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfspark.filter(dfspark.Educ.isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|         Client_Type|\n",
      "+--------------------+\n",
      "|Online Training C...|\n",
      "|   Online Registrant|\n",
      "|Onward To Opportu...|\n",
      "|Workshop Participant|\n",
      "|                 AVR|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfspark.select(\"Client_Type\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----------+-------------+------+-----------------+---------+---------+----------+----------+-------------+---------+----------------+-----+---------------------+\n",
      "|S_years|R_years|Resume_Tips|Resume_OnFile|Client|      Client_Type|Service_R|Service_B|HHUSA_hire|Com_Method|responsive__c|active__c|Created_Linkedin|hired|Client_Type_Numerical|\n",
      "+-------+-------+-----------+-------------+------+-----------------+---------+---------+----------+----------+-------------+---------+----------------+-----+---------------------+\n",
      "|   8.11|  -0.37|          1|            1|     1|Online Registrant|      E-5|     Army|         0| Telephone|            1|      Yes|               1|    0|                  0.0|\n",
      "|  10.24|    0.4|          1|            1|     1|Online Registrant|      E-5|     Navy|         1| Telephone|            1|      Yes|               1|    0|                  0.0|\n",
      "|   4.69|   6.34|          1|            1|     1|Online Registrant|      E-4|     Army|         0| Telephone|            1|      Yes|               1|    0|                  0.0|\n",
      "|  24.86|    3.1|          1|            1|     1|Online Registrant|      E-9|     Army|         0| Telephone|            1|      Yes|               1|    0|                  0.0|\n",
      "|   5.43|   0.49|          0|            1|     1|Online Registrant|      E-4|     Navy|         0| Telephone|            1|      Yes|               0|    0|                  0.0|\n",
      "|    4.0|  -0.22|          1|            1|     1|Online Registrant|      E-4|  Marines|         0| Telephone|            1|      Yes|               1|    0|                  0.0|\n",
      "|  20.01|  -0.74|          1|            1|     1|Online Registrant|      E-6|Air Force|         0| Telephone|            1|      Yes|               0|    0|                  0.0|\n",
      "|  30.16|  -0.24|          1|            1|     1|Online Registrant|      O-4|     Navy|         0| Telephone|            1|      Yes|               1|    0|                  0.0|\n",
      "|   14.5|   2.48|          1|            1|     1|Online Registrant|      E-4|     Army|         0| Telephone|            1|      Yes|               1|    0|                  0.0|\n",
      "|  23.21|  -0.07|          0|            0|     1|Online Registrant|      O-5|     Army|         0|    E-Mail|            1|      Yes|               0|    0|                  0.0|\n",
      "|    6.7|  -0.49|          1|            1|     1|Online Registrant|      O-3|  Marines|         0| Telephone|            1|      Yes|               0|    0|                  0.0|\n",
      "|  15.04|  22.14|          1|            1|     1|Online Registrant|      E-5|     Army|         0| Telephone|            1|      Yes|               1|    0|                  0.0|\n",
      "|  21.66|  10.44|          1|            1|     1|Online Registrant|      E-8|  Marines|         0| Telephone|            1|      Yes|               1|    0|                  0.0|\n",
      "|   17.0|  -5.08|          1|            1|     1|Online Registrant|      O-4|Air Force|         1| Telephone|            1|      Yes|               1|    0|                  0.0|\n",
      "|   8.58|   6.85|          1|            1|     1|Online Registrant|      E-5|     Navy|         0| Telephone|            1|      Yes|               1|    0|                  0.0|\n",
      "|  14.82|   1.36|          1|            1|     1|Online Registrant|      E-6|     Army|         1| Telephone|            1|      Yes|               0|    0|                  0.0|\n",
      "|   7.76|   2.11|          0|            1|     1|Online Registrant|      E-5|     Army|         1| Telephone|            1|      Yes|               0|    0|                  0.0|\n",
      "|   5.36|  -0.15|          1|            1|     1|Online Registrant|      E-5|     Army|         0| Telephone|            1|      Yes|               1|    0|                  0.0|\n",
      "|  21.67|   2.43|          0|            0|     1|Online Registrant|      E-7|     Army|         0| Telephone|            1|      Yes|               0|    0|                  0.0|\n",
      "|   6.19|   3.86|          1|            1|     1|Online Registrant|      E-4|Air Force|         1| Telephone|            1|      Yes|               1|    0|                  0.0|\n",
      "+-------+-------+-----------+-------------+------+-----------------+---------+---------+----------+----------+-------------+---------+----------------+-----+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"Client_Type\", outputCol=\"Client_Type_Numerical\")\n",
    "indexed = indexer.fit(dfspark).transform(dfspark)\n",
    "#df = sqlContext.createDataFrame(\n",
    " #   [(0, \"a\"), (1, \"b\"), (2, \"c\"), (3, \"a\"), (4, \"a\"), (5, \"c\")],\n",
    "#    [\"id\", \"category\"])\n",
    "indexed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|Client_Type_Numerical|\n",
      "+---------------------+\n",
      "|                  0.0|\n",
      "|                  1.0|\n",
      "|                  4.0|\n",
      "|                  3.0|\n",
      "|                  2.0|\n",
      "+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(result=DenseVector([0.0271, 0.0145, 0.032]))\n",
      "Row(result=DenseVector([-0.0257, 0.0251, 0.0292]))\n",
      "Row(result=DenseVector([0.013, 0.063, -0.0221]))\n",
      "+--------------------+\n",
      "|                text|\n",
      "+--------------------+\n",
      "|[Hi, I, heard, ab...|\n",
      "|[I, wish, Java, c...|\n",
      "|[Logistic, regres...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "sc = SparkContext.getOrCreate()\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "\n",
    "# Input data: Each row is a bag of words from a sentence or document.\n",
    "documentDF = sqlContext.createDataFrame([\n",
    "  (\"Hi I heard about Spark\".split(\" \"), ),\n",
    "  (\"I wish Java could use case classes\".split(\" \"), ),\n",
    "  (\"Logistic regression models are neat\".split(\" \"), )\n",
    "], [\"text\"])\n",
    "# Learn a mapping from words to Vectors.\n",
    "word2Vec = Word2Vec(vectorSize=3, minCount=0, inputCol=\"text\", outputCol=\"result\")\n",
    "model = word2Vec.fit(documentDF)\n",
    "result = model.transform(documentDF)\n",
    "for feature in result.select(\"result\").take(3):\n",
    "    print(feature)\n",
    "\n",
    "documentDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(features=SparseVector(20, {6: 0.2877, 8: 0.6931, 13: 0.2877, 16: 0.5754}), label=0)\n",
      "Row(features=SparseVector(20, {0: 0.6931, 2: 0.6931, 7: 1.3863, 13: 0.2877, 15: 0.6931, 16: 0.2877}), label=0)\n",
      "Row(features=SparseVector(20, {3: 0.6931, 4: 0.6931, 6: 0.2877, 11: 0.6931, 19: 0.6931}), label=1)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "\n",
    "sentenceData = sqlContext.createDataFrame([\n",
    "  (0, \"Hi I heard about Spark\"),\n",
    "  (0, \"I wish Java could use case classes\"),\n",
    "  (1, \"Logistic regression models are neat\")\n",
    "], [\"label\", \"sentence\"])\n",
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
    "wordsData = tokenizer.transform(sentenceData)\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=20)\n",
    "featurizedData = hashingTF.transform(wordsData)\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "rescaledData = idfModel.transform(featurizedData)\n",
    "for features_label in rescaledData.select(\"features\", \"label\").take(3):\n",
    "    print(features_label)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---------+------+------+--------------------------------------------------------+\n",
      "|real|bool |stringNum|string|target|features                                                |\n",
      "+----+-----+---------+------+------+--------------------------------------------------------+\n",
      "|2.2 |true |1        |foo   |1     |(262144,[174475,247670,257907,262126],[2.2,1.0,1.0,1.0])|\n",
      "|3.3 |false|2        |bar   |0     |(262144,[70644,89673,173866,174475],[1.0,1.0,1.0,3.3])  |\n",
      "|4.4 |false|3        |baz   |0     |(262144,[22406,70644,174475,187923],[1.0,1.0,4.4,1.0])  |\n",
      "|5.5 |false|7        |foo   |1     |(262144,[70644,174475,251231,257907],[1.0,5.5,1.0,1.0]) |\n",
      "+----+-----+---------+------+------+--------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import FeatureHasher\n",
    "\n",
    "dataset = spark.createDataFrame([\n",
    "    (2.2, True, \"1\", \"foo\", 1),\n",
    "    (3.3, False, \"2\", \"bar\", 0),\n",
    "    (4.4, False, \"3\", \"baz\", 0),\n",
    "    (5.5, False, \"7\", \"foo\", 1)\n",
    "], [\"real\", \"bool\", \"stringNum\", \"string\", \"target\"])\n",
    "\n",
    "hasher = FeatureHasher(inputCols=[\"real\", \"bool\", \"stringNum\", \"string\"],\n",
    "                       outputCol=\"features\")\n",
    "\n",
    "featurized = hasher.transform(dataset)\n",
    "featurized.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dfspark.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:34859)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/mose/anaconda3/lib/python3.7/site-packages/py4j/java_gateway.py\", line 977, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/mose/anaconda3/lib/python3.7/site-packages/py4j/java_gateway.py\", line 1115, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n"
     ]
    },
    {
     "ename": "Py4JNetworkError",
     "evalue": "An error occurred while trying to connect to the Java server (127.0.0.1:34859)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m_get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    976\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 977\u001b[0;31m             \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeque\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    978\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: pop from an empty deque",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1114\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1115\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1116\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakefile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mPy4JNetworkError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-895a79fabf49>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfeaturized\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcolumns\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1030\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0;34m'age'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m         \"\"\"\n\u001b[0;32m-> 1032\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfields\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1033\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mschema\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_schema\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_schema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parse_datatype_json_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m                 raise Exception(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1301\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1303\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1029\u001b[0m          \u001b[0;32mif\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m         \"\"\"\n\u001b[0;32m-> 1031\u001b[0;31m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1032\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m_get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    977\u001b[0m             \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeque\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    978\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 979\u001b[0;31m             \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    980\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m_create_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    983\u001b[0m         connection = GatewayConnection(\n\u001b[1;32m    984\u001b[0m             self.gateway_parameters, self.gateway_property)\n\u001b[0;32m--> 985\u001b[0;31m         \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1125\u001b[0m                 \u001b[0;34m\"server ({0}:{1})\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1127\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mPy4JNetworkError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1129\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_authenticate_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JNetworkError\u001b[0m: An error occurred while trying to connect to the Java server (127.0.0.1:34859)"
     ]
    }
   ],
   "source": [
    "featurized.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/mose/anaconda3/lib/python3.7/site-packages/py4j/java_gateway.py\", line 1207, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/mose/anaconda3/lib/python3.7/site-packages/py4j/java_gateway.py\", line 1033, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/mose/anaconda3/lib/python3.7/site-packages/py4j/java_gateway.py\", line 1212, in send_command\n",
      "    \"Error while receiving\", e, proto.ERROR_ON_RECEIVE)\n",
      "py4j.protocol.Py4JNetworkError: Error while receiving\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "functions does not exist in the JVM",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-e861de591144>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfeaturized\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"features\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   1419\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Alice'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Bob'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m         \"\"\"\n\u001b[0;32m-> 1421\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jcols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1422\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m_jcols\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   1213\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1214\u001b[0m             \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1215\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jseq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_to_java_column\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1217\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_sort_cols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m_jseq\u001b[0;34m(self, cols, converter)\u001b[0m\n\u001b[1;32m   1200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_jseq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1201\u001b[0m         \u001b[0;34m\"\"\"Return a JVM Seq of Columns from a list of Column or names\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1202\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_to_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1204\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_jmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/sql/column.py\u001b[0m in \u001b[0;36m_to_seq\u001b[0;34m(sc, cols, converter)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \"\"\"\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/sql/column.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \"\"\"\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/sql/column.py\u001b[0m in \u001b[0;36m_to_java_column\u001b[0;34m(col)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mjcol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mjcol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_column_from_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         raise TypeError(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/sql/column.py\u001b[0m in \u001b[0;36m_create_column_from_name\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_create_column_from_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1703\u001b[0m             message = compute_exception_message(\n\u001b[1;32m   1704\u001b[0m                 \"{0} does not exist in the JVM\".format(name), error_message)\n\u001b[0;32m-> 1705\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mPy4JError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JError\u001b[0m: functions does not exist in the JVM"
     ]
    }
   ],
   "source": [
    "featurized.select(\"features\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o503.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 137.0 failed 1 times, most recent failure: Lost task 3.0 in stage 137.0 (TID 2045, mose-Inspiron-5520.lan, executor driver): java.lang.OutOfMemoryError: Java heap space\n\tat java.util.Arrays.copyOf(Arrays.java:3236)\n\tat java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:118)\n\tat java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93)\n\tat java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:153)\n\tat org.apache.spark.util.ByteBufferOutputStream.write(ByteBufferOutputStream.scala:41)\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.drain(ObjectOutputStream.java:1877)\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.setBlockDataMode(ObjectOutputStream.java:1786)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1189)\n\tat java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)\n\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:44)\n\tat org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:101)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:489)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2188)\n\tat org.apache.spark.rdd.RDD.$anonfun$fold$1(RDD.scala:1157)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1151)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$1(RDD.scala:1220)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1196)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeReduce$1(RDD.scala:1127)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.RDD.treeReduce(RDD.scala:1105)\n\tat org.apache.spark.ml.feature.VectorIndexer.fit(VectorIndexer.scala:152)\n\tat org.apache.spark.ml.feature.VectorIndexer.fit(VectorIndexer.scala:119)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.OutOfMemoryError: Java heap space\n\tat java.util.Arrays.copyOf(Arrays.java:3236)\n\tat java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:118)\n\tat java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93)\n\tat java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:153)\n\tat org.apache.spark.util.ByteBufferOutputStream.write(ByteBufferOutputStream.scala:41)\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.drain(ObjectOutputStream.java:1877)\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.setBlockDataMode(ObjectOutputStream.java:1786)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1189)\n\tat java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)\n\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:44)\n\tat org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:101)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:489)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-4cf9d9134bbd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfeatureIndexer\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m         \u001b[0mVectorIndexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'features'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"indexedFeatures\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeaturized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    127\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    316\u001b[0m         \"\"\"\n\u001b[1;32m    317\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o503.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 137.0 failed 1 times, most recent failure: Lost task 3.0 in stage 137.0 (TID 2045, mose-Inspiron-5520.lan, executor driver): java.lang.OutOfMemoryError: Java heap space\n\tat java.util.Arrays.copyOf(Arrays.java:3236)\n\tat java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:118)\n\tat java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93)\n\tat java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:153)\n\tat org.apache.spark.util.ByteBufferOutputStream.write(ByteBufferOutputStream.scala:41)\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.drain(ObjectOutputStream.java:1877)\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.setBlockDataMode(ObjectOutputStream.java:1786)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1189)\n\tat java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)\n\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:44)\n\tat org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:101)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:489)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2188)\n\tat org.apache.spark.rdd.RDD.$anonfun$fold$1(RDD.scala:1157)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1151)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$1(RDD.scala:1220)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1196)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeReduce$1(RDD.scala:1127)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.RDD.treeReduce(RDD.scala:1105)\n\tat org.apache.spark.ml.feature.VectorIndexer.fit(VectorIndexer.scala:152)\n\tat org.apache.spark.ml.feature.VectorIndexer.fit(VectorIndexer.scala:119)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.OutOfMemoryError: Java heap space\n\tat java.util.Arrays.copyOf(Arrays.java:3236)\n\tat java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:118)\n\tat java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93)\n\tat java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:153)\n\tat org.apache.spark.util.ByteBufferOutputStream.write(ByteBufferOutputStream.scala:41)\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.drain(ObjectOutputStream.java:1877)\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.setBlockDataMode(ObjectOutputStream.java:1786)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1189)\n\tat java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)\n\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:44)\n\tat org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:101)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:489)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 37766)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/mose/anaconda3/lib/python3.7/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/home/mose/anaconda3/lib/python3.7/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/home/mose/anaconda3/lib/python3.7/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/home/mose/anaconda3/lib/python3.7/socketserver.py\", line 720, in __init__\n",
      "    self.handle()\n",
      "  File \"/home/mose/anaconda3/lib/python3.7/site-packages/pyspark/accumulators.py\", line 268, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/home/mose/anaconda3/lib/python3.7/site-packages/pyspark/accumulators.py\", line 241, in poll\n",
      "    if func():\n",
      "  File \"/home/mose/anaconda3/lib/python3.7/site-packages/pyspark/accumulators.py\", line 245, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/home/mose/anaconda3/lib/python3.7/site-packages/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "featureIndexer =\\\n",
    "        VectorIndexer(inputCol='features', outputCol=\"indexedFeatures\").fit(featurized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
